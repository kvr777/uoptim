{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from adamw import AdamW\n",
    "from nadam import Nadam\n",
    "from uoptim import UOptimizer\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from utils_exp import plot_graphs\n",
    "\n",
    "import numpy as np\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "PATH = './fMNIST_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = datasets.FashionMNIST(PATH, train=True, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "# calculate statistics for fashion MNIST\n",
    "global_norm_mean = train_data.train_data.float().div(255).mean().item()\n",
    "global_norm_std = train_data.train_data.float().div(255).std().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# util to create loaders\n",
    "mnist_transform = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((global_norm_mean,), (global_norm_std,)),\n",
    "           ])\n",
    "\n",
    "def mnist(batch_size=50, valid=0, shuffle=True, transform=mnist_transform, path=PATH):\n",
    "    test_data = datasets.FashionMNIST(path, train=False, download=True, transform=transform)\n",
    "    test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    train_data = datasets.FashionMNIST(path, train=True, download=True, transform=transform)\n",
    "    if valid > 0:\n",
    "        num_train = len(train_data)\n",
    "        indices = list(range(num_train))\n",
    "        split = num_train-valid\n",
    "        np.random.shuffle(indices)\n",
    "\n",
    "        train_idx, valid_idx = indices[:split], indices[split:]\n",
    "        train_sampler = SubsetRandomSampler(train_idx)\n",
    "        valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "        train_loader = DataLoader(train_data, batch_size=batch_size, sampler=train_sampler)\n",
    "        valid_loader = DataLoader(train_data, batch_size=batch_size, sampler=valid_sampler)\n",
    "    \n",
    "        return train_loader, valid_loader, test_loader\n",
    "    else:\n",
    "        train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=shuffle)\n",
    "        return train_loader, test_loader\n",
    "\n",
    "\n",
    "def plot_mnist(images, shape):\n",
    "    fig = plt.figure(figsize=shape[::-1], dpi=80)\n",
    "    for j in range(1, len(images) + 1):\n",
    "        ax = fig.add_subplot(shape[0], shape[1], j)\n",
    "        ax.matshow(images[j - 1, 0, :, :], cmap = matplotlib.cm.binary)\n",
    "        plt.xticks(np.array([]))\n",
    "        plt.yticks(np.array([]))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_loader, valid_loader, test_loader = mnist(valid=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ConvLayer(nn.Module):\n",
    "    def __init__(self, size, padding=1, pool_layer=nn.MaxPool2d(2, stride=2),\n",
    "                 bn=False, dropout=False, activation_fn=nn.ReLU()):\n",
    "        super(ConvLayer, self).__init__()\n",
    "        layers = []\n",
    "        layers.append(nn.Conv2d(size[0], size[1], size[2], padding=padding))\n",
    "        if pool_layer is not None:\n",
    "            layers.append(pool_layer)\n",
    "        if bn:\n",
    "            layers.append(nn.BatchNorm2d(size[1]))\n",
    "        if dropout:\n",
    "            layers.append(nn.Dropout2d())\n",
    "        layers.append(activation_fn)\n",
    "        \n",
    "        self.model = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FullyConnected(nn.Module):\n",
    "    def __init__(self, sizes, dropout=False, activation_fn=nn.Tanh):\n",
    "        super(FullyConnected, self).__init__()\n",
    "        layers = []\n",
    "        \n",
    "        for i in range(len(sizes) - 2):\n",
    "            layers.append(nn.Linear(sizes[i], sizes[i+1]))\n",
    "            if dropout:\n",
    "                layers.append(nn.Dropout())\n",
    "            layers.append(activation_fn())\n",
    "        else: # нам не нужен дропаут и фнкция активации в последнем слое\n",
    "            layers.append(nn.Linear(sizes[-2], sizes[-1]))\n",
    "        \n",
    "        self.model = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, batchnorm=False, dropout=False, optim_type='UAdam', **optim_params):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self._conv1 = ConvLayer([1, 16, 3], bn=batchnorm)\n",
    "        self._conv2 = ConvLayer([16, 32, 3], bn=batchnorm, activation_fn=nn.Sigmoid())\n",
    "        \n",
    "        self.fc = FullyConnected([32*7*7, 10], dropout=dropout)\n",
    "        \n",
    "        self._loss = None\n",
    "        \n",
    "        if optim_type == 'SGD':\n",
    "            self.optim = optim.SGD(self.parameters(), **optim_params)\n",
    "        elif optim_type == 'Adadelta':\n",
    "            self.optim = optim.Adadelta(self.parameters(), **optim_params)\n",
    "        elif optim_type == 'RMSProp':\n",
    "            self.optim = optim.RMSprop(self.parameters(), **optim_params)\n",
    "        elif optim_type == 'Adam':\n",
    "            self.optim = optim.Adam(self.parameters(), **optim_params)\n",
    "        elif optim_type == 'AdamW':\n",
    "            self.optim = AdamW(self.parameters(), **optim_params)\n",
    "        elif optim_type == 'Nadam':\n",
    "            self.optim = Nadam(self.parameters(), **optim_params)\n",
    "        elif optim_type == 'Adamax':\n",
    "            self.optim = optim.Adamax(self.parameters(), **optim_params)\n",
    "        elif optim_type == 'UOptimizer':\n",
    "            self.optim = UOptimizer(params = self.parameters(), **optim_params)\n",
    "    \n",
    "    def conv(self, x):\n",
    "        x = self._conv1(x)\n",
    "        x = self._conv2(x)\n",
    "        return x\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(-1, 32*7*7)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "    def loss(self, output, target, **kwargs):\n",
    "        self._loss = F.cross_entropy(output, target, **kwargs)\n",
    "        self._correct = output.data.max(1, keepdim=True)[1]\n",
    "        self._correct = self._correct.eq(target.data.view_as(self._correct)).to(torch.float).cpu().mean()\n",
    "        return self._loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are using the AdamW optimizer\n"
     ]
    }
   ],
   "source": [
    "models = {\n",
    "          # testing correctness of SGD\n",
    "          'SGD': Net(True, False, 'SGD', lr=1e-3).to(device), \n",
    "          'UOSGD': Net(True, False, 'UOptimizer', lr=1e-3).to(device), \n",
    "    \n",
    "          'SGD_momentum':Net(True, False, 'SGD', momentum=0.9, lr=1e-3).to(device),\n",
    "          'UOSGD_momentum':Net(True, False, 'UOptimizer', use_exp_avg_norm = True, beta1_dump=0, lr=1e-3).to(device),\n",
    "    \n",
    "          'SGD_momentum_n':Net(True, False, 'SGD', momentum=0.9, nesterov=True,  lr=1e-3).to(device),\n",
    "          'UOSGD_momentum_n':Net(True, False, 'UOptimizer', use_exp_avg_norm = True, beta1_dump=0,\n",
    "                                 exp_avg_norm_type='nesterov', lr=1e-3).to(device),\n",
    "    \n",
    "          # testing RMSProp      \n",
    "          'RMSProp': Net(True, False, 'RMSProp', lr=1e-4).to(device), \n",
    "          'UORMSProp': Net(True, False, 'UOptimizer', use_exp_avg_sq_norm = True,  lr=1e-4).to(device), \n",
    "          \n",
    "          # testing Adadelta\n",
    "          'Adadelta':Net(True, False, 'Adadelta', lr=1).to(device),\n",
    "          'UOAdadelta':Net(True, False, 'UOptimizer', use_exp_avg_sq_norm = True, use_adadelta_lr=True, lr=1).to(device),\n",
    "            \n",
    "          # testing adam-like algoritms. \n",
    "          'Adam': Net(True, False, 'Adam', lr=1e-4).to(device), \n",
    "          'UOAdam': Net(True, False, 'UOptimizer', use_exp_avg_norm=True, \n",
    "                        use_exp_avg_sq_norm = True, use_bias_correction= True, lr=1e-4).to(device), \n",
    "    \n",
    "          'Amsgrad':Net(True, False, 'Adam', lr=1e-4, amsgrad=True).to(device),\n",
    "          'UOAmsgrad':Net(True, False, 'UOptimizer', use_exp_avg_norm=True, use_exp_avg_sq_norm = True, \n",
    "                            use_bias_correction=True, exp_avg_sq_norm_type='max_past_sq', lr=1e-4).to(device), \n",
    "          \n",
    "          # AdamW is not included in Pytorch, so I used fastai implementation from here:\n",
    "          # https://github.com/anandsaha/fastai.part1.v2/commit/159e1712e60f299e11c42caab35c726f367bcd61\n",
    "          'AdamW':Net(True, False, 'AdamW', lr=1e-4, weight_decay=0.00025).to(device),\n",
    "          'UOAdamW':Net(True, False, 'UOptimizer', use_exp_avg_norm=True, use_exp_avg_sq_norm = True, \n",
    "                        use_bias_correction= True, decouple_wd=True, lr=1e-4, weight_decay=0.00025).to(device),\n",
    "    \n",
    "          # Please note that pytorch Nadam is the official Nadam implementation for Keras translated to PyTorch\n",
    "          # I use the classical Nadam formulas, so the results could be differ\n",
    "          'Nadam':Net(True, False, 'Nadam', lr=1e-4).to(device),\n",
    "          'UONadam':Net(True, False, 'UOptimizer', use_exp_avg_norm=True, use_exp_avg_sq_norm = True, \n",
    "                            use_bias_correction= True,exp_avg_norm_type='nesterov',  lr=1e-4).to(device),\n",
    "          \n",
    "          'Adamax':Net(True, False, 'Adamax', lr=1e-4).to(device),\n",
    "          'UOAdamax':Net(True, False, 'UOptimizer', use_exp_avg_norm=True, use_exp_avg_sq_norm = True, \n",
    "                        use_bias_correction= True, exp_avg_sq_norm_type ='infinite_l', lr=1e-4).to(device),\n",
    "    \n",
    "    \n",
    "          # create exotic combinations for fun\n",
    "          'Adam_with_adadelta_coeff':Net(True, False, 'UOptimizer', \n",
    "                                         use_exp_avg_norm = True,\n",
    "                                         use_exp_avg_sq_norm = True,\n",
    "                                         use_adadelta_lr = True,\n",
    "                                         use_bias_correction = True,\n",
    "                                         lr=1).to(device),\n",
    "          'Adam_with_exp_avg_norm_like_sgd':Net(True, False, 'UOptimizer', \n",
    "                                         use_exp_avg_norm = True,\n",
    "                                         use_exp_avg_sq_norm = True,\n",
    "                                         use_bias_correction = True,\n",
    "                                         beta1_dump=0,\n",
    "                                         lr=1e-4).to(device),                  \n",
    "            }\n",
    "train_log = {k: [] for k in models}\n",
    "test_log = {k: [] for k in models}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(epoch, models, log=None):\n",
    "    train_size = len(train_loader.sampler)\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        for model in models.values():\n",
    "            model.optim.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = model.loss(output, target)\n",
    "            loss.backward()\n",
    "            model.optim.step()\n",
    "            \n",
    "        if batch_idx % 200 == 0:\n",
    "            line = 'Train Epoch: {} [{}/{} ({:.0f}%)]\\tLosses '.format(\n",
    "                epoch, batch_idx * len(data), train_size, 100. * batch_idx / len(train_loader))\n",
    "            losses = ' '.join(['{}: {:.4f}'.format(k, m._loss.item()) for k, m in models.items()])\n",
    "            print(line + losses)\n",
    "            \n",
    "    else:\n",
    "        batch_idx += 1\n",
    "        line = 'Train Epoch: {} [{}/{} ({:.0f}%)]\\tLosses '.format(\n",
    "            epoch, batch_idx * len(data), train_size, 100. * batch_idx / len(train_loader))\n",
    "        losses = ' '.join(['{}: {:.4f}'.format(k, m._loss.item()) for k, m in models.items()])\n",
    "        if log is not None:\n",
    "            for k in models:\n",
    "                log[k].append((models[k]._loss, models[k]._correct))\n",
    "        print(line + losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test(models, loader, log=None):\n",
    "    test_size = len(loader.sampler)\n",
    "    avg_lambda = lambda l: 'Loss: {:.4f}'.format(l)\n",
    "    acc_lambda = lambda c, p: 'Accuracy: {}/{} ({:.0f}%)'.format(c, test_size, p)\n",
    "    line = lambda i, l, c, p: '{}: '.format(i) + avg_lambda(l) + '\\t' + acc_lambda(c, p)\n",
    "\n",
    "    test_loss = {k: 0. for k in models}\n",
    "    correct = {k: 0. for k in models}\n",
    "    with torch.no_grad():\n",
    "        for data, target in loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = {k: m(data) for k, m in models.items()}\n",
    "            for k, m in models.items():\n",
    "                test_loss[k] += m.loss(output[k], target, size_average=False).item() # sum up batch loss\n",
    "                pred = output[k].data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "                correct[k] += pred.eq(target.data.view_as(pred)).cpu().sum().item()\n",
    "    \n",
    "    for k in models:\n",
    "        test_loss[k] /= test_size\n",
    "    correct_pct = {k: c / test_size for k, c in correct.items()}\n",
    "    lines = '\\n'.join([line(k, test_loss[k], correct[k], 100*correct_pct[k]) for k in models]) + '\\n'\n",
    "    report = 'Test set:\\n' + lines\n",
    "    if log is not None:\n",
    "        for k in models:\n",
    "            log[k].append((test_loss[k], correct_pct[k]))\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/50000 (0%)]\tLosses SGD: 2.2999 UOSGD: 2.3099 SGD_momentum: 2.3422 UOSGD_momentum: 2.4473 SGD_momentum_n: 2.3531 UOSGD_momentum_n: 2.3529 RMSProp: 2.3842 UORMSProp: 2.3295 Adadelta: 2.3630 UOAdadelta: 2.3002 Adam: 2.3547 UOAdam: 2.3272 Amsgrad: 2.3787 UOAmsgrad: 2.3775 AdamW: 2.2968 UOAdamW: 2.3983 Nadam: 2.3808 UONadam: 2.3416 Adamax: 2.3560 UOAdamax: 2.2726 Adam_with_adadelta_coeff: 2.3437 Adam_with_exp_avg_norm_like_sgd: 2.3443\n",
      "Train Epoch: 1 [10000/50000 (20%)]\tLosses SGD: 2.2181 UOSGD: 2.1376 SGD_momentum: 1.2718 UOSGD_momentum: 1.3821 SGD_momentum_n: 1.1687 UOSGD_momentum_n: 1.4108 RMSProp: 1.6016 UORMSProp: 0.9939 Adadelta: 0.4743 UOAdadelta: 0.4952 Adam: 1.7145 UOAdam: 1.7632 Amsgrad: 1.8000 UOAmsgrad: 1.7106 AdamW: 1.7846 UOAdamW: 1.7817 Nadam: 1.0715 UONadam: 1.7516 Adamax: 2.0142 UOAdamax: 2.0830 Adam_with_adadelta_coeff: 1.5639 Adam_with_exp_avg_norm_like_sgd: 0.5925\n",
      "Train Epoch: 1 [20000/50000 (40%)]\tLosses SGD: 2.1632 UOSGD: 2.0436 SGD_momentum: 0.9874 UOSGD_momentum: 1.0623 SGD_momentum_n: 0.9316 UOSGD_momentum_n: 1.0920 RMSProp: 1.3822 UORMSProp: 0.8877 Adadelta: 0.6555 UOAdadelta: 0.5803 Adam: 1.3673 UOAdam: 1.4275 Amsgrad: 1.4607 UOAmsgrad: 1.3640 AdamW: 1.5171 UOAdamW: 1.4826 Nadam: 0.9654 UONadam: 1.4083 Adamax: 1.8419 UOAdamax: 1.9131 Adam_with_adadelta_coeff: 1.0432 Adam_with_exp_avg_norm_like_sgd: 0.5646\n",
      "Train Epoch: 1 [30000/50000 (60%)]\tLosses SGD: 2.0693 UOSGD: 1.9181 SGD_momentum: 0.8192 UOSGD_momentum: 0.8484 SGD_momentum_n: 0.8103 UOSGD_momentum_n: 0.9037 RMSProp: 1.1196 UORMSProp: 0.7749 Adadelta: 0.5856 UOAdadelta: 0.5248 Adam: 1.1163 UOAdam: 1.2005 Amsgrad: 1.2124 UOAmsgrad: 1.1239 AdamW: 1.2732 UOAdamW: 1.2645 Nadam: 0.8634 UONadam: 1.1644 Adamax: 1.6332 UOAdamax: 1.7353 Adam_with_adadelta_coeff: 0.8388 Adam_with_exp_avg_norm_like_sgd: 0.5838\n",
      "Train Epoch: 1 [40000/50000 (80%)]\tLosses SGD: 1.9256 UOSGD: 1.7189 SGD_momentum: 0.5967 UOSGD_momentum: 0.5938 SGD_momentum_n: 0.5822 UOSGD_momentum_n: 0.6322 RMSProp: 0.8176 UORMSProp: 0.5551 Adadelta: 0.2338 UOAdadelta: 0.2536 Adam: 0.8511 UOAdam: 0.8687 Amsgrad: 0.9039 UOAmsgrad: 0.8271 AdamW: 0.9795 UOAdamW: 1.0053 Nadam: 0.6406 UONadam: 0.8453 Adamax: 1.3583 UOAdamax: 1.4715 Adam_with_adadelta_coeff: 0.5429 Adam_with_exp_avg_norm_like_sgd: 0.3457\n",
      "Train Epoch: 1 [50000/50000 (100%)]\tLosses SGD: 1.9035 UOSGD: 1.7135 SGD_momentum: 0.6662 UOSGD_momentum: 0.6543 SGD_momentum_n: 0.6560 UOSGD_momentum_n: 0.6963 RMSProp: 0.8387 UORMSProp: 0.6418 Adadelta: 0.4211 UOAdadelta: 0.3925 Adam: 0.8556 UOAdam: 0.8811 Amsgrad: 0.8901 UOAmsgrad: 0.8332 AdamW: 1.0321 UOAdamW: 1.0213 Nadam: 0.7216 UONadam: 0.8708 Adamax: 1.3433 UOAdamax: 1.4155 Adam_with_adadelta_coeff: 0.5995 Adam_with_exp_avg_norm_like_sgd: 0.4810\n",
      "Test set:\n",
      "SGD: Loss: 1.9056\tAccuracy: 6383.0/10000 (64%)\n",
      "UOSGD: Loss: 1.7023\tAccuracy: 6467.0/10000 (65%)\n",
      "SGD_momentum: Loss: 0.6567\tAccuracy: 7801.0/10000 (78%)\n",
      "UOSGD_momentum: Loss: 0.6696\tAccuracy: 7854.0/10000 (79%)\n",
      "SGD_momentum_n: Loss: 0.6428\tAccuracy: 7904.0/10000 (79%)\n",
      "UOSGD_momentum_n: Loss: 0.6902\tAccuracy: 7780.0/10000 (78%)\n",
      "RMSProp: Loss: 0.8454\tAccuracy: 7527.0/10000 (75%)\n",
      "UORMSProp: Loss: 0.6428\tAccuracy: 7857.0/10000 (79%)\n",
      "Adadelta: Loss: 0.3909\tAccuracy: 8621.0/10000 (86%)\n",
      "UOAdadelta: Loss: 0.3663\tAccuracy: 8686.0/10000 (87%)\n",
      "Adam: Loss: 0.8468\tAccuracy: 7626.0/10000 (76%)\n",
      "UOAdam: Loss: 0.8849\tAccuracy: 7558.0/10000 (76%)\n",
      "Amsgrad: Loss: 0.8925\tAccuracy: 7575.0/10000 (76%)\n",
      "UOAmsgrad: Loss: 0.8451\tAccuracy: 7573.0/10000 (76%)\n",
      "AdamW: Loss: 1.0145\tAccuracy: 7495.0/10000 (75%)\n",
      "UOAdamW: Loss: 1.0285\tAccuracy: 7419.0/10000 (74%)\n",
      "Nadam: Loss: 0.7112\tAccuracy: 7811.0/10000 (78%)\n",
      "UONadam: Loss: 0.8552\tAccuracy: 7620.0/10000 (76%)\n",
      "Adamax: Loss: 1.3232\tAccuracy: 7296.0/10000 (73%)\n",
      "UOAdamax: Loss: 1.4239\tAccuracy: 6953.0/10000 (70%)\n",
      "Adam_with_adadelta_coeff: Loss: 0.6112\tAccuracy: 7965.0/10000 (80%)\n",
      "Adam_with_exp_avg_norm_like_sgd: Loss: 0.4119\tAccuracy: 8513.0/10000 (85%)\n",
      "\n",
      "Train Epoch: 2 [0/50000 (0%)]\tLosses SGD: 1.9305 UOSGD: 1.7491 SGD_momentum: 0.6779 UOSGD_momentum: 0.6979 SGD_momentum_n: 0.6694 UOSGD_momentum_n: 0.7097 RMSProp: 0.8776 UORMSProp: 0.6827 Adadelta: 0.3477 UOAdadelta: 0.3224 Adam: 0.8665 UOAdam: 0.9214 Amsgrad: 0.9394 UOAmsgrad: 0.8526 AdamW: 1.0202 UOAdamW: 1.0536 Nadam: 0.7282 UONadam: 0.8995 Adamax: 1.3719 UOAdamax: 1.4633 Adam_with_adadelta_coeff: 0.6572 Adam_with_exp_avg_norm_like_sgd: 0.4075\n",
      "Train Epoch: 2 [10000/50000 (20%)]\tLosses SGD: 1.8793 UOSGD: 1.6770 SGD_momentum: 0.7284 UOSGD_momentum: 0.7464 SGD_momentum_n: 0.7143 UOSGD_momentum_n: 0.7651 RMSProp: 0.8752 UORMSProp: 0.7325 Adadelta: 0.4277 UOAdadelta: 0.5046 Adam: 0.8836 UOAdam: 0.8975 Amsgrad: 0.8960 UOAmsgrad: 0.8629 AdamW: 1.0396 UOAdamW: 1.0511 Nadam: 0.7991 UONadam: 0.8720 Adamax: 1.3092 UOAdamax: 1.3913 Adam_with_adadelta_coeff: 0.7026 Adam_with_exp_avg_norm_like_sgd: 0.4905\n",
      "Train Epoch: 2 [20000/50000 (40%)]\tLosses SGD: 1.7469 UOSGD: 1.4919 SGD_momentum: 0.6305 UOSGD_momentum: 0.5997 SGD_momentum_n: 0.6064 UOSGD_momentum_n: 0.6147 RMSProp: 0.7256 UORMSProp: 0.6075 Adadelta: 0.4215 UOAdadelta: 0.5062 Adam: 0.7451 UOAdam: 0.7443 Amsgrad: 0.7312 UOAmsgrad: 0.7269 AdamW: 0.8810 UOAdamW: 0.9016 Nadam: 0.6932 UONadam: 0.6973 Adamax: 1.0666 UOAdamax: 1.2032 Adam_with_adadelta_coeff: 0.5818 Adam_with_exp_avg_norm_like_sgd: 0.3536\n",
      "Train Epoch: 2 [30000/50000 (60%)]\tLosses SGD: 1.6892 UOSGD: 1.4691 SGD_momentum: 0.5104 UOSGD_momentum: 0.5287 SGD_momentum_n: 0.5125 UOSGD_momentum_n: 0.5114 RMSProp: 0.6033 UORMSProp: 0.4842 Adadelta: 0.2523 UOAdadelta: 0.2831 Adam: 0.6055 UOAdam: 0.6440 Amsgrad: 0.6559 UOAmsgrad: 0.6092 AdamW: 0.8175 UOAdamW: 0.8398 Nadam: 0.5878 UONadam: 0.6223 Adamax: 1.0025 UOAdamax: 1.0493 Adam_with_adadelta_coeff: 0.4469 Adam_with_exp_avg_norm_like_sgd: 0.3360\n",
      "Train Epoch: 2 [40000/50000 (80%)]\tLosses SGD: 1.7044 UOSGD: 1.4821 SGD_momentum: 0.6675 UOSGD_momentum: 0.7006 SGD_momentum_n: 0.6630 UOSGD_momentum_n: 0.7167 RMSProp: 0.7831 UORMSProp: 0.6732 Adadelta: 0.3842 UOAdadelta: 0.3909 Adam: 0.8105 UOAdam: 0.8339 Amsgrad: 0.8319 UOAmsgrad: 0.8123 AdamW: 0.9959 UOAdamW: 0.9931 Nadam: 0.7682 UONadam: 0.7932 Adamax: 1.0519 UOAdamax: 1.1702 Adam_with_adadelta_coeff: 0.5892 Adam_with_exp_avg_norm_like_sgd: 0.4100\n",
      "Train Epoch: 2 [50000/50000 (100%)]\tLosses SGD: 1.5850 UOSGD: 1.3450 SGD_momentum: 0.6107 UOSGD_momentum: 0.6175 SGD_momentum_n: 0.5899 UOSGD_momentum_n: 0.6054 RMSProp: 0.6678 UORMSProp: 0.5834 Adadelta: 0.4407 UOAdadelta: 0.4240 Adam: 0.6927 UOAdam: 0.7092 Amsgrad: 0.7376 UOAmsgrad: 0.7037 AdamW: 0.8690 UOAdamW: 0.8761 Nadam: 0.6592 UONadam: 0.7006 Adamax: 0.9449 UOAdamax: 1.0055 Adam_with_adadelta_coeff: 0.5303 Adam_with_exp_avg_norm_like_sgd: 0.4131\n",
      "Test set:\n",
      "SGD: Loss: 1.5743\tAccuracy: 6921.0/10000 (69%)\n",
      "UOSGD: Loss: 1.3414\tAccuracy: 7134.0/10000 (71%)\n",
      "SGD_momentum: Loss: 0.5467\tAccuracy: 8093.0/10000 (81%)\n",
      "UOSGD_momentum: Loss: 0.5439\tAccuracy: 8172.0/10000 (82%)\n",
      "SGD_momentum_n: Loss: 0.5265\tAccuracy: 8209.0/10000 (82%)\n",
      "UOSGD_momentum_n: Loss: 0.5513\tAccuracy: 8131.0/10000 (81%)\n",
      "RMSProp: Loss: 0.6183\tAccuracy: 7931.0/10000 (79%)\n",
      "UORMSProp: Loss: 0.5297\tAccuracy: 8211.0/10000 (82%)\n",
      "Adadelta: Loss: 0.3375\tAccuracy: 8797.0/10000 (88%)\n",
      "UOAdadelta: Loss: 0.3242\tAccuracy: 8841.0/10000 (88%)\n",
      "Adam: Loss: 0.6268\tAccuracy: 7961.0/10000 (80%)\n",
      "UOAdam: Loss: 0.6416\tAccuracy: 7952.0/10000 (80%)\n",
      "Amsgrad: Loss: 0.6604\tAccuracy: 7871.0/10000 (79%)\n",
      "UOAmsgrad: Loss: 0.6340\tAccuracy: 7921.0/10000 (79%)\n",
      "AdamW: Loss: 0.8286\tAccuracy: 7770.0/10000 (78%)\n",
      "UOAdamW: Loss: 0.8328\tAccuracy: 7756.0/10000 (78%)\n",
      "Nadam: Loss: 0.6214\tAccuracy: 7959.0/10000 (80%)\n",
      "UONadam: Loss: 0.6266\tAccuracy: 7981.0/10000 (80%)\n",
      "Adamax: Loss: 0.9113\tAccuracy: 7632.0/10000 (76%)\n",
      "UOAdamax: Loss: 0.9859\tAccuracy: 7407.0/10000 (74%)\n",
      "Adam_with_adadelta_coeff: Loss: 0.4752\tAccuracy: 8380.0/10000 (84%)\n",
      "Adam_with_exp_avg_norm_like_sgd: Loss: 0.3538\tAccuracy: 8724.0/10000 (87%)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 3 [0/50000 (0%)]\tLosses SGD: 1.6245 UOSGD: 1.3758 SGD_momentum: 0.5561 UOSGD_momentum: 0.5514 SGD_momentum_n: 0.5393 UOSGD_momentum_n: 0.5648 RMSProp: 0.6431 UORMSProp: 0.5524 Adadelta: 0.3040 UOAdadelta: 0.2961 Adam: 0.6634 UOAdam: 0.6451 Amsgrad: 0.6695 UOAmsgrad: 0.6376 AdamW: 0.8394 UOAdamW: 0.8492 Nadam: 0.6507 UONadam: 0.6328 Adamax: 0.9550 UOAdamax: 1.0228 Adam_with_adadelta_coeff: 0.4919 Adam_with_exp_avg_norm_like_sgd: 0.3953\n",
      "Train Epoch: 3 [10000/50000 (20%)]\tLosses SGD: 1.5079 UOSGD: 1.2643 SGD_momentum: 0.4701 UOSGD_momentum: 0.5037 SGD_momentum_n: 0.4751 UOSGD_momentum_n: 0.4982 RMSProp: 0.5583 UORMSProp: 0.4821 Adadelta: 0.3286 UOAdadelta: 0.2701 Adam: 0.5828 UOAdam: 0.5899 Amsgrad: 0.6057 UOAmsgrad: 0.5673 AdamW: 0.7567 UOAdamW: 0.7700 Nadam: 0.5548 UONadam: 0.5656 Adamax: 0.8287 UOAdamax: 0.8986 Adam_with_adadelta_coeff: 0.4396 Adam_with_exp_avg_norm_like_sgd: 0.2404\n",
      "Train Epoch: 3 [20000/50000 (40%)]\tLosses SGD: 1.4744 UOSGD: 1.2647 SGD_momentum: 0.5774 UOSGD_momentum: 0.5224 SGD_momentum_n: 0.5260 UOSGD_momentum_n: 0.5333 RMSProp: 0.6427 UORMSProp: 0.5213 Adadelta: 0.2373 UOAdadelta: 0.3206 Adam: 0.6494 UOAdam: 0.6337 Amsgrad: 0.6541 UOAmsgrad: 0.6430 AdamW: 0.8197 UOAdamW: 0.8233 Nadam: 0.6401 UONadam: 0.6201 Adamax: 0.8593 UOAdamax: 0.9087 Adam_with_adadelta_coeff: 0.4794 Adam_with_exp_avg_norm_like_sgd: 0.3354\n",
      "Train Epoch: 3 [30000/50000 (60%)]\tLosses SGD: 1.3984 UOSGD: 1.1674 SGD_momentum: 0.4187 UOSGD_momentum: 0.4161 SGD_momentum_n: 0.3780 UOSGD_momentum_n: 0.4269 RMSProp: 0.4699 UORMSProp: 0.4243 Adadelta: 0.1517 UOAdadelta: 0.2038 Adam: 0.4847 UOAdam: 0.5008 Amsgrad: 0.5450 UOAmsgrad: 0.5012 AdamW: 0.7296 UOAdamW: 0.7203 Nadam: 0.5213 UONadam: 0.4999 Adamax: 0.7416 UOAdamax: 0.8046 Adam_with_adadelta_coeff: 0.3335 Adam_with_exp_avg_norm_like_sgd: 0.1834\n",
      "Train Epoch: 3 [40000/50000 (80%)]\tLosses SGD: 1.4080 UOSGD: 1.1950 SGD_momentum: 0.5064 UOSGD_momentum: 0.5113 SGD_momentum_n: 0.4866 UOSGD_momentum_n: 0.5116 RMSProp: 0.5700 UORMSProp: 0.5201 Adadelta: 0.3915 UOAdadelta: 0.3843 Adam: 0.5785 UOAdam: 0.5730 Amsgrad: 0.6165 UOAmsgrad: 0.5895 AdamW: 0.8032 UOAdamW: 0.8180 Nadam: 0.6367 UONadam: 0.5552 Adamax: 0.7954 UOAdamax: 0.8535 Adam_with_adadelta_coeff: 0.4613 Adam_with_exp_avg_norm_like_sgd: 0.3826\n"
     ]
    }
   ],
   "source": [
    "n_epoch = 100\n",
    "for epoch in range(1, n_epoch+1):\n",
    "    for model in models.values():\n",
    "        model.train()\n",
    "    train(epoch, models, train_log)\n",
    "    for model in models.values():\n",
    "        model.eval()\n",
    "    test(models, valid_loader, test_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visual results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_graphs(test_log, 'loss', fs=(20, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Below we make a pairwise comparaison of standard algorithms and my implementation. Please note, that due to stochastic nature\n",
    "of algorithms, the descent path could differ, but not too much. Exception for Nadam. For comparaison I use official implementation for Keras and it contains some hardcoded parameters that could not fit to any dataset. My implementation more classical and in case of this dataset looks better that Keras variant. \n",
    "- For better visualisation I excluded first 5 epochs, so below are the results from 6th epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in ['SGD', 'SGD_momentum', 'SGD_momentum_n', 'RMSProp', 'Adadelta', 'Adam', 'Amsgrad', 'AdamW', 'Nadam', 'Adamax']:\n",
    "    new_test_log = {}\n",
    "    for k, v in test_log.items():\n",
    "        if k == 'UO'+i or k==i:\n",
    "            new_test_log[k] = v[1:]\n",
    "    plot_graphs(new_test_log, 'loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_graphs(test_log, 'accuracy', fs = (20, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now analyze dynamic of standard algorithms (we excluded the worst algos here like SGD, AdamW and Nadam). It looks like the best it is RMSProp here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_test_log = {}\n",
    "for i in ['SGD_momentum', 'SGD_momentum_n', 'RMSProp', 'Adadelta', 'Adam', 'Amsgrad']:\n",
    "    for k, v in test_log.items():\n",
    "        if  k==i:\n",
    "            new_test_log[k] = v[1:]\n",
    "plot_graphs(new_test_log, 'loss', fs = (20, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Lets compare RMSProp with the exotic combinations of our algorithm. **It looks like our exotic combinations provided better results that the best standard algoritm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_test_log = {}\n",
    "for i in ['RMSProp','Adam_with_adadelta_coeff', 'Adam_with_exp_avg_norm_like_sgd']:\n",
    "    for k, v in test_log.items():\n",
    "        if  k==i:\n",
    "            new_test_log[k] = v[1:]\n",
    "plot_graphs(new_test_log, 'loss', fs = (20, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
