{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision \n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from adamw import AdamW\n",
    "from nadam import Nadam\n",
    "from uoptim import UOptimizer\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "from utils_exp import plot_graphs\n",
    "import numpy as np\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "import builtins\n",
    "from IPython.lib import deepreload\n",
    "builtins.reload = deepreload.reload\n",
    "\n",
    "stats = (np.array([ 0.4914 ,  0.48216,  0.44653]), np.array([ 0.24703,  0.24349,  0.26159]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data\\cifar-10-python.tar.gz\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize(stats)])\n",
    "\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "\n",
    "train_data = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "# train_loader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
    "#                                           shuffle=True, num_workers=2)\n",
    "\n",
    "num_train = len(train_data)\n",
    "valid = int(0.2*num_train)\n",
    "indices = list(range(num_train))\n",
    "split = num_train-valid\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "train_idx, valid_idx = indices[:split], indices[split:]\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=32, sampler=train_sampler,  num_workers=2)\n",
    "valid_loader = DataLoader(train_data, batch_size=32, sampler=valid_sampler,  num_workers=2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes)\n",
    "            )\n",
    "\n",
    "        # SE layers\n",
    "        self.fc1 = nn.Conv2d(planes, planes//16, kernel_size=1)  # Use nn.Conv2d instead of nn.Linear\n",
    "        self.fc2 = nn.Conv2d(planes//16, planes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "\n",
    "        # Squeeze\n",
    "        w = F.avg_pool2d(out, out.size(2))\n",
    "        w = F.relu(self.fc1(w))\n",
    "        w = F.sigmoid(self.fc2(w))\n",
    "        # Excitation\n",
    "        out = out * w  # New broadcasting feature from v0.2!\n",
    "\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class PreActBlock(nn.Module):\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(PreActBlock, self).__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "\n",
    "        if stride != 1 or in_planes != planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, planes, kernel_size=1, stride=stride, bias=False)\n",
    "            )\n",
    "\n",
    "        # SE layers\n",
    "        self.fc1 = nn.Conv2d(planes, planes//16, kernel_size=1)\n",
    "        self.fc2 = nn.Conv2d(planes//16, planes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(x))\n",
    "        shortcut = self.shortcut(out) if hasattr(self, 'shortcut') else x\n",
    "        out = self.conv1(out)\n",
    "        out = self.conv2(F.relu(self.bn2(out)))\n",
    "\n",
    "        # Squeeze\n",
    "        w = F.avg_pool2d(out, out.size(2))\n",
    "        w = F.relu(self.fc1(w))\n",
    "        w = F.sigmoid(self.fc2(w))\n",
    "        # Excitation\n",
    "        out = out * w\n",
    "\n",
    "        out += shortcut\n",
    "        return out\n",
    "\n",
    "\n",
    "class SENet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(SENet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block,  64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(512, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "def SENet18():\n",
    "    return SENet(PreActBlock, [2,2,2,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, optim_type='UOptimizer', **optim_params):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.net = SENet18()\n",
    "\n",
    "        self._loss = None\n",
    "        \n",
    "        if optim_type == 'SGD':\n",
    "            self.optim = optim.SGD(self.parameters(), **optim_params)\n",
    "        elif optim_type == 'Adadelta':\n",
    "            self.optim = optim.Adadelta(self.parameters(), **optim_params)\n",
    "        elif optim_type == 'RMSProp':\n",
    "            self.optim = optim.RMSprop(self.parameters(), **optim_params)\n",
    "        elif optim_type == 'Adam':\n",
    "            self.optim = optim.Adam(self.parameters(), **optim_params)\n",
    "        elif optim_type == 'AdamW':\n",
    "            self.optim = AdamW(self.parameters(), **optim_params)\n",
    "        elif optim_type == 'Nadam':\n",
    "            self.optim = Nadam(self.parameters(), **optim_params)\n",
    "        elif optim_type == 'Adamax':\n",
    "            self.optim = optim.Adamax(self.parameters(), **optim_params)\n",
    "        elif optim_type == 'UOptimizer':\n",
    "            self.optim = UOptimizer(params = self.parameters(), **optim_params)\n",
    "    \n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "    def loss(self, output, target, **kwargs):\n",
    "        self._loss = F.cross_entropy(output, target, **kwargs)\n",
    "        self._correct = output.data.max(1, keepdim=True)[1]\n",
    "        self._correct = self._correct.eq(target.data.view_as(self._correct)).to(torch.float).cpu().mean()\n",
    "        return self._loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {'StandardAdam': Net('Adam', lr=3e-3, betas = (0.9, 0.99)).to(device),\n",
    "          'SGD':Net('SGD', lr=0.03, momentum=0.9).to(device),\n",
    "          'Adam_with_adadelta_coeff':Net('UOptimizer', \n",
    "                                         use_exp_avg_norm = True,\n",
    "                                         use_exp_avg_sq_norm = True,\n",
    "                                         use_adadelta_lr = True,\n",
    "                                         use_bias_correction = True,\n",
    "                                         lr=1).to(device)\n",
    "         }\n",
    "train_log = {k: [] for k in models}\n",
    "test_log = {k: [] for k in models}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(epoch, models, log=None):\n",
    "    train_size = len(train_loader.sampler)\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        for model in models.values():\n",
    "            model.optim.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = model.loss(output, target)\n",
    "            loss.backward()\n",
    "            model.optim.step()\n",
    "            \n",
    "        if batch_idx % 200 == 0:\n",
    "            line = 'Train Epoch: {} [{}/{} ({:.0f}%)]\\tLosses '.format(\n",
    "                epoch, batch_idx * len(data), train_size, 100. * batch_idx / len(train_loader))\n",
    "            losses = ' '.join(['{}: {:.4f}'.format(k, m._loss.item()) for k, m in models.items()])\n",
    "            print(line + losses)\n",
    "            \n",
    "    else:\n",
    "        batch_idx += 1\n",
    "        line = 'Train Epoch: {} [{}/{} ({:.0f}%)]\\tLosses '.format(\n",
    "            epoch, batch_idx * len(data), train_size, 100. * batch_idx / len(train_loader))\n",
    "        losses = ' '.join(['{}: {:.4f}'.format(k, m._loss.item()) for k, m in models.items()])\n",
    "        if log is not None:\n",
    "            for k in models:\n",
    "                log[k].append((models[k]._loss, models[k]._correct))\n",
    "        print(line + losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test(models, loader, log=None):\n",
    "    test_size = len(loader.sampler)\n",
    "    avg_lambda = lambda l: 'Loss: {:.4f}'.format(l)\n",
    "    acc_lambda = lambda c, p: 'Accuracy: {}/{} ({:.0f}%)'.format(c, test_size, p)\n",
    "    line = lambda i, l, c, p: '{}: '.format(i) + avg_lambda(l) + '\\t' + acc_lambda(c, p)\n",
    "\n",
    "    test_loss = {k: 0. for k in models}\n",
    "    correct = {k: 0. for k in models}\n",
    "    with torch.no_grad():\n",
    "        for data, target in loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = {k: m(data) for k, m in models.items()}\n",
    "            for k, m in models.items():\n",
    "                test_loss[k] += m.loss(output[k], target, size_average=False).item() # sum up batch loss\n",
    "                pred = output[k].data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "                correct[k] += pred.eq(target.data.view_as(pred)).cpu().sum().item()\n",
    "    \n",
    "    for k in models:\n",
    "        test_loss[k] /= test_size\n",
    "    correct_pct = {k: c / test_size for k, c in correct.items()}\n",
    "    lines = '\\n'.join([line(k, test_loss[k], correct[k], 100*correct_pct[k]) for k in models]) + '\\n'\n",
    "    report = 'Test set:\\n' + lines\n",
    "    if log is not None:\n",
    "        for k in models:\n",
    "            log[k].append((test_loss[k], correct_pct[k]))\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kvr\\AppData\\Local\\conda\\conda\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\functional.py:1006: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/40000 (0%)]\tLosses StandardAdam: 2.3164 SGD: 2.3247 Adam_with_adadelta_coeff: 2.2908\n",
      "Train Epoch: 1 [800/40000 (2%)]\tLosses StandardAdam: 2.2782 SGD: 2.3304 Adam_with_adadelta_coeff: 2.5816\n",
      "Train Epoch: 1 [1600/40000 (4%)]\tLosses StandardAdam: 2.2361 SGD: 2.2300 Adam_with_adadelta_coeff: 2.0909\n",
      "Train Epoch: 1 [2400/40000 (6%)]\tLosses StandardAdam: 2.4724 SGD: 2.2457 Adam_with_adadelta_coeff: 2.4447\n",
      "Train Epoch: 1 [3200/40000 (8%)]\tLosses StandardAdam: 2.0020 SGD: 2.3262 Adam_with_adadelta_coeff: 2.1742\n",
      "Train Epoch: 1 [4000/40000 (10%)]\tLosses StandardAdam: 3.2256 SGD: 3.3269 Adam_with_adadelta_coeff: 2.7421\n",
      "Train Epoch: 1 [4800/40000 (12%)]\tLosses StandardAdam: 2.1179 SGD: 1.8429 Adam_with_adadelta_coeff: 2.0241\n",
      "Train Epoch: 1 [5600/40000 (14%)]\tLosses StandardAdam: 2.4504 SGD: 2.0967 Adam_with_adadelta_coeff: 2.0459\n",
      "Train Epoch: 1 [6400/40000 (16%)]\tLosses StandardAdam: 2.1986 SGD: 2.5366 Adam_with_adadelta_coeff: 2.9173\n",
      "Train Epoch: 1 [7200/40000 (18%)]\tLosses StandardAdam: 1.6547 SGD: 1.3682 Adam_with_adadelta_coeff: 1.0617\n",
      "Train Epoch: 1 [8000/40000 (20%)]\tLosses StandardAdam: 1.7166 SGD: 2.0565 Adam_with_adadelta_coeff: 1.1753\n",
      "Train Epoch: 1 [8800/40000 (22%)]\tLosses StandardAdam: 2.1493 SGD: 2.2832 Adam_with_adadelta_coeff: 1.6497\n",
      "Train Epoch: 1 [9600/40000 (24%)]\tLosses StandardAdam: 1.9105 SGD: 2.1693 Adam_with_adadelta_coeff: 1.6765\n",
      "Train Epoch: 1 [10400/40000 (26%)]\tLosses StandardAdam: 1.4458 SGD: 1.7801 Adam_with_adadelta_coeff: 1.1543\n",
      "Train Epoch: 1 [11200/40000 (28%)]\tLosses StandardAdam: 1.6171 SGD: 2.1626 Adam_with_adadelta_coeff: 1.3206\n",
      "Train Epoch: 1 [12000/40000 (30%)]\tLosses StandardAdam: 1.7057 SGD: 2.1577 Adam_with_adadelta_coeff: 2.0789\n",
      "Train Epoch: 1 [12800/40000 (32%)]\tLosses StandardAdam: 1.8528 SGD: 2.3880 Adam_with_adadelta_coeff: 1.8699\n",
      "Train Epoch: 1 [13600/40000 (34%)]\tLosses StandardAdam: 0.9172 SGD: 1.1089 Adam_with_adadelta_coeff: 0.6908\n",
      "Train Epoch: 1 [14400/40000 (36%)]\tLosses StandardAdam: 2.8689 SGD: 2.2393 Adam_with_adadelta_coeff: 3.0599\n",
      "Train Epoch: 1 [15200/40000 (38%)]\tLosses StandardAdam: 1.2855 SGD: 1.3089 Adam_with_adadelta_coeff: 1.2226\n",
      "Train Epoch: 1 [16000/40000 (40%)]\tLosses StandardAdam: 1.6574 SGD: 1.5263 Adam_with_adadelta_coeff: 1.6509\n",
      "Train Epoch: 1 [16800/40000 (42%)]\tLosses StandardAdam: 1.2364 SGD: 1.6440 Adam_with_adadelta_coeff: 0.6141\n",
      "Train Epoch: 1 [17600/40000 (44%)]\tLosses StandardAdam: 1.5289 SGD: 1.3891 Adam_with_adadelta_coeff: 1.0074\n",
      "Train Epoch: 1 [18400/40000 (46%)]\tLosses StandardAdam: 1.7723 SGD: 2.0066 Adam_with_adadelta_coeff: 1.3725\n",
      "Train Epoch: 1 [19200/40000 (48%)]\tLosses StandardAdam: 2.1037 SGD: 1.9376 Adam_with_adadelta_coeff: 0.4529\n",
      "Train Epoch: 1 [20000/40000 (50%)]\tLosses StandardAdam: 2.4583 SGD: 2.3813 Adam_with_adadelta_coeff: 1.4822\n",
      "Train Epoch: 1 [20800/40000 (52%)]\tLosses StandardAdam: 1.3301 SGD: 1.5359 Adam_with_adadelta_coeff: 1.2698\n",
      "Train Epoch: 1 [21600/40000 (54%)]\tLosses StandardAdam: 1.1804 SGD: 1.6410 Adam_with_adadelta_coeff: 1.1293\n",
      "Train Epoch: 1 [22400/40000 (56%)]\tLosses StandardAdam: 1.3603 SGD: 1.5165 Adam_with_adadelta_coeff: 0.8078\n",
      "Train Epoch: 1 [23200/40000 (58%)]\tLosses StandardAdam: 2.3258 SGD: 2.0569 Adam_with_adadelta_coeff: 2.0733\n",
      "Train Epoch: 1 [24000/40000 (60%)]\tLosses StandardAdam: 1.5445 SGD: 1.4676 Adam_with_adadelta_coeff: 2.5580\n",
      "Train Epoch: 1 [24800/40000 (62%)]\tLosses StandardAdam: 0.9254 SGD: 0.7091 Adam_with_adadelta_coeff: 0.5247\n",
      "Train Epoch: 1 [25600/40000 (64%)]\tLosses StandardAdam: 1.8419 SGD: 2.3786 Adam_with_adadelta_coeff: 1.2398\n",
      "Train Epoch: 1 [26400/40000 (66%)]\tLosses StandardAdam: 1.5546 SGD: 2.1384 Adam_with_adadelta_coeff: 1.7814\n",
      "Train Epoch: 1 [27200/40000 (68%)]\tLosses StandardAdam: 1.6651 SGD: 1.8605 Adam_with_adadelta_coeff: 1.5300\n",
      "Train Epoch: 1 [28000/40000 (70%)]\tLosses StandardAdam: 1.1227 SGD: 1.7041 Adam_with_adadelta_coeff: 0.8453\n",
      "Train Epoch: 1 [28800/40000 (72%)]\tLosses StandardAdam: 1.9200 SGD: 1.6505 Adam_with_adadelta_coeff: 1.7964\n",
      "Train Epoch: 1 [29600/40000 (74%)]\tLosses StandardAdam: 1.2158 SGD: 1.6121 Adam_with_adadelta_coeff: 1.7576\n",
      "Train Epoch: 1 [30400/40000 (76%)]\tLosses StandardAdam: 1.1022 SGD: 1.4172 Adam_with_adadelta_coeff: 0.6194\n",
      "Train Epoch: 1 [31200/40000 (78%)]\tLosses StandardAdam: 0.9833 SGD: 0.8792 Adam_with_adadelta_coeff: 0.7467\n",
      "Train Epoch: 1 [32000/40000 (80%)]\tLosses StandardAdam: 0.9801 SGD: 0.9697 Adam_with_adadelta_coeff: 0.7280\n",
      "Train Epoch: 1 [32800/40000 (82%)]\tLosses StandardAdam: 1.3520 SGD: 1.4965 Adam_with_adadelta_coeff: 1.5800\n",
      "Train Epoch: 1 [33600/40000 (84%)]\tLosses StandardAdam: 1.1898 SGD: 1.0813 Adam_with_adadelta_coeff: 0.9438\n",
      "Train Epoch: 1 [34400/40000 (86%)]\tLosses StandardAdam: 1.2090 SGD: 1.0903 Adam_with_adadelta_coeff: 0.6676\n",
      "Train Epoch: 1 [35200/40000 (88%)]\tLosses StandardAdam: 1.3161 SGD: 1.2032 Adam_with_adadelta_coeff: 1.0441\n",
      "Train Epoch: 1 [36000/40000 (90%)]\tLosses StandardAdam: 1.2473 SGD: 0.9837 Adam_with_adadelta_coeff: 0.7172\n",
      "Train Epoch: 1 [36800/40000 (92%)]\tLosses StandardAdam: 1.2012 SGD: 1.6770 Adam_with_adadelta_coeff: 0.7883\n",
      "Train Epoch: 1 [37600/40000 (94%)]\tLosses StandardAdam: 1.4263 SGD: 2.2716 Adam_with_adadelta_coeff: 1.3294\n",
      "Train Epoch: 1 [38400/40000 (96%)]\tLosses StandardAdam: 1.3552 SGD: 1.3206 Adam_with_adadelta_coeff: 0.9448\n",
      "Train Epoch: 1 [39200/40000 (98%)]\tLosses StandardAdam: 1.4090 SGD: 1.1009 Adam_with_adadelta_coeff: 0.4864\n",
      "Train Epoch: 1 [40000/40000 (100%)]\tLosses StandardAdam: 1.2603 SGD: 1.1373 Adam_with_adadelta_coeff: 0.5596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kvr\\AppData\\Local\\conda\\conda\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\functional.py:52: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set:\n",
      "StandardAdam: Loss: 1.3552\tAccuracy: 5071.0/10000 (51%)\n",
      "SGD: Loss: 1.4203\tAccuracy: 4903.0/10000 (49%)\n",
      "Adam_with_adadelta_coeff: Loss: 0.9256\tAccuracy: 6756.0/10000 (68%)\n",
      "\n",
      "Train Epoch: 2 [0/40000 (0%)]\tLosses StandardAdam: 1.3047 SGD: 1.0675 Adam_with_adadelta_coeff: 0.6480\n",
      "Train Epoch: 2 [800/40000 (2%)]\tLosses StandardAdam: 1.2940 SGD: 1.7617 Adam_with_adadelta_coeff: 0.5044\n",
      "Train Epoch: 2 [1600/40000 (4%)]\tLosses StandardAdam: 2.9894 SGD: 2.8782 Adam_with_adadelta_coeff: 1.9823\n",
      "Train Epoch: 2 [2400/40000 (6%)]\tLosses StandardAdam: 1.2471 SGD: 1.4948 Adam_with_adadelta_coeff: 0.7449\n",
      "Train Epoch: 2 [3200/40000 (8%)]\tLosses StandardAdam: 1.1551 SGD: 1.1191 Adam_with_adadelta_coeff: 0.7955\n",
      "Train Epoch: 2 [4000/40000 (10%)]\tLosses StandardAdam: 0.7715 SGD: 0.4371 Adam_with_adadelta_coeff: 0.2231\n",
      "Train Epoch: 2 [4800/40000 (12%)]\tLosses StandardAdam: 1.2849 SGD: 1.6027 Adam_with_adadelta_coeff: 0.6715\n",
      "Train Epoch: 2 [5600/40000 (14%)]\tLosses StandardAdam: 0.9965 SGD: 0.9131 Adam_with_adadelta_coeff: 0.8568\n",
      "Train Epoch: 2 [6400/40000 (16%)]\tLosses StandardAdam: 1.7560 SGD: 2.2545 Adam_with_adadelta_coeff: 1.0144\n",
      "Train Epoch: 2 [7200/40000 (18%)]\tLosses StandardAdam: 0.7991 SGD: 0.6226 Adam_with_adadelta_coeff: 0.7285\n",
      "Train Epoch: 2 [8000/40000 (20%)]\tLosses StandardAdam: 1.3478 SGD: 1.3588 Adam_with_adadelta_coeff: 0.7050\n",
      "Train Epoch: 2 [8800/40000 (22%)]\tLosses StandardAdam: 1.7583 SGD: 2.0908 Adam_with_adadelta_coeff: 1.7573\n",
      "Train Epoch: 2 [9600/40000 (24%)]\tLosses StandardAdam: 2.1790 SGD: 2.2163 Adam_with_adadelta_coeff: 1.5410\n",
      "Train Epoch: 2 [10400/40000 (26%)]\tLosses StandardAdam: 0.9853 SGD: 0.9501 Adam_with_adadelta_coeff: 1.8096\n",
      "Train Epoch: 2 [11200/40000 (28%)]\tLosses StandardAdam: 2.1506 SGD: 1.8267 Adam_with_adadelta_coeff: 1.7638\n",
      "Train Epoch: 2 [12000/40000 (30%)]\tLosses StandardAdam: 1.7505 SGD: 1.3150 Adam_with_adadelta_coeff: 1.7884\n",
      "Train Epoch: 2 [12800/40000 (32%)]\tLosses StandardAdam: 1.1994 SGD: 0.7634 Adam_with_adadelta_coeff: 1.2484\n",
      "Train Epoch: 2 [13600/40000 (34%)]\tLosses StandardAdam: 0.4097 SGD: 0.7120 Adam_with_adadelta_coeff: 0.3834\n",
      "Train Epoch: 2 [14400/40000 (36%)]\tLosses StandardAdam: 1.4681 SGD: 1.3049 Adam_with_adadelta_coeff: 0.9693\n",
      "Train Epoch: 2 [15200/40000 (38%)]\tLosses StandardAdam: 1.8245 SGD: 1.1286 Adam_with_adadelta_coeff: 1.0975\n",
      "Train Epoch: 2 [16000/40000 (40%)]\tLosses StandardAdam: 1.2614 SGD: 2.0482 Adam_with_adadelta_coeff: 1.1013\n",
      "Train Epoch: 2 [16800/40000 (42%)]\tLosses StandardAdam: 1.1528 SGD: 1.2949 Adam_with_adadelta_coeff: 1.2669\n",
      "Train Epoch: 2 [17600/40000 (44%)]\tLosses StandardAdam: 2.3508 SGD: 1.5787 Adam_with_adadelta_coeff: 1.6441\n",
      "Train Epoch: 2 [18400/40000 (46%)]\tLosses StandardAdam: 1.2872 SGD: 1.1088 Adam_with_adadelta_coeff: 0.4221\n",
      "Train Epoch: 2 [19200/40000 (48%)]\tLosses StandardAdam: 1.1328 SGD: 1.5445 Adam_with_adadelta_coeff: 0.4525\n",
      "Train Epoch: 2 [20000/40000 (50%)]\tLosses StandardAdam: 1.4875 SGD: 0.9369 Adam_with_adadelta_coeff: 0.9654\n",
      "Train Epoch: 2 [20800/40000 (52%)]\tLosses StandardAdam: 0.4307 SGD: 0.6201 Adam_with_adadelta_coeff: 0.6472\n",
      "Train Epoch: 2 [21600/40000 (54%)]\tLosses StandardAdam: 1.5114 SGD: 1.8164 Adam_with_adadelta_coeff: 1.2519\n",
      "Train Epoch: 2 [22400/40000 (56%)]\tLosses StandardAdam: 0.9895 SGD: 1.1709 Adam_with_adadelta_coeff: 0.6173\n",
      "Train Epoch: 2 [23200/40000 (58%)]\tLosses StandardAdam: 0.8133 SGD: 0.8685 Adam_with_adadelta_coeff: 0.9680\n",
      "Train Epoch: 2 [24000/40000 (60%)]\tLosses StandardAdam: 0.4107 SGD: 0.5287 Adam_with_adadelta_coeff: 0.5320\n",
      "Train Epoch: 2 [24800/40000 (62%)]\tLosses StandardAdam: 1.2942 SGD: 0.9963 Adam_with_adadelta_coeff: 0.5676\n",
      "Train Epoch: 2 [25600/40000 (64%)]\tLosses StandardAdam: 1.0724 SGD: 1.2571 Adam_with_adadelta_coeff: 0.6484\n",
      "Train Epoch: 2 [26400/40000 (66%)]\tLosses StandardAdam: 2.0472 SGD: 2.2550 Adam_with_adadelta_coeff: 2.3276\n",
      "Train Epoch: 2 [27200/40000 (68%)]\tLosses StandardAdam: 0.6458 SGD: 0.7914 Adam_with_adadelta_coeff: 0.8620\n",
      "Train Epoch: 2 [28000/40000 (70%)]\tLosses StandardAdam: 0.9746 SGD: 1.0091 Adam_with_adadelta_coeff: 1.4604\n",
      "Train Epoch: 2 [28800/40000 (72%)]\tLosses StandardAdam: 1.1166 SGD: 1.2147 Adam_with_adadelta_coeff: 1.0429\n",
      "Train Epoch: 2 [29600/40000 (74%)]\tLosses StandardAdam: 1.1651 SGD: 1.1449 Adam_with_adadelta_coeff: 1.5170\n",
      "Train Epoch: 2 [30400/40000 (76%)]\tLosses StandardAdam: 0.7380 SGD: 0.7187 Adam_with_adadelta_coeff: 0.1551\n",
      "Train Epoch: 2 [31200/40000 (78%)]\tLosses StandardAdam: 1.0130 SGD: 1.6348 Adam_with_adadelta_coeff: 1.5017\n",
      "Train Epoch: 2 [32000/40000 (80%)]\tLosses StandardAdam: 1.0512 SGD: 1.0933 Adam_with_adadelta_coeff: 0.4274\n",
      "Train Epoch: 2 [32800/40000 (82%)]\tLosses StandardAdam: 1.4784 SGD: 1.6296 Adam_with_adadelta_coeff: 0.8926\n",
      "Train Epoch: 2 [33600/40000 (84%)]\tLosses StandardAdam: 1.1795 SGD: 0.6967 Adam_with_adadelta_coeff: 1.3325\n",
      "Train Epoch: 2 [34400/40000 (86%)]\tLosses StandardAdam: 0.8457 SGD: 1.0128 Adam_with_adadelta_coeff: 0.1892\n",
      "Train Epoch: 2 [35200/40000 (88%)]\tLosses StandardAdam: 1.1279 SGD: 1.5255 Adam_with_adadelta_coeff: 0.8251\n",
      "Train Epoch: 2 [36000/40000 (90%)]\tLosses StandardAdam: 1.0232 SGD: 1.1023 Adam_with_adadelta_coeff: 0.0861\n",
      "Train Epoch: 2 [36800/40000 (92%)]\tLosses StandardAdam: 2.4793 SGD: 1.6867 Adam_with_adadelta_coeff: 0.9836\n",
      "Train Epoch: 2 [37600/40000 (94%)]\tLosses StandardAdam: 1.3094 SGD: 0.9148 Adam_with_adadelta_coeff: 1.3969\n",
      "Train Epoch: 2 [38400/40000 (96%)]\tLosses StandardAdam: 0.5519 SGD: 1.0966 Adam_with_adadelta_coeff: 0.3260\n",
      "Train Epoch: 2 [39200/40000 (98%)]\tLosses StandardAdam: 1.7234 SGD: 1.1617 Adam_with_adadelta_coeff: 0.4295\n",
      "Train Epoch: 2 [40000/40000 (100%)]\tLosses StandardAdam: 0.5452 SGD: 1.5305 Adam_with_adadelta_coeff: 0.9412\n",
      "Test set:\n",
      "StandardAdam: Loss: 1.0810\tAccuracy: 6218.0/10000 (62%)\n",
      "SGD: Loss: 1.0641\tAccuracy: 6253.0/10000 (63%)\n",
      "Adam_with_adadelta_coeff: Loss: 0.6456\tAccuracy: 7721.0/10000 (77%)\n",
      "\n",
      "Train Epoch: 3 [0/40000 (0%)]\tLosses StandardAdam: 0.3401 SGD: 0.5051 Adam_with_adadelta_coeff: 0.1985\n",
      "Train Epoch: 3 [800/40000 (2%)]\tLosses StandardAdam: 0.7617 SGD: 0.5225 Adam_with_adadelta_coeff: 0.1772\n",
      "Train Epoch: 3 [1600/40000 (4%)]\tLosses StandardAdam: 1.4266 SGD: 1.9949 Adam_with_adadelta_coeff: 1.5384\n",
      "Train Epoch: 3 [2400/40000 (6%)]\tLosses StandardAdam: 0.7645 SGD: 1.0899 Adam_with_adadelta_coeff: 0.7802\n",
      "Train Epoch: 3 [3200/40000 (8%)]\tLosses StandardAdam: 1.7950 SGD: 1.4826 Adam_with_adadelta_coeff: 1.9275\n",
      "Train Epoch: 3 [4000/40000 (10%)]\tLosses StandardAdam: 0.8335 SGD: 0.9890 Adam_with_adadelta_coeff: 0.5493\n",
      "Train Epoch: 3 [4800/40000 (12%)]\tLosses StandardAdam: 1.0222 SGD: 1.5707 Adam_with_adadelta_coeff: 0.8670\n",
      "Train Epoch: 3 [5600/40000 (14%)]\tLosses StandardAdam: 0.7257 SGD: 0.8502 Adam_with_adadelta_coeff: 0.5053\n",
      "Train Epoch: 3 [6400/40000 (16%)]\tLosses StandardAdam: 1.0508 SGD: 0.6458 Adam_with_adadelta_coeff: 0.8986\n",
      "Train Epoch: 3 [7200/40000 (18%)]\tLosses StandardAdam: 0.9548 SGD: 0.7775 Adam_with_adadelta_coeff: 1.1900\n",
      "Train Epoch: 3 [8000/40000 (20%)]\tLosses StandardAdam: 1.4490 SGD: 1.5281 Adam_with_adadelta_coeff: 1.2406\n",
      "Train Epoch: 3 [8800/40000 (22%)]\tLosses StandardAdam: 0.3507 SGD: 0.6032 Adam_with_adadelta_coeff: 0.5774\n",
      "Train Epoch: 3 [9600/40000 (24%)]\tLosses StandardAdam: 0.8818 SGD: 0.9411 Adam_with_adadelta_coeff: 0.6201\n",
      "Train Epoch: 3 [10400/40000 (26%)]\tLosses StandardAdam: 0.7100 SGD: 0.6721 Adam_with_adadelta_coeff: 0.2416\n",
      "Train Epoch: 3 [11200/40000 (28%)]\tLosses StandardAdam: 0.1495 SGD: 0.2299 Adam_with_adadelta_coeff: 0.0886\n",
      "Train Epoch: 3 [12000/40000 (30%)]\tLosses StandardAdam: 1.4964 SGD: 1.8254 Adam_with_adadelta_coeff: 1.4435\n",
      "Train Epoch: 3 [12800/40000 (32%)]\tLosses StandardAdam: 0.2087 SGD: 0.3405 Adam_with_adadelta_coeff: 0.1707\n",
      "Train Epoch: 3 [13600/40000 (34%)]\tLosses StandardAdam: 1.4553 SGD: 1.6892 Adam_with_adadelta_coeff: 0.6150\n",
      "Train Epoch: 3 [14400/40000 (36%)]\tLosses StandardAdam: 0.9441 SGD: 0.5090 Adam_with_adadelta_coeff: 0.5323\n",
      "Train Epoch: 3 [15200/40000 (38%)]\tLosses StandardAdam: 1.6625 SGD: 2.7904 Adam_with_adadelta_coeff: 1.9599\n",
      "Train Epoch: 3 [16000/40000 (40%)]\tLosses StandardAdam: 1.1573 SGD: 1.4741 Adam_with_adadelta_coeff: 0.9778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 3 [16800/40000 (42%)]\tLosses StandardAdam: 1.8031 SGD: 1.5188 Adam_with_adadelta_coeff: 1.9320\n",
      "Train Epoch: 3 [17600/40000 (44%)]\tLosses StandardAdam: 0.7944 SGD: 0.9649 Adam_with_adadelta_coeff: 0.7494\n",
      "Train Epoch: 3 [18400/40000 (46%)]\tLosses StandardAdam: 0.3097 SGD: 1.1923 Adam_with_adadelta_coeff: 0.3462\n",
      "Train Epoch: 3 [19200/40000 (48%)]\tLosses StandardAdam: 0.4293 SGD: 0.4295 Adam_with_adadelta_coeff: 0.7067\n",
      "Train Epoch: 3 [20000/40000 (50%)]\tLosses StandardAdam: 0.9370 SGD: 0.8650 Adam_with_adadelta_coeff: 1.0029\n",
      "Train Epoch: 3 [20800/40000 (52%)]\tLosses StandardAdam: 1.2985 SGD: 0.5658 Adam_with_adadelta_coeff: 1.2115\n",
      "Train Epoch: 3 [21600/40000 (54%)]\tLosses StandardAdam: 2.1549 SGD: 2.0260 Adam_with_adadelta_coeff: 1.0817\n",
      "Train Epoch: 3 [22400/40000 (56%)]\tLosses StandardAdam: 0.4929 SGD: 0.4148 Adam_with_adadelta_coeff: 0.3728\n",
      "Train Epoch: 3 [23200/40000 (58%)]\tLosses StandardAdam: 1.6526 SGD: 1.7170 Adam_with_adadelta_coeff: 1.3707\n",
      "Train Epoch: 3 [24000/40000 (60%)]\tLosses StandardAdam: 1.2681 SGD: 1.4865 Adam_with_adadelta_coeff: 1.2243\n",
      "Train Epoch: 3 [24800/40000 (62%)]\tLosses StandardAdam: 0.6134 SGD: 0.7706 Adam_with_adadelta_coeff: 0.6430\n",
      "Train Epoch: 3 [25600/40000 (64%)]\tLosses StandardAdam: 1.5994 SGD: 1.9146 Adam_with_adadelta_coeff: 1.2606\n",
      "Train Epoch: 3 [26400/40000 (66%)]\tLosses StandardAdam: 0.8513 SGD: 0.6227 Adam_with_adadelta_coeff: 0.8947\n",
      "Train Epoch: 3 [27200/40000 (68%)]\tLosses StandardAdam: 0.5581 SGD: 0.8624 Adam_with_adadelta_coeff: 0.7890\n",
      "Train Epoch: 3 [28000/40000 (70%)]\tLosses StandardAdam: 1.1848 SGD: 1.2847 Adam_with_adadelta_coeff: 1.3788\n",
      "Train Epoch: 3 [28800/40000 (72%)]\tLosses StandardAdam: 1.4169 SGD: 0.9739 Adam_with_adadelta_coeff: 0.7121\n",
      "Train Epoch: 3 [29600/40000 (74%)]\tLosses StandardAdam: 1.3037 SGD: 1.1072 Adam_with_adadelta_coeff: 1.3017\n",
      "Train Epoch: 3 [30400/40000 (76%)]\tLosses StandardAdam: 0.6288 SGD: 0.0961 Adam_with_adadelta_coeff: 0.0458\n",
      "Train Epoch: 3 [31200/40000 (78%)]\tLosses StandardAdam: 0.7200 SGD: 0.5402 Adam_with_adadelta_coeff: 0.5326\n",
      "Train Epoch: 3 [32000/40000 (80%)]\tLosses StandardAdam: 0.3026 SGD: 0.0376 Adam_with_adadelta_coeff: 0.0157\n",
      "Train Epoch: 3 [32800/40000 (82%)]\tLosses StandardAdam: 1.1786 SGD: 0.9284 Adam_with_adadelta_coeff: 1.2206\n",
      "Train Epoch: 3 [33600/40000 (84%)]\tLosses StandardAdam: 0.2103 SGD: 0.2779 Adam_with_adadelta_coeff: 0.0561\n",
      "Train Epoch: 3 [34400/40000 (86%)]\tLosses StandardAdam: 1.4720 SGD: 1.6108 Adam_with_adadelta_coeff: 0.9450\n",
      "Train Epoch: 3 [35200/40000 (88%)]\tLosses StandardAdam: 0.6835 SGD: 0.5388 Adam_with_adadelta_coeff: 0.7173\n",
      "Train Epoch: 3 [36000/40000 (90%)]\tLosses StandardAdam: 0.4629 SGD: 0.5441 Adam_with_adadelta_coeff: 0.3475\n",
      "Train Epoch: 3 [36800/40000 (92%)]\tLosses StandardAdam: 0.7320 SGD: 0.2104 Adam_with_adadelta_coeff: 0.0958\n",
      "Train Epoch: 3 [37600/40000 (94%)]\tLosses StandardAdam: 0.0907 SGD: 0.1539 Adam_with_adadelta_coeff: 0.0325\n",
      "Train Epoch: 3 [38400/40000 (96%)]\tLosses StandardAdam: 0.3061 SGD: 0.0972 Adam_with_adadelta_coeff: 0.2054\n",
      "Train Epoch: 3 [39200/40000 (98%)]\tLosses StandardAdam: 1.0649 SGD: 1.0120 Adam_with_adadelta_coeff: 0.4213\n",
      "Train Epoch: 3 [40000/40000 (100%)]\tLosses StandardAdam: 0.6857 SGD: 0.8008 Adam_with_adadelta_coeff: 0.3904\n",
      "Test set:\n",
      "StandardAdam: Loss: 0.9494\tAccuracy: 6619.0/10000 (66%)\n",
      "SGD: Loss: 0.8341\tAccuracy: 7046.0/10000 (70%)\n",
      "Adam_with_adadelta_coeff: Loss: 0.6530\tAccuracy: 7686.0/10000 (77%)\n",
      "\n",
      "Train Epoch: 4 [0/40000 (0%)]\tLosses StandardAdam: 0.3419 SGD: 0.1200 Adam_with_adadelta_coeff: 0.0807\n",
      "Train Epoch: 4 [800/40000 (2%)]\tLosses StandardAdam: 1.4376 SGD: 1.4756 Adam_with_adadelta_coeff: 0.9235\n",
      "Train Epoch: 4 [1600/40000 (4%)]\tLosses StandardAdam: 2.0788 SGD: 0.7436 Adam_with_adadelta_coeff: 0.7445\n",
      "Train Epoch: 4 [2400/40000 (6%)]\tLosses StandardAdam: 1.1061 SGD: 1.1821 Adam_with_adadelta_coeff: 0.3741\n",
      "Train Epoch: 4 [3200/40000 (8%)]\tLosses StandardAdam: 0.4914 SGD: 0.4521 Adam_with_adadelta_coeff: 0.6722\n",
      "Train Epoch: 4 [4000/40000 (10%)]\tLosses StandardAdam: 1.4031 SGD: 1.8340 Adam_with_adadelta_coeff: 0.6867\n",
      "Train Epoch: 4 [4800/40000 (12%)]\tLosses StandardAdam: 0.6540 SGD: 0.4301 Adam_with_adadelta_coeff: 0.2185\n",
      "Train Epoch: 4 [5600/40000 (14%)]\tLosses StandardAdam: 0.3448 SGD: 0.3966 Adam_with_adadelta_coeff: 0.1029\n",
      "Train Epoch: 4 [6400/40000 (16%)]\tLosses StandardAdam: 0.3089 SGD: 0.3451 Adam_with_adadelta_coeff: 0.0599\n",
      "Train Epoch: 4 [7200/40000 (18%)]\tLosses StandardAdam: 0.3085 SGD: 0.1996 Adam_with_adadelta_coeff: 0.1430\n",
      "Train Epoch: 4 [8000/40000 (20%)]\tLosses StandardAdam: 0.4256 SGD: 0.2273 Adam_with_adadelta_coeff: 0.2344\n",
      "Train Epoch: 4 [8800/40000 (22%)]\tLosses StandardAdam: 0.6195 SGD: 0.2462 Adam_with_adadelta_coeff: 0.1212\n",
      "Train Epoch: 4 [9600/40000 (24%)]\tLosses StandardAdam: 0.2372 SGD: 0.3081 Adam_with_adadelta_coeff: 0.1359\n",
      "Train Epoch: 4 [10400/40000 (26%)]\tLosses StandardAdam: 0.8464 SGD: 1.3345 Adam_with_adadelta_coeff: 0.3185\n",
      "Train Epoch: 4 [11200/40000 (28%)]\tLosses StandardAdam: 1.5098 SGD: 1.0285 Adam_with_adadelta_coeff: 0.3184\n",
      "Train Epoch: 4 [12000/40000 (30%)]\tLosses StandardAdam: 0.2254 SGD: 0.2318 Adam_with_adadelta_coeff: 0.5331\n",
      "Train Epoch: 4 [12800/40000 (32%)]\tLosses StandardAdam: 0.3436 SGD: 0.4916 Adam_with_adadelta_coeff: 0.3190\n",
      "Train Epoch: 4 [13600/40000 (34%)]\tLosses StandardAdam: 1.5243 SGD: 0.7663 Adam_with_adadelta_coeff: 0.4951\n",
      "Train Epoch: 4 [14400/40000 (36%)]\tLosses StandardAdam: 0.8570 SGD: 0.7353 Adam_with_adadelta_coeff: 0.5638\n",
      "Train Epoch: 4 [15200/40000 (38%)]\tLosses StandardAdam: 1.0646 SGD: 1.2798 Adam_with_adadelta_coeff: 0.1029\n",
      "Train Epoch: 4 [16000/40000 (40%)]\tLosses StandardAdam: 0.8420 SGD: 0.5427 Adam_with_adadelta_coeff: 0.1792\n",
      "Train Epoch: 4 [16800/40000 (42%)]\tLosses StandardAdam: 0.2873 SGD: 0.2377 Adam_with_adadelta_coeff: 0.1413\n",
      "Train Epoch: 4 [17600/40000 (44%)]\tLosses StandardAdam: 1.8531 SGD: 1.0659 Adam_with_adadelta_coeff: 1.1890\n",
      "Train Epoch: 4 [18400/40000 (46%)]\tLosses StandardAdam: 0.9992 SGD: 1.2585 Adam_with_adadelta_coeff: 0.4753\n",
      "Train Epoch: 4 [19200/40000 (48%)]\tLosses StandardAdam: 0.4149 SGD: 1.3846 Adam_with_adadelta_coeff: 0.0894\n",
      "Train Epoch: 4 [20000/40000 (50%)]\tLosses StandardAdam: 0.4685 SGD: 0.5706 Adam_with_adadelta_coeff: 0.2126\n",
      "Train Epoch: 4 [20800/40000 (52%)]\tLosses StandardAdam: 1.5412 SGD: 0.4033 Adam_with_adadelta_coeff: 0.4456\n",
      "Train Epoch: 4 [21600/40000 (54%)]\tLosses StandardAdam: 0.6166 SGD: 0.6697 Adam_with_adadelta_coeff: 0.3448\n",
      "Train Epoch: 4 [22400/40000 (56%)]\tLosses StandardAdam: 0.9713 SGD: 1.1904 Adam_with_adadelta_coeff: 0.6295\n",
      "Train Epoch: 4 [23200/40000 (58%)]\tLosses StandardAdam: 1.6406 SGD: 0.7705 Adam_with_adadelta_coeff: 0.8151\n",
      "Train Epoch: 4 [24000/40000 (60%)]\tLosses StandardAdam: 0.8251 SGD: 0.4789 Adam_with_adadelta_coeff: 0.1056\n",
      "Train Epoch: 4 [24800/40000 (62%)]\tLosses StandardAdam: 0.2005 SGD: 0.1022 Adam_with_adadelta_coeff: 0.0907\n",
      "Train Epoch: 4 [25600/40000 (64%)]\tLosses StandardAdam: 0.3380 SGD: 0.5402 Adam_with_adadelta_coeff: 0.3624\n",
      "Train Epoch: 4 [26400/40000 (66%)]\tLosses StandardAdam: 1.0306 SGD: 0.2787 Adam_with_adadelta_coeff: 0.1672\n",
      "Train Epoch: 4 [27200/40000 (68%)]\tLosses StandardAdam: 0.5973 SGD: 0.4166 Adam_with_adadelta_coeff: 0.1323\n",
      "Train Epoch: 4 [28000/40000 (70%)]\tLosses StandardAdam: 0.9002 SGD: 1.6829 Adam_with_adadelta_coeff: 0.2754\n",
      "Train Epoch: 4 [28800/40000 (72%)]\tLosses StandardAdam: 0.7585 SGD: 1.4230 Adam_with_adadelta_coeff: 0.7808\n",
      "Train Epoch: 4 [29600/40000 (74%)]\tLosses StandardAdam: 0.8734 SGD: 0.4236 Adam_with_adadelta_coeff: 0.3047\n",
      "Train Epoch: 4 [30400/40000 (76%)]\tLosses StandardAdam: 0.2036 SGD: 0.1597 Adam_with_adadelta_coeff: 0.3547\n",
      "Train Epoch: 4 [31200/40000 (78%)]\tLosses StandardAdam: 1.0860 SGD: 0.9126 Adam_with_adadelta_coeff: 0.7700\n",
      "Train Epoch: 4 [32000/40000 (80%)]\tLosses StandardAdam: 0.8798 SGD: 1.2896 Adam_with_adadelta_coeff: 0.6699\n",
      "Train Epoch: 4 [32800/40000 (82%)]\tLosses StandardAdam: 0.3828 SGD: 0.9676 Adam_with_adadelta_coeff: 0.8485\n",
      "Train Epoch: 4 [33600/40000 (84%)]\tLosses StandardAdam: 0.3678 SGD: 0.1374 Adam_with_adadelta_coeff: 0.1435\n",
      "Train Epoch: 4 [34400/40000 (86%)]\tLosses StandardAdam: 0.9321 SGD: 1.5199 Adam_with_adadelta_coeff: 0.6673\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 4 [35200/40000 (88%)]\tLosses StandardAdam: 0.8785 SGD: 0.7533 Adam_with_adadelta_coeff: 0.7958\n",
      "Train Epoch: 4 [36000/40000 (90%)]\tLosses StandardAdam: 1.1610 SGD: 1.3110 Adam_with_adadelta_coeff: 0.5680\n",
      "Train Epoch: 4 [36800/40000 (92%)]\tLosses StandardAdam: 0.5618 SGD: 0.3767 Adam_with_adadelta_coeff: 0.5786\n",
      "Train Epoch: 4 [37600/40000 (94%)]\tLosses StandardAdam: 0.9962 SGD: 1.2815 Adam_with_adadelta_coeff: 1.3047\n",
      "Train Epoch: 4 [38400/40000 (96%)]\tLosses StandardAdam: 0.7176 SGD: 0.4947 Adam_with_adadelta_coeff: 0.4084\n",
      "Train Epoch: 4 [39200/40000 (98%)]\tLosses StandardAdam: 0.6891 SGD: 0.8712 Adam_with_adadelta_coeff: 0.7985\n",
      "Train Epoch: 4 [40000/40000 (100%)]\tLosses StandardAdam: 0.3723 SGD: 1.0309 Adam_with_adadelta_coeff: 0.2394\n",
      "Test set:\n",
      "StandardAdam: Loss: 0.8695\tAccuracy: 7011.0/10000 (70%)\n",
      "SGD: Loss: 0.7061\tAccuracy: 7566.0/10000 (76%)\n",
      "Adam_with_adadelta_coeff: Loss: 0.5806\tAccuracy: 8034.0/10000 (80%)\n",
      "\n",
      "Train Epoch: 5 [0/40000 (0%)]\tLosses StandardAdam: 0.2321 SGD: 0.0808 Adam_with_adadelta_coeff: 0.0410\n",
      "Train Epoch: 5 [800/40000 (2%)]\tLosses StandardAdam: 1.6291 SGD: 2.1744 Adam_with_adadelta_coeff: 0.3775\n",
      "Train Epoch: 5 [1600/40000 (4%)]\tLosses StandardAdam: 0.4783 SGD: 0.1503 Adam_with_adadelta_coeff: 0.7392\n",
      "Train Epoch: 5 [2400/40000 (6%)]\tLosses StandardAdam: 1.0324 SGD: 0.8272 Adam_with_adadelta_coeff: 0.8414\n",
      "Train Epoch: 5 [3200/40000 (8%)]\tLosses StandardAdam: 1.2416 SGD: 0.3116 Adam_with_adadelta_coeff: 0.8523\n",
      "Train Epoch: 5 [4000/40000 (10%)]\tLosses StandardAdam: 0.1272 SGD: 0.0770 Adam_with_adadelta_coeff: 0.0093\n",
      "Train Epoch: 5 [4800/40000 (12%)]\tLosses StandardAdam: 0.4963 SGD: 0.6536 Adam_with_adadelta_coeff: 0.2009\n",
      "Train Epoch: 5 [5600/40000 (14%)]\tLosses StandardAdam: 0.6750 SGD: 0.5355 Adam_with_adadelta_coeff: 0.3936\n",
      "Train Epoch: 5 [6400/40000 (16%)]\tLosses StandardAdam: 1.0788 SGD: 1.6019 Adam_with_adadelta_coeff: 0.7651\n",
      "Train Epoch: 5 [7200/40000 (18%)]\tLosses StandardAdam: 1.1302 SGD: 0.9974 Adam_with_adadelta_coeff: 1.3961\n",
      "Train Epoch: 5 [8000/40000 (20%)]\tLosses StandardAdam: 0.2198 SGD: 0.0781 Adam_with_adadelta_coeff: 0.0546\n",
      "Train Epoch: 5 [8800/40000 (22%)]\tLosses StandardAdam: 1.2682 SGD: 1.0311 Adam_with_adadelta_coeff: 0.8741\n",
      "Train Epoch: 5 [9600/40000 (24%)]\tLosses StandardAdam: 0.3320 SGD: 0.1266 Adam_with_adadelta_coeff: 0.1236\n",
      "Train Epoch: 5 [10400/40000 (26%)]\tLosses StandardAdam: 0.1953 SGD: 0.0613 Adam_with_adadelta_coeff: 0.1221\n",
      "Train Epoch: 5 [11200/40000 (28%)]\tLosses StandardAdam: 0.2996 SGD: 0.5115 Adam_with_adadelta_coeff: 0.1236\n",
      "Train Epoch: 5 [12000/40000 (30%)]\tLosses StandardAdam: 0.7753 SGD: 0.9767 Adam_with_adadelta_coeff: 0.1057\n",
      "Train Epoch: 5 [12800/40000 (32%)]\tLosses StandardAdam: 0.4685 SGD: 0.4399 Adam_with_adadelta_coeff: 0.0417\n",
      "Train Epoch: 5 [13600/40000 (34%)]\tLosses StandardAdam: 0.6255 SGD: 0.7011 Adam_with_adadelta_coeff: 0.0729\n",
      "Train Epoch: 5 [14400/40000 (36%)]\tLosses StandardAdam: 0.1846 SGD: 0.2419 Adam_with_adadelta_coeff: 0.0814\n",
      "Train Epoch: 5 [15200/40000 (38%)]\tLosses StandardAdam: 0.3288 SGD: 0.5357 Adam_with_adadelta_coeff: 1.4224\n",
      "Train Epoch: 5 [16000/40000 (40%)]\tLosses StandardAdam: 0.4885 SGD: 0.7920 Adam_with_adadelta_coeff: 0.8512\n",
      "Train Epoch: 5 [16800/40000 (42%)]\tLosses StandardAdam: 2.0682 SGD: 0.3663 Adam_with_adadelta_coeff: 0.2779\n",
      "Train Epoch: 5 [17600/40000 (44%)]\tLosses StandardAdam: 0.4546 SGD: 0.6451 Adam_with_adadelta_coeff: 0.1432\n",
      "Train Epoch: 5 [18400/40000 (46%)]\tLosses StandardAdam: 0.4723 SGD: 0.7493 Adam_with_adadelta_coeff: 0.4852\n",
      "Train Epoch: 5 [19200/40000 (48%)]\tLosses StandardAdam: 0.9733 SGD: 0.7562 Adam_with_adadelta_coeff: 0.4369\n",
      "Train Epoch: 5 [20000/40000 (50%)]\tLosses StandardAdam: 3.4138 SGD: 3.1507 Adam_with_adadelta_coeff: 1.3925\n",
      "Train Epoch: 5 [20800/40000 (52%)]\tLosses StandardAdam: 2.5830 SGD: 2.4096 Adam_with_adadelta_coeff: 2.7412\n",
      "Train Epoch: 5 [21600/40000 (54%)]\tLosses StandardAdam: 0.7179 SGD: 0.8573 Adam_with_adadelta_coeff: 0.4676\n",
      "Train Epoch: 5 [22400/40000 (56%)]\tLosses StandardAdam: 0.5453 SGD: 1.1189 Adam_with_adadelta_coeff: 0.2022\n",
      "Train Epoch: 5 [23200/40000 (58%)]\tLosses StandardAdam: 0.8034 SGD: 0.6293 Adam_with_adadelta_coeff: 0.8176\n",
      "Train Epoch: 5 [24000/40000 (60%)]\tLosses StandardAdam: 1.6153 SGD: 1.1999 Adam_with_adadelta_coeff: 0.8589\n",
      "Train Epoch: 5 [24800/40000 (62%)]\tLosses StandardAdam: 0.1837 SGD: 0.1652 Adam_with_adadelta_coeff: 0.0258\n",
      "Train Epoch: 5 [25600/40000 (64%)]\tLosses StandardAdam: 1.1844 SGD: 0.6792 Adam_with_adadelta_coeff: 0.0674\n",
      "Train Epoch: 5 [26400/40000 (66%)]\tLosses StandardAdam: 0.4627 SGD: 0.2675 Adam_with_adadelta_coeff: 0.5637\n",
      "Train Epoch: 5 [27200/40000 (68%)]\tLosses StandardAdam: 0.6671 SGD: 0.7813 Adam_with_adadelta_coeff: 0.7314\n",
      "Train Epoch: 5 [28000/40000 (70%)]\tLosses StandardAdam: 0.5592 SGD: 1.2079 Adam_with_adadelta_coeff: 0.8675\n",
      "Train Epoch: 5 [28800/40000 (72%)]\tLosses StandardAdam: 0.9879 SGD: 1.0886 Adam_with_adadelta_coeff: 0.3337\n",
      "Train Epoch: 5 [29600/40000 (74%)]\tLosses StandardAdam: 0.1883 SGD: 0.0424 Adam_with_adadelta_coeff: 0.0039\n",
      "Train Epoch: 5 [30400/40000 (76%)]\tLosses StandardAdam: 0.5068 SGD: 0.3285 Adam_with_adadelta_coeff: 0.2292\n",
      "Train Epoch: 5 [31200/40000 (78%)]\tLosses StandardAdam: 0.7983 SGD: 0.8418 Adam_with_adadelta_coeff: 0.7736\n",
      "Train Epoch: 5 [32000/40000 (80%)]\tLosses StandardAdam: 0.2228 SGD: 0.2661 Adam_with_adadelta_coeff: 0.0853\n",
      "Train Epoch: 5 [32800/40000 (82%)]\tLosses StandardAdam: 0.2029 SGD: 0.1443 Adam_with_adadelta_coeff: 0.0472\n",
      "Train Epoch: 5 [33600/40000 (84%)]\tLosses StandardAdam: 1.6325 SGD: 0.7300 Adam_with_adadelta_coeff: 0.4907\n",
      "Train Epoch: 5 [34400/40000 (86%)]\tLosses StandardAdam: 0.2488 SGD: 0.4882 Adam_with_adadelta_coeff: 0.1335\n",
      "Train Epoch: 5 [35200/40000 (88%)]\tLosses StandardAdam: 1.8109 SGD: 1.0094 Adam_with_adadelta_coeff: 1.0710\n",
      "Train Epoch: 5 [36000/40000 (90%)]\tLosses StandardAdam: 0.2326 SGD: 0.3941 Adam_with_adadelta_coeff: 0.0428\n",
      "Train Epoch: 5 [36800/40000 (92%)]\tLosses StandardAdam: 0.2795 SGD: 0.5958 Adam_with_adadelta_coeff: 0.2478\n",
      "Train Epoch: 5 [37600/40000 (94%)]\tLosses StandardAdam: 0.1366 SGD: 0.0755 Adam_with_adadelta_coeff: 0.1187\n",
      "Train Epoch: 5 [38400/40000 (96%)]\tLosses StandardAdam: 2.7261 SGD: 0.8890 Adam_with_adadelta_coeff: 0.5493\n",
      "Train Epoch: 5 [39200/40000 (98%)]\tLosses StandardAdam: 1.5532 SGD: 0.7972 Adam_with_adadelta_coeff: 0.4995\n",
      "Train Epoch: 5 [40000/40000 (100%)]\tLosses StandardAdam: 0.9326 SGD: 1.5340 Adam_with_adadelta_coeff: 0.7815\n",
      "Test set:\n",
      "StandardAdam: Loss: 0.8299\tAccuracy: 7273.0/10000 (73%)\n",
      "SGD: Loss: 0.7628\tAccuracy: 7498.0/10000 (75%)\n",
      "Adam_with_adadelta_coeff: Loss: 0.6067\tAccuracy: 7984.0/10000 (80%)\n",
      "\n",
      "Train Epoch: 6 [0/40000 (0%)]\tLosses StandardAdam: 1.1643 SGD: 0.8628 Adam_with_adadelta_coeff: 0.2831\n",
      "Train Epoch: 6 [800/40000 (2%)]\tLosses StandardAdam: 0.2345 SGD: 0.0160 Adam_with_adadelta_coeff: 0.0449\n",
      "Train Epoch: 6 [1600/40000 (4%)]\tLosses StandardAdam: 1.0679 SGD: 0.8018 Adam_with_adadelta_coeff: 0.3754\n",
      "Train Epoch: 6 [2400/40000 (6%)]\tLosses StandardAdam: 1.0568 SGD: 0.3650 Adam_with_adadelta_coeff: 0.3015\n",
      "Train Epoch: 6 [3200/40000 (8%)]\tLosses StandardAdam: 0.4004 SGD: 1.0172 Adam_with_adadelta_coeff: 0.0344\n",
      "Train Epoch: 6 [4000/40000 (10%)]\tLosses StandardAdam: 0.2762 SGD: 0.4472 Adam_with_adadelta_coeff: 0.2688\n",
      "Train Epoch: 6 [4800/40000 (12%)]\tLosses StandardAdam: 0.2408 SGD: 0.2493 Adam_with_adadelta_coeff: 0.0146\n",
      "Train Epoch: 6 [5600/40000 (14%)]\tLosses StandardAdam: 0.2766 SGD: 0.1563 Adam_with_adadelta_coeff: 0.0064\n",
      "Train Epoch: 6 [6400/40000 (16%)]\tLosses StandardAdam: 1.4505 SGD: 1.1120 Adam_with_adadelta_coeff: 0.1406\n",
      "Train Epoch: 6 [7200/40000 (18%)]\tLosses StandardAdam: 0.9024 SGD: 0.8792 Adam_with_adadelta_coeff: 0.0717\n",
      "Train Epoch: 6 [8000/40000 (20%)]\tLosses StandardAdam: 0.0318 SGD: 0.0205 Adam_with_adadelta_coeff: 0.0067\n",
      "Train Epoch: 6 [8800/40000 (22%)]\tLosses StandardAdam: 1.3472 SGD: 0.3672 Adam_with_adadelta_coeff: 0.0788\n",
      "Train Epoch: 6 [9600/40000 (24%)]\tLosses StandardAdam: 0.9818 SGD: 0.7243 Adam_with_adadelta_coeff: 0.2435\n",
      "Train Epoch: 6 [10400/40000 (26%)]\tLosses StandardAdam: 0.0440 SGD: 0.0142 Adam_with_adadelta_coeff: 0.0036\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 6 [11200/40000 (28%)]\tLosses StandardAdam: 0.3242 SGD: 0.2662 Adam_with_adadelta_coeff: 0.0914\n",
      "Train Epoch: 6 [12000/40000 (30%)]\tLosses StandardAdam: 0.2330 SGD: 0.0266 Adam_with_adadelta_coeff: 0.0358\n",
      "Train Epoch: 6 [12800/40000 (32%)]\tLosses StandardAdam: 0.1891 SGD: 0.1387 Adam_with_adadelta_coeff: 0.0322\n",
      "Train Epoch: 6 [13600/40000 (34%)]\tLosses StandardAdam: 1.2693 SGD: 0.9917 Adam_with_adadelta_coeff: 0.2073\n",
      "Train Epoch: 6 [14400/40000 (36%)]\tLosses StandardAdam: 1.4634 SGD: 1.6247 Adam_with_adadelta_coeff: 0.1901\n",
      "Train Epoch: 6 [15200/40000 (38%)]\tLosses StandardAdam: 0.1559 SGD: 0.6371 Adam_with_adadelta_coeff: 0.1827\n",
      "Train Epoch: 6 [16000/40000 (40%)]\tLosses StandardAdam: 0.3980 SGD: 0.1259 Adam_with_adadelta_coeff: 0.0087\n",
      "Train Epoch: 6 [16800/40000 (42%)]\tLosses StandardAdam: 0.5842 SGD: 0.3668 Adam_with_adadelta_coeff: 0.1314\n",
      "Train Epoch: 6 [17600/40000 (44%)]\tLosses StandardAdam: 0.4518 SGD: 0.4836 Adam_with_adadelta_coeff: 0.0657\n",
      "Train Epoch: 6 [18400/40000 (46%)]\tLosses StandardAdam: 0.6741 SGD: 0.5533 Adam_with_adadelta_coeff: 0.0554\n",
      "Train Epoch: 6 [19200/40000 (48%)]\tLosses StandardAdam: 1.8820 SGD: 2.2892 Adam_with_adadelta_coeff: 1.5049\n",
      "Train Epoch: 6 [20000/40000 (50%)]\tLosses StandardAdam: 1.0640 SGD: 1.0475 Adam_with_adadelta_coeff: 0.1718\n",
      "Train Epoch: 6 [20800/40000 (52%)]\tLosses StandardAdam: 0.5545 SGD: 0.2341 Adam_with_adadelta_coeff: 0.3599\n",
      "Train Epoch: 6 [21600/40000 (54%)]\tLosses StandardAdam: 0.2930 SGD: 0.5597 Adam_with_adadelta_coeff: 0.3754\n",
      "Train Epoch: 6 [22400/40000 (56%)]\tLosses StandardAdam: 1.3821 SGD: 0.2311 Adam_with_adadelta_coeff: 0.0284\n",
      "Train Epoch: 6 [23200/40000 (58%)]\tLosses StandardAdam: 0.3724 SGD: 0.3270 Adam_with_adadelta_coeff: 0.1376\n",
      "Train Epoch: 6 [24000/40000 (60%)]\tLosses StandardAdam: 1.2149 SGD: 1.3952 Adam_with_adadelta_coeff: 0.4139\n",
      "Train Epoch: 6 [24800/40000 (62%)]\tLosses StandardAdam: 0.6087 SGD: 0.7558 Adam_with_adadelta_coeff: 0.0986\n",
      "Train Epoch: 6 [25600/40000 (64%)]\tLosses StandardAdam: 0.0962 SGD: 0.6350 Adam_with_adadelta_coeff: 0.2599\n",
      "Train Epoch: 6 [26400/40000 (66%)]\tLosses StandardAdam: 0.0695 SGD: 0.0660 Adam_with_adadelta_coeff: 0.0365\n",
      "Train Epoch: 6 [27200/40000 (68%)]\tLosses StandardAdam: 0.5275 SGD: 0.6679 Adam_with_adadelta_coeff: 0.7216\n",
      "Train Epoch: 6 [28000/40000 (70%)]\tLosses StandardAdam: 0.8704 SGD: 0.4972 Adam_with_adadelta_coeff: 0.5423\n",
      "Train Epoch: 6 [28800/40000 (72%)]\tLosses StandardAdam: 1.0959 SGD: 0.8233 Adam_with_adadelta_coeff: 0.2306\n",
      "Train Epoch: 6 [29600/40000 (74%)]\tLosses StandardAdam: 0.4422 SGD: 0.5642 Adam_with_adadelta_coeff: 0.7956\n",
      "Train Epoch: 6 [30400/40000 (76%)]\tLosses StandardAdam: 0.5426 SGD: 0.5600 Adam_with_adadelta_coeff: 0.0310\n",
      "Train Epoch: 6 [31200/40000 (78%)]\tLosses StandardAdam: 0.7330 SGD: 0.4132 Adam_with_adadelta_coeff: 0.2354\n",
      "Train Epoch: 6 [32000/40000 (80%)]\tLosses StandardAdam: 0.5434 SGD: 0.6237 Adam_with_adadelta_coeff: 0.3869\n",
      "Train Epoch: 6 [32800/40000 (82%)]\tLosses StandardAdam: 0.1789 SGD: 0.1456 Adam_with_adadelta_coeff: 0.1086\n",
      "Train Epoch: 6 [33600/40000 (84%)]\tLosses StandardAdam: 0.1920 SGD: 0.5183 Adam_with_adadelta_coeff: 0.1213\n",
      "Train Epoch: 6 [34400/40000 (86%)]\tLosses StandardAdam: 1.3647 SGD: 0.2909 Adam_with_adadelta_coeff: 0.0452\n",
      "Train Epoch: 6 [35200/40000 (88%)]\tLosses StandardAdam: 0.0279 SGD: 0.0676 Adam_with_adadelta_coeff: 0.0744\n",
      "Train Epoch: 6 [36000/40000 (90%)]\tLosses StandardAdam: 0.7760 SGD: 0.2919 Adam_with_adadelta_coeff: 0.0167\n",
      "Train Epoch: 6 [36800/40000 (92%)]\tLosses StandardAdam: 0.3048 SGD: 0.3727 Adam_with_adadelta_coeff: 0.0146\n",
      "Train Epoch: 6 [37600/40000 (94%)]\tLosses StandardAdam: 1.2734 SGD: 1.1558 Adam_with_adadelta_coeff: 0.9251\n",
      "Train Epoch: 6 [38400/40000 (96%)]\tLosses StandardAdam: 0.6176 SGD: 0.3840 Adam_with_adadelta_coeff: 0.2792\n",
      "Train Epoch: 6 [39200/40000 (98%)]\tLosses StandardAdam: 1.2174 SGD: 0.3066 Adam_with_adadelta_coeff: 0.7428\n",
      "Train Epoch: 6 [40000/40000 (100%)]\tLosses StandardAdam: 0.2632 SGD: 0.6640 Adam_with_adadelta_coeff: 0.1054\n",
      "Test set:\n",
      "StandardAdam: Loss: 0.7216\tAccuracy: 7603.0/10000 (76%)\n",
      "SGD: Loss: 0.6595\tAccuracy: 7820.0/10000 (78%)\n",
      "Adam_with_adadelta_coeff: Loss: 0.5803\tAccuracy: 8127.0/10000 (81%)\n",
      "\n",
      "Train Epoch: 7 [0/40000 (0%)]\tLosses StandardAdam: 0.3569 SGD: 0.3372 Adam_with_adadelta_coeff: 0.1153\n",
      "Train Epoch: 7 [800/40000 (2%)]\tLosses StandardAdam: 0.3143 SGD: 0.2685 Adam_with_adadelta_coeff: 0.0743\n",
      "Train Epoch: 7 [1600/40000 (4%)]\tLosses StandardAdam: 0.9409 SGD: 0.7557 Adam_with_adadelta_coeff: 0.0570\n",
      "Train Epoch: 7 [2400/40000 (6%)]\tLosses StandardAdam: 1.4289 SGD: 1.3709 Adam_with_adadelta_coeff: 0.4104\n",
      "Train Epoch: 7 [3200/40000 (8%)]\tLosses StandardAdam: 0.5210 SGD: 0.1502 Adam_with_adadelta_coeff: 0.0428\n",
      "Train Epoch: 7 [4000/40000 (10%)]\tLosses StandardAdam: 0.0165 SGD: 0.0234 Adam_with_adadelta_coeff: 0.0041\n",
      "Train Epoch: 7 [4800/40000 (12%)]\tLosses StandardAdam: 1.2001 SGD: 1.5191 Adam_with_adadelta_coeff: 0.2922\n",
      "Train Epoch: 7 [5600/40000 (14%)]\tLosses StandardAdam: 0.1139 SGD: 0.1874 Adam_with_adadelta_coeff: 0.0407\n",
      "Train Epoch: 7 [6400/40000 (16%)]\tLosses StandardAdam: 0.6855 SGD: 0.3357 Adam_with_adadelta_coeff: 0.1684\n",
      "Train Epoch: 7 [7200/40000 (18%)]\tLosses StandardAdam: 0.8107 SGD: 0.3574 Adam_with_adadelta_coeff: 0.1804\n",
      "Train Epoch: 7 [8000/40000 (20%)]\tLosses StandardAdam: 0.0211 SGD: 0.0252 Adam_with_adadelta_coeff: 0.0356\n",
      "Train Epoch: 7 [8800/40000 (22%)]\tLosses StandardAdam: 0.0087 SGD: 0.0133 Adam_with_adadelta_coeff: 0.0288\n",
      "Train Epoch: 7 [9600/40000 (24%)]\tLosses StandardAdam: 0.0278 SGD: 0.0034 Adam_with_adadelta_coeff: 0.0011\n",
      "Train Epoch: 7 [10400/40000 (26%)]\tLosses StandardAdam: 2.4431 SGD: 1.1780 Adam_with_adadelta_coeff: 1.3978\n",
      "Train Epoch: 7 [11200/40000 (28%)]\tLosses StandardAdam: 0.2997 SGD: 0.5908 Adam_with_adadelta_coeff: 0.0463\n",
      "Train Epoch: 7 [12000/40000 (30%)]\tLosses StandardAdam: 0.9121 SGD: 0.6241 Adam_with_adadelta_coeff: 0.2668\n",
      "Train Epoch: 7 [12800/40000 (32%)]\tLosses StandardAdam: 0.2257 SGD: 0.6103 Adam_with_adadelta_coeff: 0.0811\n",
      "Train Epoch: 7 [13600/40000 (34%)]\tLosses StandardAdam: 0.2893 SGD: 0.3904 Adam_with_adadelta_coeff: 0.0255\n",
      "Train Epoch: 7 [14400/40000 (36%)]\tLosses StandardAdam: 2.2072 SGD: 1.1506 Adam_with_adadelta_coeff: 0.5225\n",
      "Train Epoch: 7 [15200/40000 (38%)]\tLosses StandardAdam: 2.6158 SGD: 2.3650 Adam_with_adadelta_coeff: 0.5423\n",
      "Train Epoch: 7 [16000/40000 (40%)]\tLosses StandardAdam: 0.8771 SGD: 0.6622 Adam_with_adadelta_coeff: 0.2381\n",
      "Train Epoch: 7 [16800/40000 (42%)]\tLosses StandardAdam: 0.6082 SGD: 0.6193 Adam_with_adadelta_coeff: 0.0786\n",
      "Train Epoch: 7 [17600/40000 (44%)]\tLosses StandardAdam: 0.3933 SGD: 0.4273 Adam_with_adadelta_coeff: 0.0317\n",
      "Train Epoch: 7 [18400/40000 (46%)]\tLosses StandardAdam: 0.2401 SGD: 0.0289 Adam_with_adadelta_coeff: 0.0045\n",
      "Train Epoch: 7 [19200/40000 (48%)]\tLosses StandardAdam: 0.6261 SGD: 0.0018 Adam_with_adadelta_coeff: 0.0883\n",
      "Train Epoch: 7 [20000/40000 (50%)]\tLosses StandardAdam: 0.1302 SGD: 0.0425 Adam_with_adadelta_coeff: 0.0143\n",
      "Train Epoch: 7 [20800/40000 (52%)]\tLosses StandardAdam: 0.0401 SGD: 0.0412 Adam_with_adadelta_coeff: 0.1560\n",
      "Train Epoch: 7 [21600/40000 (54%)]\tLosses StandardAdam: 0.8878 SGD: 0.4331 Adam_with_adadelta_coeff: 0.2494\n",
      "Train Epoch: 7 [22400/40000 (56%)]\tLosses StandardAdam: 0.5279 SGD: 0.4161 Adam_with_adadelta_coeff: 0.5669\n",
      "Train Epoch: 7 [23200/40000 (58%)]\tLosses StandardAdam: 0.2230 SGD: 0.0602 Adam_with_adadelta_coeff: 0.0326\n",
      "Train Epoch: 7 [24000/40000 (60%)]\tLosses StandardAdam: 0.1082 SGD: 0.1291 Adam_with_adadelta_coeff: 0.0126\n",
      "Train Epoch: 7 [24800/40000 (62%)]\tLosses StandardAdam: 0.6972 SGD: 0.2182 Adam_with_adadelta_coeff: 0.0823\n",
      "Train Epoch: 7 [25600/40000 (64%)]\tLosses StandardAdam: 1.1387 SGD: 1.2787 Adam_with_adadelta_coeff: 0.0614\n",
      "Train Epoch: 7 [26400/40000 (66%)]\tLosses StandardAdam: 0.2644 SGD: 0.5286 Adam_with_adadelta_coeff: 0.0261\n",
      "Train Epoch: 7 [27200/40000 (68%)]\tLosses StandardAdam: 0.3009 SGD: 0.2306 Adam_with_adadelta_coeff: 0.0104\n",
      "Train Epoch: 7 [28000/40000 (70%)]\tLosses StandardAdam: 0.7243 SGD: 0.3269 Adam_with_adadelta_coeff: 0.1069\n",
      "Train Epoch: 7 [28800/40000 (72%)]\tLosses StandardAdam: 0.3611 SGD: 0.3045 Adam_with_adadelta_coeff: 0.3405\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 7 [29600/40000 (74%)]\tLosses StandardAdam: 0.8895 SGD: 0.7098 Adam_with_adadelta_coeff: 0.2166\n",
      "Train Epoch: 7 [30400/40000 (76%)]\tLosses StandardAdam: 1.4034 SGD: 0.4999 Adam_with_adadelta_coeff: 0.0157\n",
      "Train Epoch: 7 [31200/40000 (78%)]\tLosses StandardAdam: 0.1113 SGD: 0.1168 Adam_with_adadelta_coeff: 0.0090\n",
      "Train Epoch: 7 [32000/40000 (80%)]\tLosses StandardAdam: 0.3954 SGD: 0.3117 Adam_with_adadelta_coeff: 0.0538\n",
      "Train Epoch: 7 [32800/40000 (82%)]\tLosses StandardAdam: 1.0452 SGD: 0.9700 Adam_with_adadelta_coeff: 0.5850\n",
      "Train Epoch: 7 [33600/40000 (84%)]\tLosses StandardAdam: 0.4490 SGD: 0.2497 Adam_with_adadelta_coeff: 0.0156\n",
      "Train Epoch: 7 [34400/40000 (86%)]\tLosses StandardAdam: 0.3659 SGD: 0.3631 Adam_with_adadelta_coeff: 0.0509\n",
      "Train Epoch: 7 [35200/40000 (88%)]\tLosses StandardAdam: 1.2130 SGD: 0.0529 Adam_with_adadelta_coeff: 0.1338\n",
      "Train Epoch: 7 [36000/40000 (90%)]\tLosses StandardAdam: 0.1064 SGD: 0.1301 Adam_with_adadelta_coeff: 0.0055\n",
      "Train Epoch: 7 [36800/40000 (92%)]\tLosses StandardAdam: 0.6334 SGD: 0.9039 Adam_with_adadelta_coeff: 0.3289\n",
      "Train Epoch: 7 [37600/40000 (94%)]\tLosses StandardAdam: 0.7238 SGD: 0.3931 Adam_with_adadelta_coeff: 0.1301\n",
      "Train Epoch: 7 [38400/40000 (96%)]\tLosses StandardAdam: 0.2299 SGD: 0.3155 Adam_with_adadelta_coeff: 0.1275\n",
      "Train Epoch: 7 [39200/40000 (98%)]\tLosses StandardAdam: 1.2521 SGD: 1.5538 Adam_with_adadelta_coeff: 0.8107\n",
      "Train Epoch: 7 [40000/40000 (100%)]\tLosses StandardAdam: 0.9974 SGD: 0.5745 Adam_with_adadelta_coeff: 0.2174\n",
      "Test set:\n",
      "StandardAdam: Loss: 0.6993\tAccuracy: 7658.0/10000 (77%)\n",
      "SGD: Loss: 0.6251\tAccuracy: 7939.0/10000 (79%)\n",
      "Adam_with_adadelta_coeff: Loss: 0.5971\tAccuracy: 8165.0/10000 (82%)\n",
      "\n",
      "Train Epoch: 8 [0/40000 (0%)]\tLosses StandardAdam: 0.2527 SGD: 0.1525 Adam_with_adadelta_coeff: 0.0898\n",
      "Train Epoch: 8 [800/40000 (2%)]\tLosses StandardAdam: 0.2150 SGD: 0.0306 Adam_with_adadelta_coeff: 0.0027\n",
      "Train Epoch: 8 [1600/40000 (4%)]\tLosses StandardAdam: 0.0434 SGD: 0.1315 Adam_with_adadelta_coeff: 0.0030\n",
      "Train Epoch: 8 [2400/40000 (6%)]\tLosses StandardAdam: 0.7287 SGD: 0.1366 Adam_with_adadelta_coeff: 0.2868\n",
      "Train Epoch: 8 [3200/40000 (8%)]\tLosses StandardAdam: 1.3203 SGD: 0.2456 Adam_with_adadelta_coeff: 0.1056\n",
      "Train Epoch: 8 [4000/40000 (10%)]\tLosses StandardAdam: 1.0527 SGD: 0.5136 Adam_with_adadelta_coeff: 0.1657\n",
      "Train Epoch: 8 [4800/40000 (12%)]\tLosses StandardAdam: 1.0738 SGD: 0.5772 Adam_with_adadelta_coeff: 0.3130\n",
      "Train Epoch: 8 [5600/40000 (14%)]\tLosses StandardAdam: 1.8497 SGD: 1.8337 Adam_with_adadelta_coeff: 0.3854\n",
      "Train Epoch: 8 [6400/40000 (16%)]\tLosses StandardAdam: 0.4498 SGD: 0.0886 Adam_with_adadelta_coeff: 0.0049\n",
      "Train Epoch: 8 [7200/40000 (18%)]\tLosses StandardAdam: 0.2838 SGD: 0.3647 Adam_with_adadelta_coeff: 0.2903\n",
      "Train Epoch: 8 [8000/40000 (20%)]\tLosses StandardAdam: 0.0242 SGD: 0.1158 Adam_with_adadelta_coeff: 0.0468\n",
      "Train Epoch: 8 [8800/40000 (22%)]\tLosses StandardAdam: 0.0244 SGD: 0.0252 Adam_with_adadelta_coeff: 0.0183\n",
      "Train Epoch: 8 [9600/40000 (24%)]\tLosses StandardAdam: 1.1633 SGD: 0.9836 Adam_with_adadelta_coeff: 0.0838\n",
      "Train Epoch: 8 [10400/40000 (26%)]\tLosses StandardAdam: 0.6626 SGD: 0.4908 Adam_with_adadelta_coeff: 0.1077\n",
      "Train Epoch: 8 [11200/40000 (28%)]\tLosses StandardAdam: 0.1608 SGD: 0.0509 Adam_with_adadelta_coeff: 0.0386\n",
      "Train Epoch: 8 [12000/40000 (30%)]\tLosses StandardAdam: 0.7549 SGD: 0.2559 Adam_with_adadelta_coeff: 0.0074\n",
      "Train Epoch: 8 [12800/40000 (32%)]\tLosses StandardAdam: 1.5799 SGD: 1.1948 Adam_with_adadelta_coeff: 0.2938\n",
      "Train Epoch: 8 [13600/40000 (34%)]\tLosses StandardAdam: 0.5152 SGD: 0.6744 Adam_with_adadelta_coeff: 0.0249\n",
      "Train Epoch: 8 [14400/40000 (36%)]\tLosses StandardAdam: 0.5071 SGD: 1.3520 Adam_with_adadelta_coeff: 0.5741\n",
      "Train Epoch: 8 [15200/40000 (38%)]\tLosses StandardAdam: 0.1850 SGD: 0.0155 Adam_with_adadelta_coeff: 0.0030\n",
      "Train Epoch: 8 [16000/40000 (40%)]\tLosses StandardAdam: 0.0688 SGD: 0.1801 Adam_with_adadelta_coeff: 0.0616\n",
      "Train Epoch: 8 [16800/40000 (42%)]\tLosses StandardAdam: 0.0346 SGD: 0.1243 Adam_with_adadelta_coeff: 0.2108\n",
      "Train Epoch: 8 [17600/40000 (44%)]\tLosses StandardAdam: 2.0040 SGD: 0.5530 Adam_with_adadelta_coeff: 0.2477\n",
      "Train Epoch: 8 [18400/40000 (46%)]\tLosses StandardAdam: 0.7315 SGD: 0.0567 Adam_with_adadelta_coeff: 0.0012\n",
      "Train Epoch: 8 [19200/40000 (48%)]\tLosses StandardAdam: 0.5038 SGD: 0.2521 Adam_with_adadelta_coeff: 0.1186\n",
      "Train Epoch: 8 [20000/40000 (50%)]\tLosses StandardAdam: 0.0528 SGD: 0.0486 Adam_with_adadelta_coeff: 0.0015\n",
      "Train Epoch: 8 [20800/40000 (52%)]\tLosses StandardAdam: 0.2558 SGD: 0.2007 Adam_with_adadelta_coeff: 0.5063\n",
      "Train Epoch: 8 [21600/40000 (54%)]\tLosses StandardAdam: 0.4469 SGD: 0.0980 Adam_with_adadelta_coeff: 0.0425\n",
      "Train Epoch: 8 [22400/40000 (56%)]\tLosses StandardAdam: 1.3918 SGD: 1.8739 Adam_with_adadelta_coeff: 0.1737\n",
      "Train Epoch: 8 [23200/40000 (58%)]\tLosses StandardAdam: 0.2739 SGD: 0.0095 Adam_with_adadelta_coeff: 0.1022\n",
      "Train Epoch: 8 [24000/40000 (60%)]\tLosses StandardAdam: 1.0272 SGD: 2.0538 Adam_with_adadelta_coeff: 0.4246\n",
      "Train Epoch: 8 [24800/40000 (62%)]\tLosses StandardAdam: 0.8427 SGD: 1.3375 Adam_with_adadelta_coeff: 1.0281\n",
      "Train Epoch: 8 [25600/40000 (64%)]\tLosses StandardAdam: 0.4577 SGD: 0.4349 Adam_with_adadelta_coeff: 0.0515\n",
      "Train Epoch: 8 [26400/40000 (66%)]\tLosses StandardAdam: 0.6171 SGD: 0.2204 Adam_with_adadelta_coeff: 0.0162\n",
      "Train Epoch: 8 [27200/40000 (68%)]\tLosses StandardAdam: 1.1972 SGD: 1.6381 Adam_with_adadelta_coeff: 0.0225\n",
      "Train Epoch: 8 [28000/40000 (70%)]\tLosses StandardAdam: 0.5206 SGD: 0.9358 Adam_with_adadelta_coeff: 0.5091\n",
      "Train Epoch: 8 [28800/40000 (72%)]\tLosses StandardAdam: 0.1183 SGD: 0.1175 Adam_with_adadelta_coeff: 0.2093\n",
      "Train Epoch: 8 [29600/40000 (74%)]\tLosses StandardAdam: 0.1288 SGD: 0.0048 Adam_with_adadelta_coeff: 0.0182\n",
      "Train Epoch: 8 [30400/40000 (76%)]\tLosses StandardAdam: 0.8874 SGD: 0.4953 Adam_with_adadelta_coeff: 0.4901\n",
      "Train Epoch: 8 [31200/40000 (78%)]\tLosses StandardAdam: 0.0097 SGD: 0.0009 Adam_with_adadelta_coeff: 0.0000\n",
      "Train Epoch: 8 [32000/40000 (80%)]\tLosses StandardAdam: 1.7152 SGD: 1.4669 Adam_with_adadelta_coeff: 0.4820\n",
      "Train Epoch: 8 [32800/40000 (82%)]\tLosses StandardAdam: 1.1075 SGD: 1.3290 Adam_with_adadelta_coeff: 0.3115\n",
      "Train Epoch: 8 [33600/40000 (84%)]\tLosses StandardAdam: 0.0652 SGD: 0.1358 Adam_with_adadelta_coeff: 0.0194\n",
      "Train Epoch: 8 [34400/40000 (86%)]\tLosses StandardAdam: 0.9286 SGD: 0.8592 Adam_with_adadelta_coeff: 0.7349\n",
      "Train Epoch: 8 [35200/40000 (88%)]\tLosses StandardAdam: 0.0843 SGD: 0.0593 Adam_with_adadelta_coeff: 0.0014\n",
      "Train Epoch: 8 [36000/40000 (90%)]\tLosses StandardAdam: 0.6290 SGD: 0.0959 Adam_with_adadelta_coeff: 0.5815\n",
      "Train Epoch: 8 [36800/40000 (92%)]\tLosses StandardAdam: 0.1679 SGD: 0.2295 Adam_with_adadelta_coeff: 0.0523\n",
      "Train Epoch: 8 [37600/40000 (94%)]\tLosses StandardAdam: 0.1144 SGD: 0.0812 Adam_with_adadelta_coeff: 0.0064\n",
      "Train Epoch: 8 [38400/40000 (96%)]\tLosses StandardAdam: 0.6051 SGD: 0.4080 Adam_with_adadelta_coeff: 0.5134\n",
      "Train Epoch: 8 [39200/40000 (98%)]\tLosses StandardAdam: 0.9111 SGD: 0.6670 Adam_with_adadelta_coeff: 2.0848\n",
      "Train Epoch: 8 [40000/40000 (100%)]\tLosses StandardAdam: 0.0834 SGD: 0.0248 Adam_with_adadelta_coeff: 0.0692\n",
      "Test set:\n",
      "StandardAdam: Loss: 0.7499\tAccuracy: 7664.0/10000 (77%)\n",
      "SGD: Loss: 0.6529\tAccuracy: 8019.0/10000 (80%)\n",
      "Adam_with_adadelta_coeff: Loss: 0.6534\tAccuracy: 8094.0/10000 (81%)\n",
      "\n",
      "Train Epoch: 9 [0/40000 (0%)]\tLosses StandardAdam: 0.1537 SGD: 0.0642 Adam_with_adadelta_coeff: 0.0074\n",
      "Train Epoch: 9 [800/40000 (2%)]\tLosses StandardAdam: 0.6214 SGD: 0.2302 Adam_with_adadelta_coeff: 0.2743\n",
      "Train Epoch: 9 [1600/40000 (4%)]\tLosses StandardAdam: 1.2259 SGD: 0.9152 Adam_with_adadelta_coeff: 0.0581\n",
      "Train Epoch: 9 [2400/40000 (6%)]\tLosses StandardAdam: 0.6552 SGD: 0.1529 Adam_with_adadelta_coeff: 0.0339\n",
      "Train Epoch: 9 [3200/40000 (8%)]\tLosses StandardAdam: 0.8981 SGD: 0.2130 Adam_with_adadelta_coeff: 0.0747\n",
      "Train Epoch: 9 [4000/40000 (10%)]\tLosses StandardAdam: 1.0932 SGD: 0.3002 Adam_with_adadelta_coeff: 0.2339\n",
      "Train Epoch: 9 [4800/40000 (12%)]\tLosses StandardAdam: 0.2076 SGD: 0.0371 Adam_with_adadelta_coeff: 0.6360\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 9 [5600/40000 (14%)]\tLosses StandardAdam: 0.1153 SGD: 0.0121 Adam_with_adadelta_coeff: 0.0002\n",
      "Train Epoch: 9 [6400/40000 (16%)]\tLosses StandardAdam: 0.9424 SGD: 0.4429 Adam_with_adadelta_coeff: 0.4212\n",
      "Train Epoch: 9 [7200/40000 (18%)]\tLosses StandardAdam: 0.0454 SGD: 0.0033 Adam_with_adadelta_coeff: 0.0572\n",
      "Train Epoch: 9 [8000/40000 (20%)]\tLosses StandardAdam: 0.0107 SGD: 0.6337 Adam_with_adadelta_coeff: 0.2056\n",
      "Train Epoch: 9 [8800/40000 (22%)]\tLosses StandardAdam: 0.0343 SGD: 0.0051 Adam_with_adadelta_coeff: 0.0293\n",
      "Train Epoch: 9 [9600/40000 (24%)]\tLosses StandardAdam: 0.1191 SGD: 0.0562 Adam_with_adadelta_coeff: 0.0001\n",
      "Train Epoch: 9 [10400/40000 (26%)]\tLosses StandardAdam: 0.1149 SGD: 0.1339 Adam_with_adadelta_coeff: 0.0899\n",
      "Train Epoch: 9 [11200/40000 (28%)]\tLosses StandardAdam: 1.2418 SGD: 0.4111 Adam_with_adadelta_coeff: 0.6321\n",
      "Train Epoch: 9 [12000/40000 (30%)]\tLosses StandardAdam: 0.2938 SGD: 0.0237 Adam_with_adadelta_coeff: 0.6171\n",
      "Train Epoch: 9 [12800/40000 (32%)]\tLosses StandardAdam: 0.0250 SGD: 0.1281 Adam_with_adadelta_coeff: 0.0093\n",
      "Train Epoch: 9 [13600/40000 (34%)]\tLosses StandardAdam: 0.5101 SGD: 0.0196 Adam_with_adadelta_coeff: 0.0506\n",
      "Train Epoch: 9 [14400/40000 (36%)]\tLosses StandardAdam: 0.0691 SGD: 0.0109 Adam_with_adadelta_coeff: 0.0003\n",
      "Train Epoch: 9 [15200/40000 (38%)]\tLosses StandardAdam: 0.0658 SGD: 0.0434 Adam_with_adadelta_coeff: 0.0319\n",
      "Train Epoch: 9 [16000/40000 (40%)]\tLosses StandardAdam: 0.7342 SGD: 0.5178 Adam_with_adadelta_coeff: 0.1352\n",
      "Train Epoch: 9 [16800/40000 (42%)]\tLosses StandardAdam: 1.7939 SGD: 0.3958 Adam_with_adadelta_coeff: 0.7993\n",
      "Train Epoch: 9 [17600/40000 (44%)]\tLosses StandardAdam: 0.3108 SGD: 0.4413 Adam_with_adadelta_coeff: 0.1169\n",
      "Train Epoch: 9 [18400/40000 (46%)]\tLosses StandardAdam: 1.1158 SGD: 0.2124 Adam_with_adadelta_coeff: 0.0753\n",
      "Train Epoch: 9 [19200/40000 (48%)]\tLosses StandardAdam: 0.7255 SGD: 0.3026 Adam_with_adadelta_coeff: 0.0502\n",
      "Train Epoch: 9 [20000/40000 (50%)]\tLosses StandardAdam: 0.0011 SGD: 0.0646 Adam_with_adadelta_coeff: 0.0047\n",
      "Train Epoch: 9 [20800/40000 (52%)]\tLosses StandardAdam: 0.9969 SGD: 0.7250 Adam_with_adadelta_coeff: 0.3659\n",
      "Train Epoch: 9 [21600/40000 (54%)]\tLosses StandardAdam: 1.2525 SGD: 1.1653 Adam_with_adadelta_coeff: 0.1407\n",
      "Train Epoch: 9 [22400/40000 (56%)]\tLosses StandardAdam: 0.1840 SGD: 0.0171 Adam_with_adadelta_coeff: 0.0503\n",
      "Train Epoch: 9 [23200/40000 (58%)]\tLosses StandardAdam: 0.6942 SGD: 0.2254 Adam_with_adadelta_coeff: 0.1001\n",
      "Train Epoch: 9 [24000/40000 (60%)]\tLosses StandardAdam: 0.1375 SGD: 0.0262 Adam_with_adadelta_coeff: 0.0032\n",
      "Train Epoch: 9 [24800/40000 (62%)]\tLosses StandardAdam: 0.4640 SGD: 1.0778 Adam_with_adadelta_coeff: 0.7173\n",
      "Train Epoch: 9 [25600/40000 (64%)]\tLosses StandardAdam: 0.0566 SGD: 0.1368 Adam_with_adadelta_coeff: 0.0008\n",
      "Train Epoch: 9 [26400/40000 (66%)]\tLosses StandardAdam: 0.3243 SGD: 0.5674 Adam_with_adadelta_coeff: 0.8991\n",
      "Train Epoch: 9 [27200/40000 (68%)]\tLosses StandardAdam: 0.5814 SGD: 0.9524 Adam_with_adadelta_coeff: 0.9854\n",
      "Train Epoch: 9 [28000/40000 (70%)]\tLosses StandardAdam: 0.1778 SGD: 1.0417 Adam_with_adadelta_coeff: 0.0128\n",
      "Train Epoch: 9 [28800/40000 (72%)]\tLosses StandardAdam: 0.3669 SGD: 0.1020 Adam_with_adadelta_coeff: 0.0874\n",
      "Train Epoch: 9 [29600/40000 (74%)]\tLosses StandardAdam: 3.4029 SGD: 0.8565 Adam_with_adadelta_coeff: 0.0747\n",
      "Train Epoch: 9 [30400/40000 (76%)]\tLosses StandardAdam: 0.1638 SGD: 0.5467 Adam_with_adadelta_coeff: 0.0865\n",
      "Train Epoch: 9 [31200/40000 (78%)]\tLosses StandardAdam: 0.0608 SGD: 0.6080 Adam_with_adadelta_coeff: 0.1554\n",
      "Train Epoch: 9 [32000/40000 (80%)]\tLosses StandardAdam: 0.1876 SGD: 0.0801 Adam_with_adadelta_coeff: 0.0054\n",
      "Train Epoch: 9 [32800/40000 (82%)]\tLosses StandardAdam: 0.7286 SGD: 1.1539 Adam_with_adadelta_coeff: 1.0677\n",
      "Train Epoch: 9 [33600/40000 (84%)]\tLosses StandardAdam: 0.3792 SGD: 0.0380 Adam_with_adadelta_coeff: 0.0080\n",
      "Train Epoch: 9 [34400/40000 (86%)]\tLosses StandardAdam: 0.4954 SGD: 0.4559 Adam_with_adadelta_coeff: 0.0081\n",
      "Train Epoch: 9 [35200/40000 (88%)]\tLosses StandardAdam: 0.2373 SGD: 0.0179 Adam_with_adadelta_coeff: 0.0103\n",
      "Train Epoch: 9 [36000/40000 (90%)]\tLosses StandardAdam: 0.4447 SGD: 0.1541 Adam_with_adadelta_coeff: 0.0356\n",
      "Train Epoch: 9 [36800/40000 (92%)]\tLosses StandardAdam: 0.0592 SGD: 0.2470 Adam_with_adadelta_coeff: 0.0742\n",
      "Train Epoch: 9 [37600/40000 (94%)]\tLosses StandardAdam: 1.3553 SGD: 0.3050 Adam_with_adadelta_coeff: 0.6058\n",
      "Train Epoch: 9 [38400/40000 (96%)]\tLosses StandardAdam: 0.0804 SGD: 0.1154 Adam_with_adadelta_coeff: 0.0054\n",
      "Train Epoch: 9 [39200/40000 (98%)]\tLosses StandardAdam: 0.7817 SGD: 0.5397 Adam_with_adadelta_coeff: 1.0741\n",
      "Train Epoch: 9 [40000/40000 (100%)]\tLosses StandardAdam: 0.0980 SGD: 0.2849 Adam_with_adadelta_coeff: 0.0304\n",
      "Test set:\n",
      "StandardAdam: Loss: 0.7058\tAccuracy: 7815.0/10000 (78%)\n",
      "SGD: Loss: 0.7172\tAccuracy: 7907.0/10000 (79%)\n",
      "Adam_with_adadelta_coeff: Loss: 0.6109\tAccuracy: 8308.0/10000 (83%)\n",
      "\n",
      "Train Epoch: 10 [0/40000 (0%)]\tLosses StandardAdam: 0.2167 SGD: 0.2577 Adam_with_adadelta_coeff: 0.0217\n",
      "Train Epoch: 10 [800/40000 (2%)]\tLosses StandardAdam: 0.1833 SGD: 0.0300 Adam_with_adadelta_coeff: 0.2749\n",
      "Train Epoch: 10 [1600/40000 (4%)]\tLosses StandardAdam: 0.0491 SGD: 0.0195 Adam_with_adadelta_coeff: 0.0081\n",
      "Train Epoch: 10 [2400/40000 (6%)]\tLosses StandardAdam: 0.0200 SGD: 0.0334 Adam_with_adadelta_coeff: 0.0005\n",
      "Train Epoch: 10 [3200/40000 (8%)]\tLosses StandardAdam: 0.8499 SGD: 0.0018 Adam_with_adadelta_coeff: 0.0308\n",
      "Train Epoch: 10 [4000/40000 (10%)]\tLosses StandardAdam: 1.2469 SGD: 0.2277 Adam_with_adadelta_coeff: 0.0427\n",
      "Train Epoch: 10 [4800/40000 (12%)]\tLosses StandardAdam: 1.3283 SGD: 1.0028 Adam_with_adadelta_coeff: 0.1882\n",
      "Train Epoch: 10 [5600/40000 (14%)]\tLosses StandardAdam: 0.1449 SGD: 0.0203 Adam_with_adadelta_coeff: 0.0117\n",
      "Train Epoch: 10 [6400/40000 (16%)]\tLosses StandardAdam: 0.0954 SGD: 0.0657 Adam_with_adadelta_coeff: 0.0217\n",
      "Train Epoch: 10 [7200/40000 (18%)]\tLosses StandardAdam: 0.0274 SGD: 0.0035 Adam_with_adadelta_coeff: 0.0009\n",
      "Train Epoch: 10 [8000/40000 (20%)]\tLosses StandardAdam: 0.0029 SGD: 0.0005 Adam_with_adadelta_coeff: 0.0059\n",
      "Train Epoch: 10 [8800/40000 (22%)]\tLosses StandardAdam: 0.1175 SGD: 0.0814 Adam_with_adadelta_coeff: 0.0019\n",
      "Train Epoch: 10 [9600/40000 (24%)]\tLosses StandardAdam: 0.7089 SGD: 0.4104 Adam_with_adadelta_coeff: 0.3574\n",
      "Train Epoch: 10 [10400/40000 (26%)]\tLosses StandardAdam: 0.2399 SGD: 1.5478 Adam_with_adadelta_coeff: 0.0074\n",
      "Train Epoch: 10 [11200/40000 (28%)]\tLosses StandardAdam: 0.1749 SGD: 0.0204 Adam_with_adadelta_coeff: 0.0809\n",
      "Train Epoch: 10 [12000/40000 (30%)]\tLosses StandardAdam: 0.4145 SGD: 0.1343 Adam_with_adadelta_coeff: 0.0125\n",
      "Train Epoch: 10 [12800/40000 (32%)]\tLosses StandardAdam: 0.0708 SGD: 0.4449 Adam_with_adadelta_coeff: 0.0888\n",
      "Train Epoch: 10 [13600/40000 (34%)]\tLosses StandardAdam: 0.0743 SGD: 0.0004 Adam_with_adadelta_coeff: 0.0002\n",
      "Train Epoch: 10 [14400/40000 (36%)]\tLosses StandardAdam: 0.2347 SGD: 0.0905 Adam_with_adadelta_coeff: 0.0020\n",
      "Train Epoch: 10 [15200/40000 (38%)]\tLosses StandardAdam: 0.1638 SGD: 0.0476 Adam_with_adadelta_coeff: 0.0501\n",
      "Train Epoch: 10 [16000/40000 (40%)]\tLosses StandardAdam: 0.0723 SGD: 0.1432 Adam_with_adadelta_coeff: 0.3809\n",
      "Train Epoch: 10 [16800/40000 (42%)]\tLosses StandardAdam: 0.6922 SGD: 0.2487 Adam_with_adadelta_coeff: 0.0696\n",
      "Train Epoch: 10 [17600/40000 (44%)]\tLosses StandardAdam: 0.1833 SGD: 0.1945 Adam_with_adadelta_coeff: 0.0162\n",
      "Train Epoch: 10 [18400/40000 (46%)]\tLosses StandardAdam: 0.1221 SGD: 0.1713 Adam_with_adadelta_coeff: 0.2846\n",
      "Train Epoch: 10 [19200/40000 (48%)]\tLosses StandardAdam: 0.2089 SGD: 0.6001 Adam_with_adadelta_coeff: 0.0114\n",
      "Train Epoch: 10 [20000/40000 (50%)]\tLosses StandardAdam: 0.4343 SGD: 0.4756 Adam_with_adadelta_coeff: 0.0641\n",
      "Train Epoch: 10 [20800/40000 (52%)]\tLosses StandardAdam: 0.0075 SGD: 0.0072 Adam_with_adadelta_coeff: 0.0010\n",
      "Train Epoch: 10 [21600/40000 (54%)]\tLosses StandardAdam: 0.2307 SGD: 0.5407 Adam_with_adadelta_coeff: 0.0272\n",
      "Train Epoch: 10 [22400/40000 (56%)]\tLosses StandardAdam: 0.4351 SGD: 0.4016 Adam_with_adadelta_coeff: 0.4075\n",
      "Train Epoch: 10 [23200/40000 (58%)]\tLosses StandardAdam: 0.0263 SGD: 0.0436 Adam_with_adadelta_coeff: 0.0024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 10 [24000/40000 (60%)]\tLosses StandardAdam: 0.0226 SGD: 0.4229 Adam_with_adadelta_coeff: 0.0043\n",
      "Train Epoch: 10 [24800/40000 (62%)]\tLosses StandardAdam: 0.5115 SGD: 0.3611 Adam_with_adadelta_coeff: 0.0368\n",
      "Train Epoch: 10 [25600/40000 (64%)]\tLosses StandardAdam: 0.6264 SGD: 0.0787 Adam_with_adadelta_coeff: 0.0139\n",
      "Train Epoch: 10 [26400/40000 (66%)]\tLosses StandardAdam: 1.1643 SGD: 2.2291 Adam_with_adadelta_coeff: 0.5899\n",
      "Train Epoch: 10 [27200/40000 (68%)]\tLosses StandardAdam: 0.4082 SGD: 1.0062 Adam_with_adadelta_coeff: 0.3750\n",
      "Train Epoch: 10 [28000/40000 (70%)]\tLosses StandardAdam: 0.0619 SGD: 0.0108 Adam_with_adadelta_coeff: 0.0003\n",
      "Train Epoch: 10 [28800/40000 (72%)]\tLosses StandardAdam: 0.4896 SGD: 0.0987 Adam_with_adadelta_coeff: 0.0209\n",
      "Train Epoch: 10 [29600/40000 (74%)]\tLosses StandardAdam: 0.9174 SGD: 0.0350 Adam_with_adadelta_coeff: 0.0226\n",
      "Train Epoch: 10 [30400/40000 (76%)]\tLosses StandardAdam: 0.6611 SGD: 0.0029 Adam_with_adadelta_coeff: 0.0058\n",
      "Train Epoch: 10 [31200/40000 (78%)]\tLosses StandardAdam: 1.6170 SGD: 1.0104 Adam_with_adadelta_coeff: 0.2303\n",
      "Train Epoch: 10 [32000/40000 (80%)]\tLosses StandardAdam: 0.0000 SGD: 0.0015 Adam_with_adadelta_coeff: 0.0002\n",
      "Train Epoch: 10 [32800/40000 (82%)]\tLosses StandardAdam: 0.0274 SGD: 0.0418 Adam_with_adadelta_coeff: 0.0089\n",
      "Train Epoch: 10 [33600/40000 (84%)]\tLosses StandardAdam: 0.1809 SGD: 0.5408 Adam_with_adadelta_coeff: 0.0532\n",
      "Train Epoch: 10 [34400/40000 (86%)]\tLosses StandardAdam: 0.1664 SGD: 0.0401 Adam_with_adadelta_coeff: 0.0018\n",
      "Train Epoch: 10 [35200/40000 (88%)]\tLosses StandardAdam: 0.1782 SGD: 0.0286 Adam_with_adadelta_coeff: 0.0229\n",
      "Train Epoch: 10 [36000/40000 (90%)]\tLosses StandardAdam: 0.1167 SGD: 0.3250 Adam_with_adadelta_coeff: 0.0009\n",
      "Train Epoch: 10 [36800/40000 (92%)]\tLosses StandardAdam: 0.0750 SGD: 0.1513 Adam_with_adadelta_coeff: 0.0316\n",
      "Train Epoch: 10 [37600/40000 (94%)]\tLosses StandardAdam: 0.0064 SGD: 0.0012 Adam_with_adadelta_coeff: 0.0091\n",
      "Train Epoch: 10 [38400/40000 (96%)]\tLosses StandardAdam: 0.3319 SGD: 1.4969 Adam_with_adadelta_coeff: 0.0144\n",
      "Train Epoch: 10 [39200/40000 (98%)]\tLosses StandardAdam: 0.1751 SGD: 0.1417 Adam_with_adadelta_coeff: 0.0207\n",
      "Train Epoch: 10 [40000/40000 (100%)]\tLosses StandardAdam: 0.4186 SGD: 0.0725 Adam_with_adadelta_coeff: 0.1073\n",
      "Test set:\n",
      "StandardAdam: Loss: 0.7200\tAccuracy: 7692.0/10000 (77%)\n",
      "SGD: Loss: 0.7129\tAccuracy: 7950.0/10000 (80%)\n",
      "Adam_with_adadelta_coeff: Loss: 0.6956\tAccuracy: 8186.0/10000 (82%)\n",
      "\n",
      "Train Epoch: 11 [0/40000 (0%)]\tLosses StandardAdam: 0.1855 SGD: 0.0779 Adam_with_adadelta_coeff: 0.0086\n",
      "Train Epoch: 11 [800/40000 (2%)]\tLosses StandardAdam: 0.8212 SGD: 1.0516 Adam_with_adadelta_coeff: 0.8238\n",
      "Train Epoch: 11 [1600/40000 (4%)]\tLosses StandardAdam: 0.0774 SGD: 0.2784 Adam_with_adadelta_coeff: 0.0341\n",
      "Train Epoch: 11 [2400/40000 (6%)]\tLosses StandardAdam: 0.0726 SGD: 1.0751 Adam_with_adadelta_coeff: 0.0054\n",
      "Train Epoch: 11 [3200/40000 (8%)]\tLosses StandardAdam: 0.0856 SGD: 0.0595 Adam_with_adadelta_coeff: 0.0042\n",
      "Train Epoch: 11 [4000/40000 (10%)]\tLosses StandardAdam: 0.3514 SGD: 0.5313 Adam_with_adadelta_coeff: 0.0049\n",
      "Train Epoch: 11 [4800/40000 (12%)]\tLosses StandardAdam: 0.2776 SGD: 0.2968 Adam_with_adadelta_coeff: 0.0041\n",
      "Train Epoch: 11 [5600/40000 (14%)]\tLosses StandardAdam: 0.0873 SGD: 0.2371 Adam_with_adadelta_coeff: 0.0147\n",
      "Train Epoch: 11 [6400/40000 (16%)]\tLosses StandardAdam: 0.1242 SGD: 0.1328 Adam_with_adadelta_coeff: 0.0252\n",
      "Train Epoch: 11 [7200/40000 (18%)]\tLosses StandardAdam: 0.2164 SGD: 1.1939 Adam_with_adadelta_coeff: 0.0812\n",
      "Train Epoch: 11 [8000/40000 (20%)]\tLosses StandardAdam: 0.1341 SGD: 0.0565 Adam_with_adadelta_coeff: 0.0117\n",
      "Train Epoch: 11 [8800/40000 (22%)]\tLosses StandardAdam: 0.0704 SGD: 0.0106 Adam_with_adadelta_coeff: 0.0936\n",
      "Train Epoch: 11 [9600/40000 (24%)]\tLosses StandardAdam: 0.1582 SGD: 0.0800 Adam_with_adadelta_coeff: 0.0243\n",
      "Train Epoch: 11 [10400/40000 (26%)]\tLosses StandardAdam: 0.8023 SGD: 0.3599 Adam_with_adadelta_coeff: 0.0000\n",
      "Train Epoch: 11 [11200/40000 (28%)]\tLosses StandardAdam: 0.4904 SGD: 0.2882 Adam_with_adadelta_coeff: 0.0111\n",
      "Train Epoch: 11 [12000/40000 (30%)]\tLosses StandardAdam: 0.0071 SGD: 0.0045 Adam_with_adadelta_coeff: 0.0007\n",
      "Train Epoch: 11 [12800/40000 (32%)]\tLosses StandardAdam: 0.0514 SGD: 1.4910 Adam_with_adadelta_coeff: 0.0053\n",
      "Train Epoch: 11 [13600/40000 (34%)]\tLosses StandardAdam: 0.7970 SGD: 1.2920 Adam_with_adadelta_coeff: 0.1185\n",
      "Train Epoch: 11 [14400/40000 (36%)]\tLosses StandardAdam: 0.0014 SGD: 0.0017 Adam_with_adadelta_coeff: 0.0004\n",
      "Train Epoch: 11 [15200/40000 (38%)]\tLosses StandardAdam: 0.0161 SGD: 0.0640 Adam_with_adadelta_coeff: 0.0009\n",
      "Train Epoch: 11 [16000/40000 (40%)]\tLosses StandardAdam: 0.1319 SGD: 0.0347 Adam_with_adadelta_coeff: 0.0054\n",
      "Train Epoch: 11 [16800/40000 (42%)]\tLosses StandardAdam: 0.4650 SGD: 1.1129 Adam_with_adadelta_coeff: 0.0031\n",
      "Train Epoch: 11 [17600/40000 (44%)]\tLosses StandardAdam: 0.0294 SGD: 0.1477 Adam_with_adadelta_coeff: 0.1734\n",
      "Train Epoch: 11 [18400/40000 (46%)]\tLosses StandardAdam: 0.2188 SGD: 0.0114 Adam_with_adadelta_coeff: 0.0099\n",
      "Train Epoch: 11 [19200/40000 (48%)]\tLosses StandardAdam: 0.2157 SGD: 0.1610 Adam_with_adadelta_coeff: 0.0503\n",
      "Train Epoch: 11 [20000/40000 (50%)]\tLosses StandardAdam: 0.0714 SGD: 0.0530 Adam_with_adadelta_coeff: 0.0107\n",
      "Train Epoch: 11 [20800/40000 (52%)]\tLosses StandardAdam: 0.7038 SGD: 0.4108 Adam_with_adadelta_coeff: 0.0026\n",
      "Train Epoch: 11 [21600/40000 (54%)]\tLosses StandardAdam: 0.0242 SGD: 0.4812 Adam_with_adadelta_coeff: 0.0113\n",
      "Train Epoch: 11 [22400/40000 (56%)]\tLosses StandardAdam: 0.0639 SGD: 0.0515 Adam_with_adadelta_coeff: 0.0003\n",
      "Train Epoch: 11 [23200/40000 (58%)]\tLosses StandardAdam: 0.0205 SGD: 0.0020 Adam_with_adadelta_coeff: 0.1292\n",
      "Train Epoch: 11 [24000/40000 (60%)]\tLosses StandardAdam: 0.3656 SGD: 1.7321 Adam_with_adadelta_coeff: 0.0491\n",
      "Train Epoch: 11 [24800/40000 (62%)]\tLosses StandardAdam: 0.0048 SGD: 0.0066 Adam_with_adadelta_coeff: 0.0606\n",
      "Train Epoch: 11 [25600/40000 (64%)]\tLosses StandardAdam: 0.0683 SGD: 0.3746 Adam_with_adadelta_coeff: 0.0696\n",
      "Train Epoch: 11 [26400/40000 (66%)]\tLosses StandardAdam: 0.1978 SGD: 0.4841 Adam_with_adadelta_coeff: 0.1436\n",
      "Train Epoch: 11 [27200/40000 (68%)]\tLosses StandardAdam: 1.2454 SGD: 0.8452 Adam_with_adadelta_coeff: 0.7349\n",
      "Train Epoch: 11 [28000/40000 (70%)]\tLosses StandardAdam: 0.0356 SGD: 0.0078 Adam_with_adadelta_coeff: 0.0059\n",
      "Train Epoch: 11 [28800/40000 (72%)]\tLosses StandardAdam: 0.0151 SGD: 0.0001 Adam_with_adadelta_coeff: 0.0002\n",
      "Train Epoch: 11 [29600/40000 (74%)]\tLosses StandardAdam: 0.3589 SGD: 0.2446 Adam_with_adadelta_coeff: 0.0002\n",
      "Train Epoch: 11 [30400/40000 (76%)]\tLosses StandardAdam: 0.0061 SGD: 0.0208 Adam_with_adadelta_coeff: 0.0001\n",
      "Train Epoch: 11 [31200/40000 (78%)]\tLosses StandardAdam: 0.2198 SGD: 0.1051 Adam_with_adadelta_coeff: 0.0237\n",
      "Train Epoch: 11 [32000/40000 (80%)]\tLosses StandardAdam: 0.0426 SGD: 0.0047 Adam_with_adadelta_coeff: 0.0009\n",
      "Train Epoch: 11 [32800/40000 (82%)]\tLosses StandardAdam: 0.0035 SGD: 0.0135 Adam_with_adadelta_coeff: 0.0001\n",
      "Train Epoch: 11 [33600/40000 (84%)]\tLosses StandardAdam: 0.0196 SGD: 0.0016 Adam_with_adadelta_coeff: 0.0000\n",
      "Train Epoch: 11 [34400/40000 (86%)]\tLosses StandardAdam: 0.1405 SGD: 0.1121 Adam_with_adadelta_coeff: 0.0155\n",
      "Train Epoch: 11 [35200/40000 (88%)]\tLosses StandardAdam: 0.1686 SGD: 0.1470 Adam_with_adadelta_coeff: 0.0305\n",
      "Train Epoch: 11 [36000/40000 (90%)]\tLosses StandardAdam: 0.4890 SGD: 0.0899 Adam_with_adadelta_coeff: 0.0192\n",
      "Train Epoch: 11 [36800/40000 (92%)]\tLosses StandardAdam: 0.2010 SGD: 0.3496 Adam_with_adadelta_coeff: 0.6583\n",
      "Train Epoch: 11 [37600/40000 (94%)]\tLosses StandardAdam: 0.3525 SGD: 0.0831 Adam_with_adadelta_coeff: 0.0351\n",
      "Train Epoch: 11 [38400/40000 (96%)]\tLosses StandardAdam: 0.0726 SGD: 0.0982 Adam_with_adadelta_coeff: 0.2335\n",
      "Train Epoch: 11 [39200/40000 (98%)]\tLosses StandardAdam: 1.0347 SGD: 0.0247 Adam_with_adadelta_coeff: 0.0003\n",
      "Train Epoch: 11 [40000/40000 (100%)]\tLosses StandardAdam: 0.5869 SGD: 0.5486 Adam_with_adadelta_coeff: 0.0115\n",
      "Test set:\n",
      "StandardAdam: Loss: 0.8037\tAccuracy: 7704.0/10000 (77%)\n",
      "SGD: Loss: 0.8386\tAccuracy: 7767.0/10000 (78%)\n",
      "Adam_with_adadelta_coeff: Loss: 0.7464\tAccuracy: 8185.0/10000 (82%)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 12 [0/40000 (0%)]\tLosses StandardAdam: 0.4727 SGD: 0.3907 Adam_with_adadelta_coeff: 0.1605\n",
      "Train Epoch: 12 [800/40000 (2%)]\tLosses StandardAdam: 0.0103 SGD: 0.0250 Adam_with_adadelta_coeff: 0.0007\n",
      "Train Epoch: 12 [1600/40000 (4%)]\tLosses StandardAdam: 0.4970 SGD: 0.0037 Adam_with_adadelta_coeff: 0.0155\n",
      "Train Epoch: 12 [2400/40000 (6%)]\tLosses StandardAdam: 0.2102 SGD: 0.6072 Adam_with_adadelta_coeff: 0.0055\n",
      "Train Epoch: 12 [3200/40000 (8%)]\tLosses StandardAdam: 0.1332 SGD: 0.0246 Adam_with_adadelta_coeff: 0.0003\n",
      "Train Epoch: 12 [4000/40000 (10%)]\tLosses StandardAdam: 0.0303 SGD: 1.3571 Adam_with_adadelta_coeff: 0.0705\n",
      "Train Epoch: 12 [4800/40000 (12%)]\tLosses StandardAdam: 0.1400 SGD: 0.0442 Adam_with_adadelta_coeff: 0.0001\n",
      "Train Epoch: 12 [5600/40000 (14%)]\tLosses StandardAdam: 0.1093 SGD: 0.6541 Adam_with_adadelta_coeff: 0.2003\n",
      "Train Epoch: 12 [6400/40000 (16%)]\tLosses StandardAdam: 0.5456 SGD: 1.5637 Adam_with_adadelta_coeff: 0.0565\n",
      "Train Epoch: 12 [7200/40000 (18%)]\tLosses StandardAdam: 0.4356 SGD: 0.0103 Adam_with_adadelta_coeff: 0.0164\n",
      "Train Epoch: 12 [8000/40000 (20%)]\tLosses StandardAdam: 0.2403 SGD: 0.0035 Adam_with_adadelta_coeff: 0.0004\n",
      "Train Epoch: 12 [8800/40000 (22%)]\tLosses StandardAdam: 0.4266 SGD: 0.9223 Adam_with_adadelta_coeff: 0.1784\n",
      "Train Epoch: 12 [9600/40000 (24%)]\tLosses StandardAdam: 0.4200 SGD: 0.0238 Adam_with_adadelta_coeff: 0.0732\n",
      "Train Epoch: 12 [10400/40000 (26%)]\tLosses StandardAdam: 1.4553 SGD: 0.1037 Adam_with_adadelta_coeff: 0.3651\n",
      "Train Epoch: 12 [11200/40000 (28%)]\tLosses StandardAdam: 0.2524 SGD: 0.0059 Adam_with_adadelta_coeff: 0.0041\n",
      "Train Epoch: 12 [12000/40000 (30%)]\tLosses StandardAdam: 0.3204 SGD: 0.1344 Adam_with_adadelta_coeff: 0.5456\n",
      "Train Epoch: 12 [12800/40000 (32%)]\tLosses StandardAdam: 0.3251 SGD: 0.3554 Adam_with_adadelta_coeff: 0.0054\n",
      "Train Epoch: 12 [13600/40000 (34%)]\tLosses StandardAdam: 0.0240 SGD: 0.7447 Adam_with_adadelta_coeff: 0.0223\n",
      "Train Epoch: 12 [14400/40000 (36%)]\tLosses StandardAdam: 0.6170 SGD: 0.6194 Adam_with_adadelta_coeff: 0.0003\n",
      "Train Epoch: 12 [15200/40000 (38%)]\tLosses StandardAdam: 0.1357 SGD: 0.0006 Adam_with_adadelta_coeff: 0.0101\n",
      "Train Epoch: 12 [16000/40000 (40%)]\tLosses StandardAdam: 0.1506 SGD: 0.0178 Adam_with_adadelta_coeff: 0.0130\n",
      "Train Epoch: 12 [16800/40000 (42%)]\tLosses StandardAdam: 0.0102 SGD: 0.0116 Adam_with_adadelta_coeff: 0.0047\n",
      "Train Epoch: 12 [17600/40000 (44%)]\tLosses StandardAdam: 0.8665 SGD: 0.1275 Adam_with_adadelta_coeff: 0.0310\n",
      "Train Epoch: 12 [18400/40000 (46%)]\tLosses StandardAdam: 0.6360 SGD: 0.1702 Adam_with_adadelta_coeff: 0.0018\n",
      "Train Epoch: 12 [19200/40000 (48%)]\tLosses StandardAdam: 0.0698 SGD: 0.0126 Adam_with_adadelta_coeff: 0.0001\n",
      "Train Epoch: 12 [20000/40000 (50%)]\tLosses StandardAdam: 0.1654 SGD: 0.0378 Adam_with_adadelta_coeff: 0.1390\n",
      "Train Epoch: 12 [20800/40000 (52%)]\tLosses StandardAdam: 0.0510 SGD: 0.1657 Adam_with_adadelta_coeff: 0.0770\n",
      "Train Epoch: 12 [21600/40000 (54%)]\tLosses StandardAdam: 0.4544 SGD: 0.0349 Adam_with_adadelta_coeff: 0.0570\n",
      "Train Epoch: 12 [22400/40000 (56%)]\tLosses StandardAdam: 0.2535 SGD: 0.0743 Adam_with_adadelta_coeff: 0.0005\n",
      "Train Epoch: 12 [23200/40000 (58%)]\tLosses StandardAdam: 0.1779 SGD: 0.0332 Adam_with_adadelta_coeff: 0.0149\n",
      "Train Epoch: 12 [24000/40000 (60%)]\tLosses StandardAdam: 0.0133 SGD: 0.0009 Adam_with_adadelta_coeff: 0.0004\n",
      "Train Epoch: 12 [24800/40000 (62%)]\tLosses StandardAdam: 1.3991 SGD: 0.0449 Adam_with_adadelta_coeff: 0.0018\n",
      "Train Epoch: 12 [25600/40000 (64%)]\tLosses StandardAdam: 0.1362 SGD: 0.0602 Adam_with_adadelta_coeff: 0.0932\n",
      "Train Epoch: 12 [26400/40000 (66%)]\tLosses StandardAdam: 0.3608 SGD: 0.0372 Adam_with_adadelta_coeff: 0.0052\n",
      "Train Epoch: 12 [27200/40000 (68%)]\tLosses StandardAdam: 2.1398 SGD: 0.0827 Adam_with_adadelta_coeff: 0.2123\n",
      "Train Epoch: 12 [28000/40000 (70%)]\tLosses StandardAdam: 0.9646 SGD: 0.6096 Adam_with_adadelta_coeff: 0.4331\n",
      "Train Epoch: 12 [28800/40000 (72%)]\tLosses StandardAdam: 0.4929 SGD: 1.1644 Adam_with_adadelta_coeff: 0.7198\n",
      "Train Epoch: 12 [29600/40000 (74%)]\tLosses StandardAdam: 0.3358 SGD: 0.0436 Adam_with_adadelta_coeff: 0.0006\n",
      "Train Epoch: 12 [30400/40000 (76%)]\tLosses StandardAdam: 0.4504 SGD: 0.0102 Adam_with_adadelta_coeff: 0.0040\n",
      "Train Epoch: 12 [31200/40000 (78%)]\tLosses StandardAdam: 0.0852 SGD: 0.4017 Adam_with_adadelta_coeff: 0.6607\n",
      "Train Epoch: 12 [32000/40000 (80%)]\tLosses StandardAdam: 0.0005 SGD: 0.0025 Adam_with_adadelta_coeff: 0.0033\n",
      "Train Epoch: 12 [32800/40000 (82%)]\tLosses StandardAdam: 0.2832 SGD: 0.0017 Adam_with_adadelta_coeff: 0.0002\n",
      "Train Epoch: 12 [33600/40000 (84%)]\tLosses StandardAdam: 0.0657 SGD: 0.1159 Adam_with_adadelta_coeff: 0.0019\n",
      "Train Epoch: 12 [34400/40000 (86%)]\tLosses StandardAdam: 0.0001 SGD: 0.0001 Adam_with_adadelta_coeff: 0.0000\n",
      "Train Epoch: 12 [35200/40000 (88%)]\tLosses StandardAdam: 0.1483 SGD: 0.1935 Adam_with_adadelta_coeff: 0.0308\n",
      "Train Epoch: 12 [36000/40000 (90%)]\tLosses StandardAdam: 0.0503 SGD: 0.8188 Adam_with_adadelta_coeff: 0.0931\n",
      "Train Epoch: 12 [36800/40000 (92%)]\tLosses StandardAdam: 0.3272 SGD: 0.5617 Adam_with_adadelta_coeff: 1.0095\n",
      "Train Epoch: 12 [37600/40000 (94%)]\tLosses StandardAdam: 0.6373 SGD: 0.8557 Adam_with_adadelta_coeff: 0.3333\n",
      "Train Epoch: 12 [38400/40000 (96%)]\tLosses StandardAdam: 0.1319 SGD: 0.0075 Adam_with_adadelta_coeff: 0.0034\n",
      "Train Epoch: 12 [39200/40000 (98%)]\tLosses StandardAdam: 0.0113 SGD: 0.0129 Adam_with_adadelta_coeff: 0.0003\n",
      "Train Epoch: 12 [40000/40000 (100%)]\tLosses StandardAdam: 0.3448 SGD: 0.1673 Adam_with_adadelta_coeff: 0.0681\n",
      "Test set:\n",
      "StandardAdam: Loss: 0.8365\tAccuracy: 7657.0/10000 (77%)\n",
      "SGD: Loss: 0.8197\tAccuracy: 7870.0/10000 (79%)\n",
      "Adam_with_adadelta_coeff: Loss: 0.7236\tAccuracy: 8206.0/10000 (82%)\n",
      "\n",
      "Train Epoch: 13 [0/40000 (0%)]\tLosses StandardAdam: 0.3258 SGD: 0.0322 Adam_with_adadelta_coeff: 0.0010\n",
      "Train Epoch: 13 [800/40000 (2%)]\tLosses StandardAdam: 0.1038 SGD: 0.0050 Adam_with_adadelta_coeff: 0.0114\n",
      "Train Epoch: 13 [1600/40000 (4%)]\tLosses StandardAdam: 0.0371 SGD: 2.9613 Adam_with_adadelta_coeff: 0.0669\n",
      "Train Epoch: 13 [2400/40000 (6%)]\tLosses StandardAdam: 0.0865 SGD: 0.0023 Adam_with_adadelta_coeff: 0.0005\n",
      "Train Epoch: 13 [3200/40000 (8%)]\tLosses StandardAdam: 1.3053 SGD: 0.3550 Adam_with_adadelta_coeff: 0.0950\n",
      "Train Epoch: 13 [4000/40000 (10%)]\tLosses StandardAdam: 0.3985 SGD: 0.0949 Adam_with_adadelta_coeff: 0.0128\n",
      "Train Epoch: 13 [4800/40000 (12%)]\tLosses StandardAdam: 0.5220 SGD: 0.2247 Adam_with_adadelta_coeff: 0.5834\n",
      "Train Epoch: 13 [5600/40000 (14%)]\tLosses StandardAdam: 0.5935 SGD: 0.4817 Adam_with_adadelta_coeff: 0.1092\n",
      "Train Epoch: 13 [6400/40000 (16%)]\tLosses StandardAdam: 0.0329 SGD: 0.0013 Adam_with_adadelta_coeff: 0.0390\n",
      "Train Epoch: 13 [7200/40000 (18%)]\tLosses StandardAdam: 0.0687 SGD: 0.0059 Adam_with_adadelta_coeff: 0.0043\n",
      "Train Epoch: 13 [8000/40000 (20%)]\tLosses StandardAdam: 0.0012 SGD: 0.1339 Adam_with_adadelta_coeff: 0.0001\n",
      "Train Epoch: 13 [8800/40000 (22%)]\tLosses StandardAdam: 0.0041 SGD: 0.0007 Adam_with_adadelta_coeff: 0.0002\n",
      "Train Epoch: 13 [9600/40000 (24%)]\tLosses StandardAdam: 0.0843 SGD: 0.0023 Adam_with_adadelta_coeff: 0.0030\n",
      "Train Epoch: 13 [10400/40000 (26%)]\tLosses StandardAdam: 0.0001 SGD: 0.0004 Adam_with_adadelta_coeff: 0.0001\n",
      "Train Epoch: 13 [11200/40000 (28%)]\tLosses StandardAdam: 0.0017 SGD: 0.0001 Adam_with_adadelta_coeff: 0.0035\n",
      "Train Epoch: 13 [12000/40000 (30%)]\tLosses StandardAdam: 0.0024 SGD: 0.0054 Adam_with_adadelta_coeff: 0.0012\n",
      "Train Epoch: 13 [12800/40000 (32%)]\tLosses StandardAdam: 0.8254 SGD: 0.3704 Adam_with_adadelta_coeff: 0.0015\n",
      "Train Epoch: 13 [13600/40000 (34%)]\tLosses StandardAdam: 0.0577 SGD: 0.0591 Adam_with_adadelta_coeff: 0.0047\n",
      "Train Epoch: 13 [14400/40000 (36%)]\tLosses StandardAdam: 0.0551 SGD: 0.0537 Adam_with_adadelta_coeff: 0.0142\n",
      "Train Epoch: 13 [15200/40000 (38%)]\tLosses StandardAdam: 0.2629 SGD: 0.1818 Adam_with_adadelta_coeff: 0.0019\n",
      "Train Epoch: 13 [16000/40000 (40%)]\tLosses StandardAdam: 1.5127 SGD: 0.4392 Adam_with_adadelta_coeff: 0.0006\n",
      "Train Epoch: 13 [16800/40000 (42%)]\tLosses StandardAdam: 0.1413 SGD: 0.0813 Adam_with_adadelta_coeff: 0.0041\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 13 [17600/40000 (44%)]\tLosses StandardAdam: 0.0462 SGD: 0.0189 Adam_with_adadelta_coeff: 0.2312\n",
      "Train Epoch: 13 [18400/40000 (46%)]\tLosses StandardAdam: 0.1212 SGD: 0.2600 Adam_with_adadelta_coeff: 0.0003\n",
      "Train Epoch: 13 [19200/40000 (48%)]\tLosses StandardAdam: 0.5235 SGD: 0.3902 Adam_with_adadelta_coeff: 0.3852\n",
      "Train Epoch: 13 [20000/40000 (50%)]\tLosses StandardAdam: 0.1723 SGD: 0.3059 Adam_with_adadelta_coeff: 0.0066\n",
      "Train Epoch: 13 [20800/40000 (52%)]\tLosses StandardAdam: 1.1137 SGD: 0.0114 Adam_with_adadelta_coeff: 0.3194\n",
      "Train Epoch: 13 [21600/40000 (54%)]\tLosses StandardAdam: 0.1392 SGD: 0.0110 Adam_with_adadelta_coeff: 0.0181\n",
      "Train Epoch: 13 [22400/40000 (56%)]\tLosses StandardAdam: 0.0125 SGD: 0.1675 Adam_with_adadelta_coeff: 0.0868\n",
      "Train Epoch: 13 [23200/40000 (58%)]\tLosses StandardAdam: 0.0267 SGD: 0.0052 Adam_with_adadelta_coeff: 0.0067\n",
      "Train Epoch: 13 [24000/40000 (60%)]\tLosses StandardAdam: 0.0034 SGD: 0.0194 Adam_with_adadelta_coeff: 0.0031\n",
      "Train Epoch: 13 [24800/40000 (62%)]\tLosses StandardAdam: 0.4327 SGD: 0.0897 Adam_with_adadelta_coeff: 0.0001\n",
      "Train Epoch: 13 [25600/40000 (64%)]\tLosses StandardAdam: 0.0039 SGD: 0.0384 Adam_with_adadelta_coeff: 0.0005\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-ae4f35d5a41c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_log\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-23-d21388007928>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(epoch, models, log)\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m             \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\pytorch\\lib\\site-packages\\torch\\optim\\optimizer.py\u001b[0m in \u001b[0;36mzero_grad\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    154\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m                     \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 156\u001b[1;33m                     \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    157\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    158\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclosure\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_epoch = 30\n",
    "for epoch in range(1, n_epoch+1):\n",
    "    for model in models.values():\n",
    "        model.train()\n",
    "    train(epoch, models, train_log)\n",
    "    for model in models.values():\n",
    "        model.eval()\n",
    "    test(models, valid_loader, test_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_graphs(test_log, 'loss') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzs3XlclVX+wPHPYZFFVkHcARUTxQUVNZdMp1LbNKupLM2aJlumpmlmbJsazZnKaWqm/LXaamaaZZk1mlq5r+CSC+4KguzIJuu93PP747ngBVEuyOWCfN+vF697n+c5z/N8QbxfzjnPOUdprRFCCCEuxsXZAQghhGj6JFkIIYSolSQLIYQQtZJkIYQQolaSLIQQQtRKkoUQQohaSbIQQghRK0kWQgghaiXJQggnUAb5/yeaDfllFS2aUuoZpdRxpVSBUipeKTXJ5tiDSqmDNscGWvd3UUp9o5TKVEplK6Xesu6fpZT63Ob8cKWUVkq5WbfXKaVeUkptBoqAbkqp+23ucUIp9VC1+CYqpfYopfKtcY5XSv1WKbWzWrm/KKWWOe4nJVo6SRaipTsOXAX4Ay8CnyulOiilfgvMAu4F/IAJQLZSyhX4AUgEwoFOwOI63G8qMB3wtV4jA7jJeo/7gf/aJKUhwGfADCAAGAUkAMuBrkqpXjbXnQIsqNN3LkQdSLIQLZrW+iutdYrW2qK1/hI4CgwBfg+8qrWO1YZjWutE67GOwAytdaHWukRrvakOt/xUa31Aa23WWpu01v/TWh+33mM9sBojeQE8AHystV5jje+01vqQ1roU+BIjQaCUisJIXD80wI9EiBpJshAtmlLqXmszT65SKhfoAwQDXTBqHdV1ARK11uZ63jKp2v2vV0ptU0qdsd7/Buv9K+5VUwwA84G7lVIKo7ayxJpEhHAISRaixVJKhQEfAI8BQVrrAGA/oDA+1LvXcFoSEFrRD1FNIeBts92+hjKV0zwrpTyApcBrQDvr/VdY719xr5piQGu9DSjDqIXcjTRBCQeTZCFastYYH96ZAEqp+zFqFgAfAn9VSg2yPrkUYU0uO4BUYI5SqrVSylMpNcJ6zh5glFIqVCnlDzxby/1bAR7W+5uVUtcDY22OfwTcr5S6RinlopTqpJSKtDn+GfAWYK5jU5gQdSbJQrRYWut44HVgK5AO9AU2W499BbwEfAEUAMuANlrrcuBmIAI4BSQDd1rPWYPRl7AX2EktfQha6wLgj8ASIAejhrDc5vgOrJ3eQB6wHgizucQCjOQmtQrhcEoWPxKieVJKeWE8TTVQa33U2fGIy5vULIRovh4BYiVRiMZQUyedEKKJU0olYHSE3+LkUEQLIc1QQgghaiXNUEIIIWp12TRDBQcH6/DwcGeHIYQQzcrOnTuztNZtayt32SSL8PBw4uLinB2GEEI0K0qpRHvKSTOUEEKIWkmyEEIIUStJFkIIIWolyUIIIUStJFkIIYSolSQLIYQQtZJkIYQQolaXzTgLIUTzYLKY+OXUL+SV5hHTPoaufl0xFvwTTZkkCyFEoygoK2DpkaUsPLSQtMK0yv1tvdoyuP1ghnYYyuD2g+ns01mSRxMkyUII4VApZ1P4/ODnfHP0GwpNhQxuP5i/Df0bXf27EpsWy47UHWxP3c6KkysA6Ni6Y5Xk0b51TavTisZ22cw6GxMTo2W6DyGajv1Z+5l/YD5rEtcAMC58HPdG3UtUUNR5ZbXWnMg7wfbU7cSmxRKbHkteaR4AYX5hRvJoP5SY9jEEewU36vdxuVNK7dRax9RaTpKFEKKhWLSFdUnrmH9gPrsyduHj7sPtV9zOPb3uqVMNwaItHMk5Upk84tLjKDQVAhAREMGQ9kMY0n4IMe1j8Pfwd9S30yJIshBCNJpiczHLjy1nwcEFJOYn0qF1B6b0msKtPW7Fp5XPJV/fbDFzMPsg29OM5LErfRcl5SUoFJFtIo3k0WEIg9oNorV76wb4jloOSRZCCIfLKs5i0aFFLDm8hNzSXPoE9WFa1DSuDbsWNxfHdYmayk3sy9pXmTz2ZOzBZDHhqlyJCo6qrHlEh0Tj5eblsDgamtaakvISis3FxpepmCJz0bltczFFpnPbFcdCvEP4XZ/f1euekiyEEA5zLOcYn8V/xg8nfsBsMTO6y2imRU1jYMhApzzJVGIuYU/mHnak7mBH2g4OZB3ArM24u7jTr20/BrcfTKBHIEBlfIpqr9Xirq1cxXZN5S3aUvUD3vYDv1oCsP3wLzYXo7H/M9lVueLl5kX/kP68d+179v/AqsYtyUKI5qjEXEJOSQ75ZfkEeQUR5BnUJB4l1VqzLXUb8+Pns/n0ZjxdPZkYMZEpvaYQ7h/u7PCqKDQVsit9F7FpsWxP287B7IN1+hBuSAqFl5vXuS93L7zdvKvs83avtl1x3Kas7TkV5d1d3C/5d8PeZCGPzgrhQOWWcvLL8skpySGnNKfyNbcklzMlZ8gtza2yL6c0h2JzcZVr+Lj7EOYXRphfGOF+4cZ7/zDCfMMapD+gNqZyEysTVvLZgc84nHOYIM8gHot+jDt63kGgZ6DD718frd1bc1Xnq7iq81UAFJmKKC0vrUwYFX8kV08g1ffbXa5aeaXOJQhPV88mkewvlSQLIeqgyFRU5QM+p8T4yi2t+cM/tzT3gn/Rert5E+gZSKBHIG0829Ddv7uxbd3n28qXzOJMEvMTScxP5NfMX1l5cmWV67X1altjIuni0wV3V/dL+l7zSvP46shXLDq4iIziDCICIpg9fDY3druRVq6tLunajc3b3Rtvd29nh9GsSbIQ4iJKy0uJTYtlXdI6NiRvILUwtcZyrsqVAI+Ayg/7iIAI2ni2ObfPI5AAz4Aq+zxcPeoVT1J+Egn5CSTkJ1QmkrVJazlTcqZKPJ18OlVNJP7Ga4h3CC7qwtPCJeUnseDgApYdW0axuZhhHYYxe8Rshnccfln8hSzqR5KFENVkFWexMXkj65LWsTV1K8XmYrzcvLiyw5Xc0fMOgjyDqiSGAI8A/Fr5NcoHqYerBxGBEUQERpx3LK80j1P5p85LJHHpcVWatjxdPQn1C62siYT7G6+mchNfHPqCn0/9jIty4YauN3Bv73vp2aanw78v0fRJB7do8bTWHMk5wvrk9axPWs++rH1oNO2823F156u5usvVDGk/BE83T2eHWi9aazKKMkjMTzwvkSQXJFOuyyvL+rXy446edzA5cjIh3iFOjFo0FungFuIiLtS81CeoD49GP8roLqPpGdjzsmh2UUrRrnU72rVux5AOQ6ocM1lMnC44TWJ+IoWmQkZ3GS1t+6JGkixEi1HRvLQ+eT1bUrZUaV56uP/DjOo8qsXNO+Tu4k64f3iTe/RVND0OTRZKqfHAm4Ar8KHWek6146HAfCDAWuYZrfUK67FngQeAcuCPWutVjoxVXLqCsgI+i/+Mcks5Id4hhHiH0M67HSHeIbTxbIOri2ujxnOx5qUJ3ScwqvOoZt28JERjcliyUEq5Am8D1wHJQKxSarnWOt6m2PPAEq31u0qp3sAKINz6/i4gCugI/KSUukJrm8ZV0aTszdzLUxueIuVsCi7KpUo7OBhP5wR5BVUmD9tk0ta7beX7S53Xp6y8jB1pO1pE89Il0RrOpkPGQcg8ZLwWZUPHaAgdBp0GgXvzmSZDOJ4jaxZDgGNa6xMASqnFwETANllowM/63h9Isb6fCCzWWpcCJ5VSx6zX2+rAeEU9WLSFT/Z/wlu73yLEO4TPrv+MvsF9ySnNIb0onYzCDDKKMoz3RRmVHa070nZQUFZw3vVau7eumki82lapoYR4hxDkFVRl3qHs4mw2JG+Q5qWaaA2FmVWTQsVrSe65cl6B4B0Eh34wtl3czyWO0GHQZSi0DnLO9yCaBEcmi05Aks12MjC0WplZwGql1ONAa+Bam3O3VTu3U/UbKKWmA9MBQkNDGyRoYb+s4iye2/gcW1O3cl3YdcwaPgu/VkbuD/YKJtgruMa1CyoUmYrILM48L5lUfMWmxZJZlIlZm6uc56JcCPIMqnxaJz47XpqXAAqzak4KxefGX+AZACG9IGqS8do20nht3RaUgqIzkBwLp7ZC4lbY/h5smWucG9wTQq+0JpArITDcOEe0CI5MFjX9FlV/Tncy8KnW+nWl1DBggVKqj53norWeB8wD49HZS4xX1MHm05t5btNzFJoK+fuwv3N7j9vr3LTj7e5NmLsxaOxCLNrCmZIzVZJIelE6mUVGkiktL215zUtFZ6zJ4CBkHLJpRso6V8bDH0IiodfNVZOCT7uLf8B7t4ErxhlfAKYSSNltJI9T2yB+Geyabxzz7VA1ebTrA43cLyUajyOTRTLQxWa7M+eamSo8AIwH0FpvVUp5AsF2niucwFRuYu7uuXx64FMiAiL4aOxHNQ4QayguyqWyltI7qLfD7tMkFedCRny12sJhKMw4V6aVr5EUel5fNSn4dmiYv/rdPSFsmPEFYLEYSaoieZzaBge+PRdLl8Hnmq46DYJWjfQYrsVi1KDOZhg/n7OZ1tcMoxmu6Ax0GQLRd4OvLNNaHw4blKeUcgOOANcAp4FY4G6t9QGbMiuBL7XWnyqlegE/YzQ39Qa+wOin6Gjd3+NiHdwyKM/xkvKTeGrDU+zP3s+dPe/krzF/bXlNPY5UdAYSN0PCJuMrff+5Y618rIkg0nht28t479fJ+U1BuUnWxGFNIBnxgAYXN+gQXbX20boOfUeWcqPT/UIJoMr+TKjp48HFHXxCoFVryDoCyhV6XAcDpsAV4+ES58+6HDSJKcqVUjcAb2A8Fvux1volpdRsIE5rvdz61NMHgA9GM9NTWuvV1nP/BvwOMAN/0lqvvNi9JFk41g8nfuCf2/6Ji3Jh9vDZXBt2be0niYu7UHJw84LQoRA20uhkbhsJ/p2dnxTsVZwDSbHnksfpnVBeahwLvuJc8vAJuUACsL4WZYG2nH99Vw/j3NZtq72GGK+V79safTQVP7fs47D7c9jzBZxNM87rdycMmGok3haqSSSLxiTJwjGKTEW8tP0llh9fzoCQAfzrqn/RwaeDs8NqnmpLDuEjIfwq6DgQ3JrXrK4XZSqB1D1Vm65sn8QC42fg0/bcB/7FEoCH36UlznIzHP8Fdn8Gh1eCxQydBxu1jahbwdOv9mtcRiRZiEt2MPsgT214isT8RKb3m87D/R926FKZl53C7KrJIcPaAuvubTyKWpkcBlxeyaE2FovRB1Oafy4ptPJxTs2pMAv2fgm7Fhh9MW5eEHWLUdsIG948anPlJuP78KvfH3GSLES9aa1ZeHAh/9n5HwI9Apkzag6D2w92dlhNnySH5ktrOL0Ldi+A/UuNRNamG0TfY3SK+3V0doQGrSEv2Xi8+fROSI4zam0dB8DvfqzXJSVZiHrJKcnhhc0vsD55PaM7j2b2iNlNdjU0p5PkcHkqK4KDy43+jYSNoFwg4lprp/j1jftvWVpgPLqcHAvJO+F0nDHyHsDNEzr0N5rQQodBr5vqdQtJFqLOYtNieWbDM+SU5vCXmL9wd+TdLWPcgr1K8uHEOkkOLUn2caNDfM8XUJBijHLvd5eRONo18KPclnKjeS45zkgKyXHGdkUnf1AEdIqBztavdn0a5GkuSRbCbmaLmXd/fZcP9n5AmF8Y/77630S2ablPh9To4A/ww5+MJ3XcvY0neiqSQ4doSQ52SjpTxKoDaayOT6fUbGFYtyCGdw8iJjwQ71ZNuD/MUm7tFF8Ah1aAxWQ8iDBwKvS5DTz9637NgnRrUog1EkPKbig7axzzDLAmhcFGgug00Bgw6QCSLIRdUs+m8vTGp9mdsZtbIm7h2SHPynoGtopzYOXTRido+74w7hWjFiHJwS5aaw6nF7Bqfzo/HkjjYGo+AJHtffHxcGNPUi5mi8bdVTGgSyDDuhvJIzo0AA+3JjoavDAb9i0xOsUzDhjNQb0nGrWNsJHgUsOStaZiSP21aq0hzzobkoub8btVWWsYbPSXNFKtXpKFqNVPiT/x9y1/x6It/P3Kv3NDtxucHVLTcnQNLH/cqE1c9VcY9VcZxGUHi0WzOymHVQfSWXUgjcTsIpSCQaGBjItqz7io9oQGGX+QFJaaiUvMYcvxLLYez2bf6Ty0Bk93FwaHt2F492CGdw8iqqMfbq4XXjfcKbQ2agO7P4d9X0NpnjFfVvQUiLjGGARYUWtI3288ogvgH3quKalTDHTo59QZfiVZiAsqMZfw79h/s+TIEvoE9eHVUa/Sxa9L7Se2FCX5sOo5o8mhbS+Y9J4xOE5cUJnZwtYT2aw6kMaa+HQyC0pxd1UM7x7MuKj2XNs7hBDf2kf75xWZ2H4ymy3Hs9l6PJvD6cbMxL6ebgztatQ6hkcEcUWILy4uTag/razImLF39wI4ueHc/la+0GmAtdYw2EgQPk1ruVpJFqJGx3KOMWPDDI7lHuP+qPt5fMDjuMtfy+ecWAffPQb5p2HEEzD6WXDzcHZUTVJRmZn1hzNZdSCNnw9lUFBixruVK6N7tmVcVHvGRIbg53lpv1uZBaVsPZHN1uNZbDmeTWJ2EQBBrVtxpbXJanj3YMKDvJvOwxg5CUZtIqQ3tO3Z5CdXlGTRROWV5rE6cTWt3VoT4BGAv6c//q38CfAIoLV7a4f9wmut+fro17y641W83b15ZeQrDO803CH3apZKz8Kav0PcRxDUA25515gUT1SRU1jGTwfTWXUgnY1HMyk1Wwj0dufaXu0YF9WekT2C8XR33Idjck4RW621ji3Hs0nLLwGgo78nw6xNVsMjgujgLws32UuSRRP1j63/YMmRJTUec3Nxq0wc/h7++HsY76tvV9/fyvXina35ZfnM2jKLNYlrGN5xOC+NfKnlLgZUk4TN8N2jkJMIVz4K17wgq8TZSM0rZvWBdH7cn8aOhDOUWzQd/T0Za+1/GBwe6JT+BK01J7MKK5ustp7I5kxhGQBdg1tXdpZf2S2IYB/7aodaa8wWjancQpnZQpn11VSura8WSq2vFdvn9p0ro7Um2NeDEF9PQnw9CPHzaLJPe0myaILSCtO4/pvrubnbzdzX5z7ySvPILckltzSXvNI88sryKt/nltrsL82jtGIithp4uXlVJg8/D78qicTH3YfFhxaTUZTBHwf+kWlR03BRTayj0FlMxfDzbNj2LgSGGbWJMKltARzLOGs84nogjV+T8wCICPFhXJRRg+jbyb/pNPtYWSzGk1dG8shi+4kzFJQancpXtPPBz9O98sO/rNz2A19X2eeoj0RfDzfa+nkQ4utBOz9rEvH1JMTP9tUDHw+3Rv3Z2pssmmaqu0x9tO8jAB7p/0idJ+MrNhfXmERq2j5ceJjc0lzyy/KxaAudfToby5227euIb6t5SoqFZQ9D9jEY/Hu49kXw8HF2VA6htcaiodyisWhNucX469li0ZRr49Vs0aTnl7Am3niC6XhmIQD9uwTw1PiejItqT/e2Tfvn4+Ki6NXBj14d/HhgZFfM5Rb2p+Sz5XgWsSfPUFZuwdfTDXdXF1q5udDK+uru6mKzT1Xus31t5Wq7T513fvUyWmuyzpaRnl9CRkEpGQUlZOSXkllQSnp+CbtP5ZJRUEKJ6fxZdb3cXQnx86Cdr2dlcgnx9aRdtaTi7+XeqElFkkUjSS9MZ+nRpUzsPrFes7Z6uXnh5eZF+9b2L9xi0RYKygpo7d5aJgCsYC6FtS8bS4X6dYJ7v4Nuo50Wjtaa/GIzKXnFpOQWk5JXQkpuMam5xaTmlVBithgf6jYf9OXWD/lyy7kP+irHKpMAleXs5eqiuLJbG6YND+e63u2addu/m6sL0V0CiO4SAKMb//5BPh70bO97weNaawpKzWTkG4mkIqmkV7zPL+FgSj7r8ksoLDt/rY5Wbi7WROJBdJdA/n6zYxcHk0+QRvLJgU/QWvP7vr9vtHu6KBf8PeoxsvRylbIbvn3EmF104L0w9iWHT0ddYionzZoATlsTQPWkUP2DwN1V0d7fkw5+XgR4uePqonBRClcX48Pc1cUFV2X8Je2qFG6uFcfPvbq5qMrjLtZt2+u4KJt91nI+nm6MjAgmwFsGHDYGpRR+nu74eboTEXLhpALGeJSKBJJufc0sMJJKen4JZ0tNDo9XkkUjyCzK5OsjX3Nz95vp7NvZ2eG0POYy2PgabHjNeMb9nq+N1dIuUblFk1lQWlkrSM0tsSaEYlJyS0jNKybrbNl55wX7eNAxwJOItj6M6tGWjgGedAzwooO/J50CvAj28WhaYwiE07X2cKOrhxtdg1s7LQZJFo3gkwOfYLaYebDvg84OpeVJPwDfPgRp+4wJ4K6fA151n0VXa82yPadZdzizMimk55dgrtbE07qVK50Cvejg70WfTv509LcmggAjEbT392y601gIcRGSLBwsqziLrw5/xY3dbpRR0o2p3Ayb34B1c8ArAO76AiJvrNelMgtKefabvfx0MIMO/p50aePNkK5t6GBNBOdqBl74eTbukyxCNBZJFg42/8B8yixlTO833dmhOJ7W1mUqTdCmuzEZWisnTEqYeRiWPWIsDhM1CW54HVoH1etSqw6k8dw3+ygoNfP8jb343Yiu0kQkWiRJFg50puQMXx7+khu63kCYX5izw3G8tS/Dhler7vPtYE0cXSHImkAqtls1cPurpRy2vQM//8O49u2fQJ9b63WpghITs7+P56udyUR19GPRndFc0e7inZBCXM4cmiyUUuOBNwFX4EOt9Zxqx/8LjLFuegMhWusA67FyYJ/12Cmt9QRHxuoI8w/Mp8RcwoP9WkBfxbb3jEQxYCoMedBYNObMcThz0nh/ZBUUZlQ9x7eDNXlYvyqTSbe6J5Ls47DsUUjaBj1vhJvfqPeEbdtPZPOXr34lJbeYx8ZE8MdretDKTQYyipbNYclCKeUKvA1cByQDsUqp5Vrr+IoyWusnbco/DgywuUSx1rrZTvWZU5LDokOLGN91PN38uzk7HMfa+xX8+DRE3gQ3vQGubsZyj9WV5EOONXmcOXHuq6ZE4tPemjy6nmvSqkgmtonEYoHYD+Gnmcb04ZPmQb876rUWQKm5nNdXH+GDjScIbePNVw8PZ1CYLCkrBDi2ZjEEOKa1PgGglFoMTATiL1B+MjDTgfE0qgXxCygxlzC972XeV3H0J2MkdPhVcNtHRqK4EE8/I4nUlEhKC84lj2xrjeTMcWNNibOfVy1rm0jOJEDiJmON5An/B34d6/VtxKfk8+cleziUVsDdQ0P52w29aO0hrbRCVHDk/4ZOQJLNdjIwtKaCSqkwoCvwi81uT6VUHGAG5mitl9Vw3nRgOkBoaGgDhX3p8krz+OLQF1wXdh0RgRHODsdxknbAkqnGVMx3fQHuta9XcEEevrUkEmvyOHMCsq1J5egaKC+Dm+cag+zqUZsot2jmbTjBf9YcJsC7FZ/cN5gxkU1rvQEhmgJHJoua/udeaN6Bu4Cvtda2Q1lDtdYpSqluwC9KqX1a6+NVLqb1PGAeGBMJNkTQDeHzg59TaCrkof4POTsUx8k4CAt/C77tYcpSx46E9vA1VhPr0O/8Y1rXe/nJpDNF/HnJHmITcri+T3temtSXNq1l9LIQNXFkskgGbAcWdAZSLlD2LuAPtju01inW1xNKqXUY/RnHzz+1ackvy2dh/EKuDb2WKwKvcHY4jpF7Chbcaqw9PPVb5678VY9EobVmSVwSs7+Px0Up/nNHfyYN6CTjI4S4CEcmi1igh1KqK3AaIyHcXb2QUqonEAhstdkXCBRprUuVUsHACODV6uc2RQsPLqTAVHD51ioKs2DBJDAVwv0rjTWHmxHbAXbDugXx2h396RTQfCfLE6KxOCxZaK3NSqnHgFUYj85+rLU+oJSaDcRprZdbi04GFuuqC2v0At5XSlkAF4w+iwt1jDcZBWUFLIhfwJguY4hsE+nscBpeaQEsvB3yko3ZWttFOTuiOpEBdkLUn0Mf99BarwBWVNv392rbs2o4bwvQ7BZfWHRoEQVlBTzc/2Fnh9LwzKWw+B5I3QuTF0Holc6OyG4ywE6ISyfPBjaQQlMhn8V/xtWdr6Z3kGPnlW90lnL45kE4uR4mvQ9XjHN2RHaTAXZCNAxJFg1k0aFF5JXmXX61Cq3hf3+B+O9g3MvQ/y5nR2SX8wfYDWNQWBtnhyVEsyXJogEUmYr47MBnjOw0kj7BfZwdTsNa+zLs/ARGPgnD/lB7+SZABtgJ0fDkf1AD+PLwl+SU5lx+tQrb+Z6uafqD62WAnRCOI8niEhWbi/n0wKcM7zic/m1rGH3cXFWf76mJj0GQAXZCOJYki0u05PASzpScubxqFXWZ78nJZICdEI2j6X4KNAMl5hI+2f8JQzsMZUDIgNpPaA4acr4nB0vLK+H5ZftkgJ0QjUCSxSX4+sjXZJdk81q/15wdSsNozPmeLkFBiYn315/gw00nsGhkgJ0QjUCSRT2Vlpfy8f6PiWkXQ0z7GGeHc+ma0nxPF2Aqt7Boxyne/Oko2YVlTOjfkRnjetKljROWbhWihZFkUU/fHP2GzOJM5lw1p/bCTV0Tn+9Ja82qA+m8+uMhTmQVMrRrGz6+oRf9uwQ4OzQhWgxJFvVQVl7GR/s+YmDIQAa3H+zscC6N7XxPU5c1ufmedibm8MqKg8Ql5hAR4sOH98ZwTa8Q6cAWopFJsqiHZceWkV6Uzj9G/KN5f2hVn+8pbJizI6qUkFXIq6sOsWJfGsE+Hrw8qS93xHTGzVWm6hDCGSRZ1JGp3MSH+z6kf9v+XNmh+Uymdx7b+Z5uea/JzPd0prCMuT8fZeH2RNxdXfjTtT148KpuMgJbCCeT/4F19N3x70gtTGXmsJnNt1ZhO9/T2JcgerKzI6LEVM7Hm0/y7trjFJaZuXNwKE9e24MQv6b76K4QLYkkizowWYxaRd/gvgzvONzZ4dSf7XxPwx9zaigWi+ab3ad5ffVhUvNKuLZXCE+Pj6SHTCEuRJMiyaIOfjj+A6fPnua5oc8131pFE5rvaePRTF5ecYiDqfn06+zPf+6IZlj3IKfGJISomSQLO5ktZubtnUfvoN5c1ekqZ4dTP01kvqeDqfm8svIQG45k0jnQi7mTB3BT3w4yqE64Oa9LAAAgAElEQVSIJkyShZ3+d+J/JJ9NZu7guc2zVtEE5ntKzSvm9dVHWLorGT9Pd56/sRdTh4Xh4eba6LEIIepGkoUdzBYzH+z7gMg2kYzuMtrZ4dSdk+d7Kigx8d7643y06SQWCzx4VTf+MDoCf2/3Ro1DCFF/kizssPLkShLzE3lj9BvNr1bhxPmeTOUWvth+ijd/PsqZwjImRnfkr2Nleg4hmiOHJgul1HjgTcAV+FBrPafa8f8CY6yb3kCI1jrAemwa8Lz12D+11vMdGeuFlFvKmbd3HlcEXsGY0DG1n9CUmEvhy6ng5tGo8z0Z03Ok8a8fD3Myq5Bh3YJ47oZe9O3s3yj3F0I0PIclC6WUK/A2cB2QDMQqpZZrreMrymitn7Qp/zgwwPq+DTATiAE0sNN6bo6j4r2QVQmrSMhP4PWrX8dFNbPRw1vfhuyjcM/SRpvvaWdiDi+vOMjOxBx6hPjw8X0xjOkp03MI0dw5smYxBDimtT4BoJRaDEwE4i9QfjJGggAYB6zRWp+xnrsGGA8scmC857FoC+/vfZ+IgAiuDbu2MW996XKTYMO/jSefejg+dq01764/zr9XHaatjwdzbu3L7YNkeg4hLheOTBadgCSb7WRgaE0FlVJhQFfgl4uc26mG86YD0wFCQ0MvPeJq1iSu4UTeCf496t/Nr1ax+m/GSO3xrzj8ViWmcp5eupfv9qRwc/+OzLm1r0zPIcRlxpH/o2tqd9AXKHsX8LXWurwu52qt5wHzAGJiYi507XqxaAvv/foeXf27cl3YdQ15acc79rMxlcdvXoCAhk+ittLySnhoQRy/JucxY1xPHh3dXZqchLgMOfLP5WSgi812ZyDlAmXvomoTU13OdYhfTv3CsdxjPNTvIVxdmtE4AHMprHwK2nSH4Y879FZ7knKZ8NYmjmWcZd7UQfxhTIQkCiEuU3YlC6XUUqXUjUrVqS0mFuihlOqqlGqFkRCW13DtnkAgsNVm9ypgrFIqUCkVCIy17msUFbWKcL9wxoePb6zbNoytb0H2Mbj+VeMpKAf5dncyd7y/FQ93F5Y+OpyxUe0ddi8hhPPZ++H/LnA3cFQpNUcpFVnbCVprM/AYxof8QWCJ1vqAUmq2UmqCTdHJwGKttbY59wzwD4yEEwvMrujsbgzrktZxOOcw0/tNb161itwkWP9v6HWzwzq1yy2aV1Ye5Mkvf2VgaADf/WEkke2b5lrdQoiGo2w+o2svrJQ/xof73zA6oD8APtdamxwTnv1iYmJ0XFzcJV9Ha82dP9xJoamQ7275DjeXZtRR++UUY1qPx2IhoEvt5euooMTEE4v38MuhDO4ZGsqsCVG4y9NOQjRrSqmdWuuY2srZ/T9dKRUE3Af8HtiNMdhuILCmnjE2SRuSN3DwzEEe7Pdg80oUx36Cg9/D1TMckigSsgqZ9M4W1h/J5B+39OGlSX0lUQjRgtj1aaiU+gaIBBYAN2utU62HvlRKXfqf802E1pr3fn2PTj6duLHbjc4Ox37mUljxFARFwLCGX59i87EsHl24C6VgwQNDGN49uMHvIYRo2uz90/ktrfUvNR2wp/rSXGw6vYn92fuZNWwW7i7NaJK7Lf8HZ47DlG8atFNba81nWxOZ/UM83du25sN7BxMaJPM6CdES2duO0EspFVCxYX1K6VEHxeQUFbWKjq07MqH7hNpPaCpyT8GG16DXBIi4psEuW2a28Ny3+5m5/ABjerZl6SPDJVEI0YLZmywe1FrnVmxY52h60DEhOcfWlK3szdrLA30fwN21GdUqfnzWWMRo3MsNdsnss6VM+XA7i3ac4tHR3Zk3NQZfz2b0MxFCNDh7m6FclFKq4vFW6ySBrRwXVuPSWvPur+/SvnV7bom4xdnh2O/oGjj0g7E8agN1ah9Mzef38+PIOlvKm3dFMzH6vFlWhBAtkL01i1XAEqXUNUqp32CMtv7RcWE1ru1p29mTuYcH+jxAK9dmkgNNJbBiBgT1aLBO7R/3p3Hbu1swWywseWiYJAohRCV7axZPAw8Bj2DM27Qa+NBRQTW29359jxDvEG7tcauzQ7Hflv+DnJPGOhVul5bgtNb83y/H+M+aI/TvEsAHUwcR4te4q+kJIZo2u5KF1tqCMYr7XceG0/gS8xP5NeNX/jr4r82nVpGTCBtfh963QPffXNKlisvK+etXv/K/falMGtCJV27ti6d7Mxq1LoRoFPaOs+gBvAL0Bir/5NRad3NQXI0mzC+M/936P9p4tnF2KPZb9Rwol0vu1E7JLebBz+KIT83n2esjmT6qm0wEKISokb3NUJ9gLExUsQzq/dQ8jXiz1NGno7NDsN+R1Uan9rWzwL/+fQo7E8/w0IJdlJrK+XjaYMZENs6Sq0KI5sneDm4vrfXPGHNJJWqtZwGX1v4h6s5UYkw/HtQDrvxDvS/zVVwSk+dtp7WHK9/+YbgkCiFEreytWZRYpyc/qpR6DDgNyCdMY9sy19qpvaxendrmcguvrDzER5tOMiIiiLfvHkiAdzPppxFCOJW9yeJPgDfwR4ypw8cA0xwVlKhBToLRqR01CbqPqfPpecUmHl+0mw1HMrlveDjP39hL1scWQtit1mRhHYB3h9Z6BnAWo79CNLYfnwXlCmNfqvOpxzPP8uD8OJJyinjl1r5MHuLYpVaFEJefWpOF1rpcKTXIdgS3aGRHVsHhFXDd7Dp3am84kskfvtiFu6sLC39/JUO6NqOnvoQQTYa9zVC7ge+UUl8BhRU7tdbfOCQqcU5Fp3ZwTxj6SJ1Onb8lgRe/P8AV7Xz54N4YurSRiQCFEPVjb7JoA2RT9QkoDUiycLTNbxr9Ffcut7tT21xu4cXv41mwLZFre7Xjzbuiae3RjBZyEkI0OfaO4JZ+Cmc4cxI2/QeiboVuV9t1Sl6xice+2MXGo1lMH9WNp8dH4upy2QyJEUI4ib0juD/BqElUobX+XYNHJM6p6NQeZ1+ndmJ2Ib/7NJbE7CL+dVtf7hwsHdlCiIZhb9vEDzbvPYFJQEptJymlxmOs1e0KfKi1nlNDmTuAWRjJ6Fet9d3W/eXAPmuxU1rrZrQiUQM4vBKOrITr/gF+tY8w33HyDA8tiEMDCx4YyrDuQY6PUQjRYtjbDLXUdlsptQj46WLnWB+5fRu4DkgGYpVSy7XW8TZlegDPAiO01jlKKduBfsVa62j7vo3LjKkYVj4NbSPhyto7tb+KS+K5b/fRpY03H08bTHhw60YIUgjRktS317MHUFsbxxDgmNb6BIBSajEwEYi3KfMg8LZ15T201hn1jOfysukNyE2Ead/DRVbts1g0r646zHvrjzMiIoh37h6Ev7esaCeEaHj29lkUULXPIg1jjYuL6QQk2WwnA0OrlbnCev3NGE1Vs7TWFYsqeSql4gAzMEdrvayGuKYD0wFCQy+T9vkzJ2HTf6HP7dB11AWLFZWZ+dPiPayOT+eeoaHMmhCFu4zIFkI4iL3NUL71uHZNj+BU7yR3w6iljAY6AxuVUn2s632Haq1TlFLdgF+UUvu01serxTUPmAcQExNzeQwY/PEZozYx9p8XLJKaV8wDn8ZxKC2fmTf35r7h4TK1uBDCoez6U1QpNUkp5W+zHaCUqm2x6mTAdmHozpzfKZ4MfKe1NmmtTwKHMZIHWusU6+sJYB0wwJ5Ym7XDK+HIjzD6GfDrUGORX5NymfjWZk6dKeKj+wZz/4iukiiEEA5nb7vFTK11XsWG9S//mbWcEwv0UEp1VUq1Au4CllcrswxjUkKUUsEYzVInlFKBSikPm/0jqNrXcfkxFRsjtdtGwtCHayzyv72p3PH+VtxdXVj6yHDG9JSJf4UQjcPeDu6akspFz9Vam63Tma/C6I/4WGt9QCk1G4jTWi+3HhurlIoHyoEZWutspdRw4H2llMV67zm2T1Fdljb9F3JPwbQfzuvU1lrz1i/HeH3NEQaFBfL+1EEE+3g4KVAhREuk7JkbUCn1MZCL8SisBh4HArXW9zk0ujqIiYnRcXFxzg6jfrKPwzvDoPcEuO3DKodKTOU8s3Qvy/akyBrZQogGp5TaqbWOqa2cvc1QjwNlwJfAEqAYqP9SbeIcra2d2q2MAXg2ss6WcvcH21i2J4W/jr2C/9zRXxKFEMIp7H0aqhB4xsGxtEyHV8LR1TDu5Sqd2ofTCvjdp7FkF5byzj0DuaFvzR3eQgjRGOx9GmqNUirAZjtQKbXKcWG1EGVF1pHavWDI9Mrdaw9lcNu7WzCVW1jy0DBJFEIIp7O3gzvY+gQUADVMzSHqY9N/Ie8U3Pc/cHVHa83HmxN46X/x9Orgx4fTYujg7+XsKIUQwu5kYVFKhWqtTwEopcKpYRZaUQfZx2HzG9D3DggfiancwszlB/hi+ynG9m7HG3dF491K1qAQQjQN9n4a/Q3YpJRab90ehXWaDVEPWhvNT64eMPYf5BWZePSLnWw+ls0jo7szY2xPXGQNCiFEE2JvB/ePSqkYjASxB/gO44koUR+H/gfH1sC4VzhZ6ssDn24mKaeI137bn9sHdXZ2dEIIcR57JxL8PfAExpQde4Arga1UXWZV2KOsyHhUNiSKrcG38fDbm3FRsPD3VzKkaxtnRyeEEDWyd5zFE8BgIFFrPQZjnqZMh0V1Odv5KeQl8XO3vzL1k5209fVg2R9GSKIQQjRp9vZZlGitS5RSKKU8tNaHlFI9HRrZZUrvXUyqdy8eWOfBVT2CePuegfh5yhoUQoimzd5kkWwdZ7EMWKOUysGOZVVFNZlHUKm/8pFpCtOGhfHCTb1xkzUohBDNgL0d3JOsb2cppdYC/sCPFzlF1EDvXYJGkdjhej6c2MfZ4QghhN3q/CC/1np97aXEebSmZPeX7Czvzc0jL/+lOYQQlxdpA2ksp3fhdfYUv7iPYnyf9s6ORggh6kSGCDeSgriFtNLu+A+6HQ83mTlWCNG8SM2iMZSbUQe+Za0lmttHRDk7GiGEqDNJFo2g7Pg6fExnONHxRjoFyMSAQojmR5JFI0jbuIB87UXf0b91dihCCFEvkiwczVRMcPIqNrsPZ0TPTs6ORggh6kWShYMlbf8Gb12M7vtbmUlWCNFsOTRZKKXGK6UOK6WOKaVqXJZVKXWHUipeKXVAKfWFzf5pSqmj1q9pjozTkfJ3LCJDBzDimkm1FxZCiCbKYY/OKqVcgbeB64BkIFYptVxrHW9TpgfwLDDCdvU9pVQbYCYQg7HI0k7ruTmOitcR8nIy6ZG3le3Bk7jKx9PZ4QghRL05smYxBDimtT6htS4DFgMTq5V5EHi7IglorTOs+8cBa7TWZ6zH1gDjHRirQ+xb/RmtlJkOV93r7FCEEOKSODJZdAKSbLaTrftsXQFcoZTarJTappQaX4dzUUpNV0rFKaXiMjOb1ozpWmtaH/mWFNeORPS/ytnhCCHEJXFksqipN7f6ut1uQA9gNDAZ+NA6u60956K1nqe1jtFax7Rt2/YSw21YcfsO0N+8n9yISaCkY1sI0bw5MlkkA11stjtz/rTmycB3WmuT1vokcBgjedhzbpOWsP4zXJSm+5hm2zcvhBCVHJksYoEeSqmuSqlWwF3A8mpllgFjAJRSwRjNUieAVcBYpVSgUioQGGvd1yyk5hUTmbmK1Na98Ggva0QJIZo/hyULrbUZeAzjQ/4gsERrfUApNVspNcFabBWQrZSKB9YCM7TW2VrrM8A/MBJOLDDbuq9Z+HHtBvq6JOA58C5nhyKEEA3CobPOaq1XACuq7fu7zXsN/Nn6Vf3cj4GPHRmfI5SZLZh/XYIFReAQSRZCiMuDjOBuYKv2p3Jd+UZy2w0DX1m3QghxeZBk0cC2blhNuEs6AUPvdnYoQgjRYCRZNKBDafn0yFiJ2aUVLr0n1H6CEEI0E5IsGtDCLce5yXUbloix4Onv7HCEEKLByLKqDaSgxETqntW0dcmD6DudHY4QQjQoqVk0kG93n2a83kR5Kz/oMdbZ4QghRIOSZNEAtNYs3nKEG9xicY2aAO4yw6wQ4vIiyaIBbDtxhvDsjXjrYugrS6cKIS4/kiwawOfbErm91Va0T3sIlxlmhRCXH0kWlyg9v4StB44xSu1B9bkNXFydHZIQQjQ4SRaXaNGOU1ynduCmTdD3dmeHI4QQDiGPzl4CU7mFL7af4lOfWPCJgI4DnB2SEEI4hNQsLsGa+HRUQSq9Sn81OrZlkSMhxGVKksUlWLA1kSk+cSi0PAUlhLisSTNUPR1NL2DriWzmBm8Fv4EQ1N3ZIQknMplMJCcnU1JS4uxQhKiRp6cnnTt3xt3dvV7nS7Kop8+3JdLLNZW2Zw/DiFecHY5wsuTkZHx9fQkPD0dJc6RoYrTWZGdnk5ycTNeuXet1DWmGqofCUjNLd53mTx1+BeUCfW51dkjCyUpKSggKCpJEIZokpRRBQUGXVPOVZFEP3+4+zdlSE6NL10HXUbLIkQCQRCGatEv9/ZRkUUdaaz7flsitIWl4FJySjm0hRIsgyaKOYhNyOJRWwENtdoGrB/S62dkhCVHppZdeIioqin79+hEdHc327dt54403KCoqarB7hIeHk5WVVe/z161bx0033VRl38SJExk2bNhFz/Px8an3PcWlc2iyUEqNV0odVkodU0o9U8Px+5RSmUqpPdav39scK7fZv9yRcdbFgm2JBHgqrshYDVeMk0WORJOxdetWfvjhB3bt2sXevXv56aef6NKlS4Mni7oqLy+/6PHc3Fx27dpFbm4uJ0+ebKSoRF057GkopZQr8DZwHZAMxCqllmut46sV/VJr/VgNlyjWWkc7Kr76yCgo4cf9qczsnY46milNUKJGL35/gPiU/Aa9Zu+Ofsy8OeqiZVJTUwkODsbDwwOA4OBg5s6dS0pKCmPGjCE4OJi1a9fyyCOPEBsbS3FxMbfffjsvvvgiYNQYpk2bxvfff4/JZOKrr74iMjKS7OxsJk+eTGZmJkOGDEFrXXnPW265haSkJEpKSnjiiSeYPn06YNQC/vznP7Nq1Spef/11zp49y5/+9CeCg4MZOHBglbiXLl3KzTffTLt27Vi8eDHPPvssACdPnuTuu+/GbDYzfvz4yvJnz55l4sSJ5OTkYDKZ+Oc//8nEiRNJSEhg/PjxjBw5km3bttG/f3/uv/9+Zs6cSUZGBgsXLmTIkCGX/o/RQjmyZjEEOKa1PqG1LgMWAxMdeD+H+3JHEqZyzUSXTeDhL4sciSZl7NixJCUlccUVV/Doo4+yfv16/vjHP9KxY0fWrl3L2rVrAaOpKi4ujr1797J+/Xr27t1beY3g4GB27drFI488wmuvvQbAiy++yMiRI9m9ezcTJkzg1KlTleU//vhjdu7cSVxcHHPnziU7OxuAwsJC+vTpw/bt24mJieHBBx/k+++/Z+PGjaSlpVWJe9GiRUyePJnJkyezaNGiyv1PPPFEZWJr3/7cQySenp58++237Nq1i7Vr1/KXv/ylMoEdO3aMJ554gr1793Lo0CG++OILNm3axGuvvcbLL7/cwD/xlsWR4yw6AUk228nA0BrK3aaUGgUcAZ7UWlec46mUigPMwByt9bLqJyqlpgPTAUJDQxsy9vOYyy18seMUY7r74nvyR4i6RRY5EjWqrQbgKD4+PuzcuZONGzeydu1a7rzzTubMmXNeuSVLljBv3jzMZjOpqanEx8fTr18/AG691XgMfNCgQXzzzTcAbNiwofL9jTfeSGBgYOW15s6dy7fffgtAUlISR48eJSgoCFdXV2677TYADh06RNeuXenRowcAU6ZMYd68eQCkp6dz7NgxRo4ciVIKNzc39u/fT58+fdi8eTNLly4FYOrUqTz99NOA8ZDJc889x4YNG3BxceH06dOkp6cD0LVrV/r27QtAVFQU11xzDUop+vbtS0JCQgP9pFsmRyaLmp7T0tW2vwcWaa1LlVIPA/OB31iPhWqtU5RS3YBflFL7tNbHq1xM63nAPICYmJjq125QPx/KIDWvhHcHnILTZ6HvHY68nRD14urqyujRoxk9ejR9+/Zl/vz5VY6fPHmS1157jdjYWAIDA7nvvvuqPHtf0YTl6uqK2Wyu3F/TY5fr1q3jp59+YuvWrXh7ezN69OjKa3l6euLq6nrR8wG+/PJLcnJyKgeK5efns3jxYv75z39e8LyFCxeSmZnJzp07cXd3Jzw8vPK+FfEDuLi4VG67uLhU+X5E3TmyGSoZ6GKz3RlIsS2gtc7WWpdaNz8ABtkcS7G+ngDWAU6d0nXB1kQ6+nvS78xq8GkP4SOdGY4Q5zl8+DBHjx6t3N6zZw9hYWH4+vpSUFAAGB/GrVu3xt/fn/T0dFauXFnrdUeNGsXChQsBWLlyJTk5OQDk5eURGBiIt7c3hw4dYtu2bTWeHxkZycmTJzl+3Phbz7apadGiRfz4448kJCSQkJDAzp07Wbx4MQAjRoyofF9x/4r7hoSE4O7uztq1a0lMTLT7ZyTqz5HJIhbooZTqqpRqBdwFVHmqSSnVwWZzAnDQuj9QKeVhfR8MjACqd4w3muOZZ9l0LIv7BwbgcmwNyCJHogk6e/Ys06ZNo3fv3vTr14/4+HhmzZrF9OnTuf766xkzZgz9+/dnwIABREVF8bvf/Y4RI0bUet2ZM2eyYcMGBg4cyOrVqyubfMePH4/ZbKZfv3688MILXHnllTWe7+npybx587jxxhsZOXIkYWFhACQkJHDq1Kkq53Xt2hU/Pz+2b9/Om2++ydtvv83gwYPJy8urLHPPPfcQFxdHTEwMCxcuJDIy8lJ+bMJOyvbJhga/uFI3AG8ArsDHWuuXlFKzgTit9XKl1CsYScIMnAEe0VofUkoNB94HLBgJ7Q2t9UcXu1dMTIyOi4tzyPfx4vcH+HxbIjtvSMFvzV9g+jpZu0JUcfDgQXr16uXsMIS4qJp+T5VSO7XWMbWd69CJBLXWK4AV1fb93eb9s8CzNZy3BejryNjsVVRm5uudyYzv0wG/o+9AUAR0aFJP9AohhMPJCO5aLN+TQkGJmQf6tYKETbLIkRCiRZJkcRFaaz7bmkhke1/65/4MssiREKKFkmRxEbtO5RKfms+UK8NQ+76CjrLIkRCiZZJkcRELtibg4+HGrV3OQtpe6CdjK4QQLZMkiwvIOlvKin1p3DawE96HvzUWOYqa5OywhBDCKSRZXMCSuCTKyi1MvTIU9n0lixyJZqGmKcrNZjPPPfccPXr0IDo6mujoaF566aXKc1xdXYmOjiYqKor+/fvzn//8B4vF4sTvQjRFsgZ3DcotmoXbTjGsWxARpiOQkwCjnnJ2WEJclO0U5R4eHmRlZVFWVsbzzz9PWloa+/btw9PTk4KCAl5//fXK87y8vNizZw8AGRkZ3H333eTl5VXORisESLKo0dpDGZzOLeZvN/aCvf+1LnJ0U+0nCgGw8hlI29ew12zfF64/f1JAWzVNUV5UVMQHH3xAQkICnp7GxJe+vr7MmjWrxmuEhIQwb948Bg8ezKxZs2SpWFFJmqFqsGBbIu38PLguMggOfCOLHIlmoaYpyo8dO0ZoaCi+vr52X6dbt25YLBYyMjIcGK1obqRmUU1CViHrj2Typ2t74J64EQoz5SkoUTe11AAcpaYpyp977rkqZT755BPefPNNsrOz2bJlC126dKnxWo6cBkg0T1KzqGbh9kTcXBSTh1g7tj38IeI6Z4clhF0qpih/8cUXeeutt/j+++85depU5ayz999/P3v27MHf3/+Cy52eOHECV1dXQkJCGjN00cRJsrBRXFbOkrhkxkW1p52XhoPfQ+8JssiRaBZqmqK8Z8+ePPDAAzz22GOVaz6Ul5dTVlZW4zUyMzN5+OGHeeyxx6S/QlQhzVA2vt+bQl6xiSlXhsHhlVB2Vqb3EM3G2bNnefzxx8nNzcXNzY2IiAjmzZuHv78/L7zwAn369MHX1xcvLy+mTZtGx44dASguLiY6OhqTyYSbmxtTp07lz3/+s5O/G9HUSLKw8fm2RHqE+HBltzaw+GtZ5Eg0K4MGDWLLli01HpszZ06NS6wCF2yOEsKWNENZ7UnKZW9yHlOHhaFKcuHoauh7uyxyJIQQSLKotGBrIq1buTJpQCeI/w4sJiNZCCGEkGQBkFNYxvd7U5g0sBO+nu6w72tZ5EgIIWxIssA6D5TZYnRs5522LnJ0hyxyJIQQVi0+WVgsms+3JzIkvA2R7f1g/1KMRY6kCUoIISq0+GSRnFOMyayZOizM2LHvK+g0SBY5EkIIGw5NFkqp8Uqpw0qpY0qpZ2o4fp9SKlMptcf69XubY9OUUketX9McFWNokDebnh7D9X3aQ+ZhY5EjGVshhBBVOCxZKKVcgbeB64HewGSlVO8ain6ptY62fn1oPbcNMBMYCgwBZiqlAh0Vq5urC26uLkatQrlA1K2OupUQDvXtt9+ilOLQoUM1Hr/vvvv4+uuvGzmqc2644QZyc3PJzc3lnXfeqdy/bt06brqpYWd2TkhIoE+fPnU6x56fj22ZN954g6KionrH2BBmzJhBVFQUM2bMIDMzk6FDhzJgwAA2btzYoPdx5KC8IcAxrfUJAKXUYmAiEG/HueOANVrrM9Zz1wDjgUUOihW0ti5ydDX4tnPYbcTl7187/sWhMzV/WNdXZJtInh7ydK3lFi1axMiRI1m8ePEFpyF3phUrVgDGB/k777zDo48+6uSILs0bb7zBlClT8Pb2dloM77//PpmZmXh4eLB48WIiIyOZP39+g9/Hkc1QnYAkm+1k677qblNK7VVKfa2UqpgC065zlVLTlVJxSqm4zMzMS4s2Oc5Y5EiaoEQzdfbsWTZv3sxHH33E4sWLAWP22Mcee4zevXtz4403Vpl2fPbs2QwePJg+ffowffr0yplmR48ezZNPPsmoUaPo1asXsbGx3HrrrfTo0YPnn3/+gvd/9dVXmTt3LgBPPvkkv/nNbwD4+eefmTJlCgDh4YxIH2YAAA0oSURBVOFkZWXxzDPPcPz4caKjo5kxY0Zl/LfffjuRkZHcc889F5359kKx79y5k/79+zNs2DDefvvtyvIJCQlcddVVDBw4kIEDB1aOdL/Yz2fnzp1cffXVDBo0iHHjxpGamlolhrlz55KSksKYMWMYM2YMAI888ggxMTFERUUxc+bMC8YPEBsby/Dhw+nfvz9DhgyhoKCAkpIS7r//fvr27cuAAQNYu3YtYIyynzFjBoMHD6Zfv368//77AEyYMIHCwkKGDh3Kv/71L5566ilWrFhBdHQ0xcXFF71/nWmtHfIF/Bb40GZ7KvB/1coEAR7W9w8Dv1jfzwCetyn3AvD/7d1/cFVlfsfx9zcEEogSFFgnEoaEFoGNcQMBIWLCWnSRsYObAae4BUFWHQdWls5aq/3J+KNTZ7LVjrMjtFujskwQolim1S6VRIUZuiS4UIEUupJIAlTwbg1IE8gNn/5xL7cBE24g93JI8n39k3OfnB/f5+TmfM/znHuf5yeXOl5hYaF65F/+WHp2pNTS3LP9uH5p//79QYegtWvXaunSpZKkoqIi7dq1S2+//bbuvvtuhcNhHTlyRJmZmdq4caMkKRQKxbZduHChNm/eLEmaOXOmnnrqKUnSyy+/rKysLB09elStra0aNWqUvvzyy06Pv2PHDs2fP1+SdOedd2rq1Kk6e/asVq1apdWrV0uSxowZoxMnTqi+vl55eXmxbaurqzV06FA1Njaqvb1d06dP17Zt27qsa1ex5+fn68MPP5QkPfnkk7FjnD59Wi0tLZKkgwcP6vz1oqvzc/bsWRUVFen48eOSpPXr1+vhhx+WJC1evDh2Ds/X5+K4wuGwZs6cqT179nQa/5kzZ5Sbm6udO3dKkpqbm9XW1qaysjItWbJEklRXV6fRo0erpaVFa9as0XPPPSdJam1tVWFhoQ4dOiRJysjIiO23vLxcy5cv7/K8dfY+BWrVjWt6MlsWTUDHwfKzgaMdV5AUknQm+vIfgMLubptQ7eHIJEfj74X0oUk7jHPJVFFRwYIFCwBYsGABFRUVfPzxxzz44IMMGDCAm2++OXa3D1BdXc20adPIz8+nqqqKffv2xX43d+5cAPLz88nLyyMrK4u0tDTGjh1LY2MjnSksLGTXrl2cOnWKtLQ0ioqKqK2tZdu2bRQXF8eN//bbbyc7O5uUlBQKCgpoaGjoct3OYm9ubuarr75i5syZACxatCi2fltbG48++ij5+fk88MAD7N8f6Q3v6vwcOHCAvXv3cs8991BQUMDzzz9PU1NT3Dps2LCByZMnM2nSJPbt2xc7zsUOHDhAVlYWU6dOBWDo0KGkpqayffv2WNwTJkxgzJgxHDx4kC1btvDmm29SUFDAtGnTCIVCF4wwfDUk85lFDTDOzHKBI8AC4AcdVzCzLEnn23Zzgbro8i+Bv+7wUPt7wDNJi7T+w8gkR94F5XqpUChEVVUVe/fuxcxob2/HzCgtLe10qPHW1laWLVtGbW0to0ePZtWqVbEhzIHY1KwpKSmx5fOvw+FwpzEMHDiQnJwcysvLueOOO7jtttuorq7ms88+Y+LEiXHr0PE4AwYM6PI4XcUuqcth1V966SVuuukm9uzZw7lz52JTzAKdbiOJvLw8duzYETfu8+rr6ykrK6OmpoYbbriBJUuWXHBOL95/V8ftav1XXnmF2bNndzueREtay0JSGPgRkQt/HbBB0j4ze9bM5kZXW2Fm+8xsD7ACWBLd9rfAc0QSTg3wbLQsOT6tjExyNO57STuEc8lUWVnJQw89xOeff05DQwONjY3k5uZy4403sn79etrb2zl27FisD/z8RWzEiBF8/fXXCfuEVElJCWVlZZSUlFBcXMzq1aspKCj4xoXx+uuvj03IdLm6in3YsGFkZmayfft2ANatWxfbprm5maysLFJSUli7dm1spN2SkpJOz8/48eM5ceJELFm0tbVd0PLqrB4nT54kIyODzMxMvvjiC95///0u6zBhwgSOHj1KTU0NAKdOnSIcDlNSUhKL++DBgxw+fJjx48cze/ZsXn31Vdra2mK/O3369BWdvyuV1CHKJb0HvHdR2V92WH6GLloMkl4DXktmfACc/d/IJEd5pZCaFn99565BFRUVPP30hV9lmjdvHnV1dYwbN478/HxuueWWWBfNsGHDYt0yOTk5se6QniouLuaFF16gqKiIjIwM0tPTO+2CGj58ODNmzODWW29lzpw53Hfffd0+xqViLy8vZ+nSpQwZMuSCu/Bly5Yxb948Nm7cyF133UVGRgYApaWlVFVVfeP8DBo0iMrKSlasWEFzczPhcJiVK1eSl5d3QSyPPfYYc+bMISsri+rqaiZNmkReXh5jx45lxowZXdZh0KBBvPXWWzzxxBO0tLQwePBgPvjgA5YtW8bjjz9Ofn4+qampvP7666SlpfHII4/Q0NDA5MmTkcTIkSN59913u33OEsG6avb0NlOmTFFtbe3lb3jyGGz5M5jyQ8jp+o/r3KXU1dV1q6vFuSB19j41s12SpsTb1ic/GpoF85PfgHHOud7Mk4Vz7rKEQiFmzZr1jfKtW7cyfPjwhB6rtLSU+vr6C8pefPHFQB/0Xq6+UAfwZOFcwlzq0zh9yfDhw9m9e/dVOdamTZuuynGS6VqpQ08fOfT7UWedS4T09HRCoVCP/yGdSwZJhEKhCz4yfLm8ZeFcAmRnZ9PU1ESPh51xLknS09PJzs6+4u09WTiXAAMHDiQ3NzfoMJxLGu+Gcs45F5cnC+ecc3F5snDOORdXn/kGt5mdAD7vwS5GAF8mKJxrjdet9+rL9fO6XRvGSBoZb6U+kyx6ysxqu/OV997I69Z79eX6ed16F++Gcs45F5cnC+ecc3F5svh/fx90AEnkdeu9+nL9vG69iD+zcM45F5e3LJxzzsXlycI551xc/T5ZmNm9ZnbAzH5jZk/H36L3MLPRZlZtZnXRuc5/HHRMiWZmA8zs12b2z0HHkkhmNszMKs3sP6N/v6KgY0okM/uj6Htyr5lVmNmVD4caMDN7zcyOm9neDmU3mtm/mdl/RX/eEGSMidCvk4WZDQB+BswBvg08aGbfDjaqhAoDP5E0EZgOLO9j9QP4MVAXdBBJ8HfAv0qaAHyHPlRHMxsFrACmSLoVGAAsCDaqHnkduPeisqeBrZLGAVujr3u1fp0sgNuB30g6JOkssB64P+CYEkbSMUmfRJdPEbngjAo2qsQxs2zgPuDnQceSSGY2FCgB/hFA0llJXwUbVcKlAoPNLBUYAhwNOJ4rJulj4LcXFd8PvBFdfgP4/lUNKgn6e7IYBTR2eN1EH7qYdmRmOcAk4FfBRpJQLwNPAeeCDiTBxgIngPJoF9vPzSwj6KASRdIRoAw4DBwDmiVtCTaqhLtJ0jGI3LQB3wo4nh7r78miszkw+9xnic3sOuBtYKWkk0HHkwhm9vvAcUm7go4lCVKBycCrkiYBp+kD3RjnRfvv7wdygZuBDDNbGGxULp7+niyagNEdXmfTi5vDnTGzgUQSxTpJ7wQdTwLNAOaaWQOR7sPfM7NfBBtSwjQBTZLOtwIriSSPvuJuoF7SCUltwDvAHQHHlGhfmFkWQPTn8YDj6bH+nixqgHFmlmtmg4g8ZNsccEwJY2ZGpN+7TtLfBh1PIkl6RlK2pBwif7cqSX3i7lTSfwONZjY+WjQL2B9gSIl2GJhuZkOi79FZ9KEH+FGbgcXR5cXAPwUYS0L062lVJYXN7EfAL4l8IuM1SfsCDiuRZgCLgE/NbHe07E8lvRdgTK57ngDWRW9iDgEPBxxPwkj6lZlVAp8Q+cTer+nFw2OYWQXwXWCEmTUBfwX8DbDBzH5IJDk+EFyEieHDfTjnnIurv3dDOeec6wZPFs455+LyZOGccy4uTxbOOefi8mThnHMuLk8Wzl0DzOy7fW3kXNe3eLJwzjkXlycL5y6DmS00s51mttvM1kTn0/jazH5qZp+Y2VYzGxldt8DM/t3M/sPMNp2f08DMftfMPjCzPdFtfie6++s6zGGxLvrtZueuCZ4snOsmM5sI/AEwQ1IB0A78IZABfCJpMvARkW/wArwJ/Imk24BPO5SvA34m6TtExkQ6Fi2fBKwkMrfKWCLfwHfumtCvh/tw7jLNAgqBmuhN/2AiA8SdA96KrvML4B0zywSGSfooWv4GsNHMrgdGSdoEIKkVILq/nZKaoq93AznA9uRXy7n4PFk4130GvCHpmQsKzf7iovUuNYbOpbqWznRYbsf/P901xLuhnOu+rcB8M/sWxOZZHkPk/2h+dJ0fANslNQP/Y2bF0fJFwEfR+USazOz70X2kmdmQq1oL566A37k4102S9pvZnwNbzCwFaAOWE5mcKM/MdgHNRJ5rQGRo6tXRZNBx5NhFwBozeza6j14/Iqnr+3zUWed6yMy+lnRd0HE4l0zeDeWccy4ub1k455yLy1sWzjnn4vJk4ZxzLi5PFs455+LyZOGccy4uTxbOOefi+j/bdTWAKGz/4gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_graphs(test_log, 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
